# \chapter{绪论}

## \section{选题背景与意义}

近年来，深度学习领域涌现出越来越多规模庞大的模型，并且模型的参数量成指数级增长\cite{bommasani2021opportunities}。以最近风靡全球的人工智能聊天机器人ChatGPT为例，其前身为大规模预训练语言模型GPT。2018年发布的GPT-1\cite{radford2018improving}的参数量为1.17亿，数据量约为5GB；2019年发布的GPT-2\cite{radford2019language}参数量上升到15亿，预训练的数据量也激增到40GB；一年后，有着1750亿参数的GPT-3\cite{brown2020language}横空出世，其预训练的数据量更是达到了惊人的45TB。这些庞大的模型能够处理高复杂度和多模态的任务，并且在这些任务上能够取得相当高的精度。按照传统机器学习的观点，模型的复杂度需要与数据集的规模相匹配，模型过于复杂可能会导致过拟合现象的产生\cite{ying2019overview}。然而，对于参数量远远多于传统机器学习模型的深度神经网络来说，参数量与测试准确率的关系并非如此。Mikhail Belkin等人的研究\cite{belkin2019reconciling}发现，随着模型容量从小到大，模型的测试准确率先增加后减小，而后再次增加，且逐渐刷新最优的测试准确率。深度学习的这种过参数化（over-parameterized）性质可以用大模型包含小模型的解空间来解释。因此，深度学习模型的容量越大，测试准确率和表达能力越好，能处理的任务也就越复杂。凭借着优越的性能，大模型相关领域的研究正得到众多科研工作者、工业界人士甚至是全社会的广泛关注。

对于一个规模庞大的深度学习模型的训练来说，充足且优质的数据是必不可少的。然而，对于一些特定的或者复杂的任务来说，训练数据往往不够充足，数据的质量也参差不齐。近些年来，边缘计算的发展促进了深度学习模型在各类边缘设备上的部署和应用，边缘智能的提供方可以通过边缘设备采集数量庞大的训练数据，甚至利用原始数据直接在边缘设备上进行深度学习的训练。其中，仅利用边缘设备有限的数据难以训练强大的深度学习模型，而数据持有方出于隐私保护的考虑，往往会拒绝分享原始的数据。在Brendan McMahan等人于2017年提出的联邦学习（federated learning）\cite{mcmahan2017communication}的概念中，客户端（clients）无需上传自己的数据，而是以迭代的方式，首先在本地用自己的数据训练模型，然后将本地的模型上传给服务器，最后服务器端通过模型聚合得到一个强大的全局模型。在保护数据隐私的基础上，联邦学习中全局模型的训练相当于利用了众多边缘设备上的数据信息，因此可以成为一种重要的大模型训练的方式。

利用联邦学习的框架训练大模型并不是完美无缺和一劳永逸的。首先，正如前面所述，大模型的参数数量是非常多的。对于如今有限的云边传输带宽，尤其是从客户端到服务器更加有限的上传带宽，过多的数据传输量会带来巨大的通信成本，影响网络传输的效率。其次，过多的模型参数也会导致训练过程中需要更多的计算资源。对于大多数的物联网边缘设备来说，其计算能力远远弱于云端服务器的计算能力。因此在联邦学习中，直接将服务器端的全局大模型通过有限带宽的网络传输给有限资源的边缘设备进行训练和推理是不切实际的。对于这一挑战，网络剪枝（network pruning）\cite{han2015learning}、知识蒸馏（knowledge distillation）\cite{hinton2015distilling}、参数量化（parameter quantization）\cite{courbariaux2015binaryconnect,courbariaux2016binarized}等技术可以对规模庞大的深度学习模型进行压缩。其中，知识蒸馏通常需要服务器端有公共数据集，或者获取客户端学习的知识，而参数量化对硬件的要求更高，通常只在特定的硬件上更有效。因此，网络剪枝技术更加适应联邦学习框架重视隐私保护的特点，同时更加适应普遍的硬件条件。

网络剪枝是模型压缩中一类通用且有效的技术，但这并不意味着网络剪枝技术可以完全解决上述挑战。首先，常见的模型剪枝框架是先训练、后修剪、再微调\cite{han2015learning}，或者在训练过程中逐渐修剪越来越多的权重并省略了微调的过程\cite{gale2019state}，又或者如“彩票假说”（Lottery Ticket Hypothesis）\cite{frankle2018lottery}在每轮迭代中对完整的模型进行修剪。然而，这些方法对于训练过程中的通信和计算成本的优化非常有限。从另一个方面说，若采用静态拓扑的方式\cite{molchanov2016pruning}从随机初始化开始对模型进行剪枝，即使有效减少了模型的参数量，其训练的效果通常远远不如同等规模的完整密集模型\cite{mocanu2016topological}，甚至有可能不如同等参数数量的小型密集的网络\cite{evci2019difficulty}。Mocanu等人提出“稀疏进化训练”（SET）\cite{mocanu2018scalable}和Utku Evci等人提出的“作弊彩票”（RigL）\cite{evci2020rigging}本质上都是基于动态拓扑的剪枝方法，具备表达和泛化能力强、训练和推理成本低的特点\cite{liu2021we}，能帮助大模型在联邦学习的框架中进行训练。因此，本文将借助动态剪枝的思想减少大模型联邦训练算法的通信量和计算量。

对于众多边缘设备上的数据，标签的分布和质量也会对大模型联邦训练的效果造成影响。联邦学习一个重要的特征是分布在不同客户端上的数据常常不能满足独立同分布的性质，因为每个客户端采集数据的方式、对象、能力不尽相同，因此不同客户端之间的数据分布更有可能是非独立同分布的。数据非独立同分布可以分为特征分布不同（feature distribution skew）、标签分布不同（label distribution skew）、相同标签不同特征、相同特征不同标签和数据量不同（quantity skew）五种情况\cite{kairouz2021advances}。其中，客户端之间标签分布不同相比特征分布不同通常更能损害联邦大模型的训练效果，而大部分工作（包括本文）不涉及相同标签不同特征、相同特征不同标签这两类情况。对于客户端之间数据量不同的数据分布，将客户端的数据量占总数据量的比例作为模型聚合权重的FedAvg算法\cite{mcmahan2017communication}有效解决了这一挑战。因此，本文关注的重点将是标签分布不同和标签噪声对全局大模型的训练效果造成的影响。标签分布不同的含义是每个客户端拥有标签的种类和数量不同，而标签噪声意味着某些客户端上的某些数据标签可能在采集过程中无意地出错，这两种情况都会影响客户端的数据质量，从而影响客户端对于联邦训练的贡献大小，进而影响全局大模型的训练效果\cite{zhao2018federated}。因此，本文将试图从数据质量和贡献评估的角度调整客户端对联邦训练的参与程度，从而使得大模型联邦训练算法对异构且带噪的标签数据具备鲁棒性。

为了解决上述提到的诸多挑战，本文提出了一个新颖的基于剪枝优化的大模型联邦训练算法。该算法的目标是训练一个性能良好的全局大模型，减少通信和计算资源的消耗，同时对分布和质量不同的标签数据具备鲁棒性。具体来说，本文提出的大模型联邦训练算法利用基于动态拓扑的剪枝技术对全局大模型进行压缩，在尽可能保持训练效果的同时减少模型的参数量，本地训练完成后，利用基于贡献评估的聚合技术对分布在不同边缘设备上的稀疏模型进行聚合，根据边缘设备的贡献调整其在联邦训练中的参与程度。总的来说，本文的贡献可以总结为以下几个方面：

\begin{enumerate}
    \item 将基于动态拓扑的剪枝算法应用于联邦学习的框架中，减少大模型联邦训练算法的通信量和计算量，并通过消融实验验证了剪枝算法的有效性，还研究了剪枝算法中的超参数对全局大模型训练效果的影响。
    \item 将基于贡献评估的聚合算法应用于联邦学习的框架中，使得大模型联邦训练算法对异构且带噪的标签数据具备鲁棒性，并通过实验展示了贡献评估方法的合理性和超参数对全局大模型训练效果的影响，还通过消融实验验证了聚合算法的有效性。
    \item 本文通过实验说明，相比其他现有的联邦剪枝算法，本文提出的大模型联邦训练算法能够用更少的通信和计算开销在测试数据集上实现更优的准确率，同时减轻异构且带噪的标签数据对全局大模型训练效果的影响。
    \item 本文还通过实验研究了在边缘设备数据的各种分布下，剪枝算法中的剪枝率对全局大模型训练效果的影响，以及根据边缘设备的贡献程度自适应剪枝对全局大模型训练效果的影响。

\end{enumerate}



## \section{国内外研究现状和相关工作}

### \subsection{联邦学习中的模型压缩}

正如绪论所述，联邦学习框架中受限的通信和计算能力难以满足如今精度更高，但规模更大的深度学习模型的训练需求，因此在联邦学习中采用一些模型压缩技术是十分必要的。FedGKT\cite{he2020group}设计了一种交替最小化方法的变体，在边缘设备上训练小型网络，并通过双向知识蒸馏定期在大模型和小模型之间迁移知识。然而，FedGKT需要知道客户端每条数据的标签、特征和预测结果，与联邦学习隐私保护的目标相悖。Fed-ET\cite{cho2022heterogeneous}提出了一种具有多样性正则化的加权共识精馏方法，考虑客户端模型异构和数据非独立同分布的同时，使用无标签数据实现集成转移（ensemble transfer），但是Fed-ET需要在模型后加一个表示层，并且要求所有模型的表示层的维度保持一致。FedPCL\cite{tan2022federated}使用局部原型和全局原型以有监督的对比方式进行知识共享，通过对比学习，最大化融合表示与其对应原型之间的一致性。然而，FedPCL需要知道客户端拥有的数据类别和数量，与FedGKT一样存在隐私泄露的风险。



### \subsection{网络剪枝}

深度学习模型从卷积层到全连接层存在着大量冗余的参数，网络剪枝将模型中不重要的神经元、滤波器或网络层剪去，这些被剪掉的神经元连接或结构会在计算时被忽略。网络剪枝在剪枝对象上可以分为非结构化剪枝和结构化剪枝。非结构化剪枝\cite{han2015learning}的对象是神经元之间的权重连接，这是一种简单、直接且细粒度的剪枝方式，但这会导致剪枝后的模型变得稀疏，因此只有部分能够加速稀疏矩阵计算的硬件架构能够降低实际上的计算量。结构化剪枝\cite{kruschke1991benefits}的对象通常是整个滤波器或网络层，能够通过常见的GPU或其他硬件得到加速，但对滤波器进行剪枝时，滤波器之前和之后的特征图都会发生变化，因此实现起来更加复杂。在剪枝的标准上，一个直观且有效的标准是剪去绝对值较小的权重\cite{gale2019state}，另外，还有一些方法\cite{blalock2020state}会利用训练过程中产生的梯度信息，根据梯度与相应权重之间的乘积大小进行修剪。在剪枝的方法上，绪论中提到的经典框架\cite{han2015learning}、扩展框架\cite{gale2019state}和“彩票假说”\cite{frankle2018lottery}都无法在训练过程中有效减少资源的消耗，而静态稀疏训练\cite{molchanov2016pruning}对模型训练效果的损害较大。在动态稀疏训练中，随机增长权重的SET\cite{mocanu2018scalable}和按梯度绝对值大小增长权重的RigL\cite{evci2020rigging}都能有效对参数空间进行探索，从而动态寻找较为重要的连接或结构保留下来。除此之外，剪枝方法中还包括掩码学习\cite{huang2018learning,yamamoto2018pcas}和对权重本身施加各种惩罚\cite{gao2019vacl,kingma2015variational}等方法。



### \subsection{联邦学习中的网络剪枝}

现有的基于剪枝优化的联邦学习算法主要围绕剪枝的对象和剪枝的程度进行研究。PruneFL\cite{jiang2022model}提出了两阶段的剪枝策略和经验误差近似方法，通过最大化单位时间经验误差动态选择剪枝的参数。为了适应资源受限且异构的边缘设备，FedMP\cite{jiang2022fedmp}使用多臂老虎机根据每个客户端的计算能力动态自适应地决定剪枝率，并提出了基于模型残差的聚合策略。FedDUAP\cite{zhang2022fedduap}使用边缘设备和服务器上的数据动态更新模型，而服务器根据全局模型准确率和数据的非独立同分布的程度自适应调整全局的大模型的训练。Sub-FedAvg\cite{vahidian2021personalized}将混合剪枝技术应用到联邦学习的框架中，为不同的客户端提供不同的子网络，服务器收到子网络后只在模型参数交集处进行聚合。ZeroFL\cite{qiu2022zerofl}采用了高度稀疏操作来加速设备上的训练，但没有考虑客户端数据标签分布不同和带有噪声的情况。FedDST\cite{bibikar2022federated}同样利用了动态稀疏训练的思想，但FedDST主要是根据本地的非独立同分布数据自适应调整拓扑结构，没有考虑全局大模型的训练效果，也没有考虑标签噪声对训练效果产生的影响。



### \subsection{联邦学习中的非独立同分布}

在联邦学习中，数据由分布在世界各地的客户端采集和存储，因此数据之间的分布容易出现不一致的情况，相比独立同分布的数据，非独立同分布的数据会让模型的训练效果大幅度下降\cite{zhao2018federated}。FedAvg算法\cite{mcmahan2017communication}通过将客户端的数据量占总数据量的比例作为模型聚合的权重，适应了非独立同分布中最简单的数据量不同的情况。FedProx算法\cite{li2020federated}通过在客户端优化目标中添加近端项使得本地更新不要太过远离全局模型，在容忍系统异构性的前提下减少非独立同分布的影响。FedMo\cite{ozdayi2020improving}参考了动量梯度下降算法，将客户端的模型上传到服务器，并在服务器进行动量累加和模型更新。FedPer\cite{arivazhagan2019federated}将计算机视觉模型中位于前端的层作为基础层进行联邦训练，而对后端剩余的层作为个性化层进行本地训练。除此之外，还有一些方法\cite{chen2018federated,fallah2020personalized}将元学习与联邦学习进行结合，从而实现对客户端模型的个性化学习。



### \subsection{联邦学习中的贡献评估}

客户端的数据质量和贡献大小会影响全局大模型的训练效果。对于数据估值，常见的方法是根据联邦模型的测试准确率\cite{koh2017understanding}、数据量\cite{zhan2021survey}和数据分布的多样性\cite{xu2021validation}来度量客户端的数据价值，但是需要访问原始数据或统计信息。除此之外，客户端模型和全局模型的梯度相似度\cite{xu2021gradient}、基于模型参数的信息增益\cite{sim2020collaborative}也可以用来评估客户端的数据价值。然而，数据的价值并不总是等同于客户端对联邦模型训练的贡献，因为单个数据质量差的客户端可能会在客户端组合中带来互补的效果，因此需要考虑客户端对联邦合作所带来的边际价值增益。贡献评估的方法有将联邦排除某个客户端后的组合价值边际损失作为客户端贡献的留一法\cite{wang2019measure}，枚举所有不包含某个客户端时引入该客户端带来的边际贡献期望的夏普利值（shapley value）法\cite{van2018new}，以及追求客户端组合公平的最小核（least core）法\cite{yan2021if}。联邦学习中的贡献评估主要用于实现贡献公平性\cite{song2019profit}和均衡公平性\cite{yang2021federated}，本文则探索在客户端数据标签分布不同且带有噪声的情况下，贡献评估方法对于提升全局大模型训练效果的作用。



## \section{本文的论文结构与相关安排}

本文共分为三章，各章节内容安排如下：

第一章绪论。简单说明了本文的选题背景、意义与主要贡献，总结了国内外研究现状和相关工作。

第二章为基于剪枝优化的大模型联邦训练算法。首先从整体上介绍了大模型联邦训练框架的组成部分。然后分别介绍了基于动态拓扑的剪枝算法和基于贡献评估的聚合算法这两个重要模块的技术细节。

第三章为实验与结果。本章将先后展示剪枝算法消融实验及其超参数探究实验、聚合算法消融实验及其超参数探究实验、与其他联邦剪枝算法对比实验、剪枝率探究实验的实验设置和实验结果，并推出相关的实验结论。
