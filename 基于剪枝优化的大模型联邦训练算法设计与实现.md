# 基于剪枝优化的大模型联邦训练算法设计与实现

## 摘要

## 绪论

### 选题的背景与意义

近年来，深度学习领域涌现出越来越多规模庞大的模型，并且模型的参数量成指数级增长【On the Opportunities and Risks of Foundation Models】。以最近风靡全球的人工智能聊天机器人ChatGPT为例，其前身为大规模预训练语言模型GPT。2018年发布的GPT-1【Improving language understanding by generative pre-training】的参数量为1.17亿，数据量约为5GB；2019年发布的GPT-2【Language models are unsupervised multitask learners】参数量上升到15亿，预训练的数据量也激增到40GB；一年后，有着1750亿参数的GPT-3【Language models are few-shot learners】横空出世，其预训练的数据量更是达到了惊人的45TB。这些庞大的模型能够处理高复杂度和多模态的任务，并且在这些任务上能够取得相当高的精度。按照传统机器学习的观点，模型的复杂度需要与数据集的规模相匹配，模型过于复杂可能会导致过拟合现象的产生【机器学习】。然而，对于参数量远远多于传统机器学习模型的深度神经网络来说，参数量与测试准确率的关系并非如此。Mikhail Belkin等人的研究【Reconciling modern machine learning practice and the bias-variance trade-off】发现，随着模型容量从小到大，模型的测试准确率先增加后减小，而后再次增加，且逐渐刷新最优的测试准确率。深度学习的这种过参数化（over-parameterized）性质可以用大模型包含小模型的解空间来解释。因此，深度学习模型的容量越大，测试准确率和表达能力越好，能处理的任务也就越复杂。凭借着优越的性能，大模型相关领域的研究正得到众多科研工作者、工业界人士甚至是全社会的广泛关注。

对于一个规模庞大的深度学习模型的训练来说，充足且优质的数据是必不可少的。然而，对于一些特定的或者复杂的任务来说，训练数据往往不够充足，数据的质量也参差不齐。近些年来，边缘计算的发展促进了深度学习模型在各类终端设备上的部署和应用，边缘智能的提供方可以通过边缘设备采集数量庞大的训练数据，甚至利用原始数据直接在边缘设备上进行深度学习的训练。其中，仅利用边缘设备有限的数据难以训练强大的深度学习模型，而数据持有方出于隐私保护的考虑，往往会拒绝分享原始的数据。在Brendan McMahan等人于2017年提出的联邦学习【Communication-Efficient Learning of Deep Networks from Decentralized Data】的概念中，用户无需上传自己的数据，而是以迭代的方式，首先在本地用自己的数据训练模型，然后将本地的模型上传给云端，最后云端通过模型聚合得到一个强大的全局模型。在保护数据隐私的基础上，联邦学习中全局模型的训练相当于利用了众多边缘设备上的数据信息，因此可以成为一种重要的大模型训练的方式。

利用联邦学习的框架训练大模型并不是完美无缺的。首先，正如前面所述，大模型的参数数量是非常多的。对于如今有限的云边传输带宽，尤其是从终端到云端更加有限的上传带宽，过多的数据传输量会带来巨大的通信成本，影响网络通信的效率。其次，过多的模型参数也会导致训练过程中需要更多的计算和存储资源。对于大多数的物联网终端设备来说，其计算和存储能力远远弱于云端服务器的计算和存储能力。因此在联邦学习中，直接将云端的全局大模型通过有限带宽的网络传输给有限资源的边缘设备进行训练和推理是不切实际的。对于这一挑战，网络剪枝【Learning both weights and connections for efficient neural network】、知识蒸馏【Distilling the Knowledge in a Neural Network】、参数量化【BinaryConnect: Training Deep Neural Networks with binary weights during propagations】【Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1】等技术可以对规模庞大的深度学习模型进行压缩。其中，知识蒸馏通常需要云端有公共数据集，或者获取客户端学习的知识，而参数量化对硬件的要求更高，通常只在特定的硬件上更有效。因此，网络剪枝技术更加适应联邦学习框架重视隐私保护的特点，同时更加适应普遍的硬件条件。

网络剪枝是模型压缩中一类通用且有效的技术，但这并不意味着模型剪枝技术可以有效。首先，常见的模型剪枝框架是先训练、后修剪、再微调【Learning both weights and connections for efficient neural network】，或者在训练过程中逐渐修剪越来越多的权重并省略了微调的过程【The state of sparsity in deep neural networks】，又或者如彩票假说【The lottery ticket hypothesis: Finding sparse, trainable neural networks】在每轮迭代中对完整的模型进行修剪。然而，这些方法对于训练过程中的通信、计算和存储成本的优化非常有限。从另一个方面说，若采用静态拓扑的方式从随机初始化开始对模型进行剪枝，即使有效减少了模型的参数量，但训练的效果通常远远不如同等规模的完整密集模型【The difficulty of training sparse neural networks】【A topological insight into restricted boltzmann machines】。另外，Shiwei Liu等人的工作【Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training】

除此之外，分布在众多边缘设备上的训练数据集的质量也会对大模型联邦训练效果造成影响。联邦学习一个重要的特征是分布在不同终端上的数据常常不能满足独立同分布的性质，因为不同终端采集数据的方式、对象、能力不尽相同，因此不同终端之间的数据分布更有可能是非独立同分布的。

静态剪枝

非独立同分布和噪声数据、数据质量和贡献评估参与程度

为了解决上述提到的诸多挑战，本文提出了一个新颖的基于剪枝优化的大模型联邦训练算法。该算法的目标是训练一个性能良好的全局大模型，减少通信资源的消耗，减少边缘设备的计算量和存储需求，同时对低质量且带噪声的训练数据具备鲁棒性。具体来说，本文提出的大模型联邦训练算法利用基于动态拓扑的剪枝技术对全局大模型进行压缩，在尽可能保持训练效果的同时减少模型的参数量，本地训练完成后，利用基于贡献评估的聚合技术对分布在不同边缘设备上的稀疏模型进行聚合，根据边缘设备的贡献调整其在联邦训练中的参与程度。总的来说，本文的贡献可以总结为以下几个方面：

- 将基于动态拓扑的剪枝算法应用于联邦学习的框架中，减少大模型联邦训练算法的通信量、计算量和存储需求，并通过**消融实验**和**对比小型不剪枝模型**的训练效果验证了剪枝算法的有效性，还研究了**超参数**对全局大模型训练效果的影响。
- 将基于贡献评估的聚合算法应用于联邦学习的框架中，使得大模型联邦训练算法对低质量且带噪声的训练数据具备鲁棒性，并通过实验展示了**贡献评估方法的合理性**和**超参数**对全局大模型训练效果的影响，通过**消融实验**验证了聚合算法的有效性。
- 本文通过实验说明，**相比其他现有的联邦剪枝算法**，本文提出的大模型联邦训练算法能够用更少的通信、计算和存储开销在测试数据集上实现更优的准确率，同时减轻低质量且带噪声的训练数据对全局大模型训练效果的影响。
- 本文还通过实验研究了在边缘设备数据的各种分布下，剪枝算法中的**剪枝率**对全局大模型训练效果的影响，以及根据边缘设备的贡献程度**自适应剪枝**对全局大模型训练效果的影响。



### 国内外研究现状和相关工作

#### 联邦模型压缩

联邦量化、知识蒸馏、分割学习、预训练对比学习、剪枝

#### 网络剪枝

剪枝对象：非结构化剪枝、结构化剪枝

剪枝标准：权重大小标准、梯度幅度剪枝、全局或局部剪枝

剪枝方法：训练修剪和微调、扩展经典框架、初始化修剪（彩票假说）、稀疏训练（SET、RigL）、掩码学习、基于惩罚的方法

【彩票假说：The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks】

【SET：Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science】

【RigL：Rigging the Lottery: Making All Tickets Winners】

【及时自集成：Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training】

#### 联邦剪枝

PruneFL Sub-FedAvg FedDST FedDUAP ZeroFL FedMP

#### 联邦学习针对noniid和噪声数据的解决方法

FedAvg【Konecn ˇ y, J.; McMahan, H. B.; Yu, F. X.; Richt ´ arik, P.; ´ Suresh, A. T.; and Bacon, D. 2017. Federated Learning: Strategies for Improving Communication Efficiency. arXiv:1610.05492.  】

系统异构性+统计异质性FedProx【Federated Optimization in Heterogeneous Networks】

粗略更新仅在通信过程压缩模型更新，结构化更新直接在压缩后的表征上进行本地训练：【Federated Learning: Strategies for Improving Communication Efficiency】

#### 贡献评估

以往考虑公平性更多 目标不同

现有的基于剪枝优化的联邦学习算法主要围绕剪枝的对象和剪枝的程度进行研究。其中一些工作直接将现有的剪枝方法加入联邦训练的过程，或者预先设定相同的剪枝率，忽视了联邦学习中系统异构的场景。另外一些工作采用了自适应剪枝的方式，为不同的客户端设置不同的剪枝率，虽然这些算法在非独立同分布的数据中有不错的表现，但这些算法的设计回避了，容易造成模型训练的等问题。对基于剪枝优化的联邦学习算法进行研究，使其有效适应系统异构和数据异构的场景，是该研究方向的一个发展趋势。

### 本文的论文结构与相关安排

本文共分为六章，各章节内容安排如下：

第一章绪论。简单说明了本文章的选题背景与意义，总结了国内外研究现状和相关工作。       第二章为基于剪枝优化的大模型联邦训练框架。分为三块介绍了联邦剪枝的训练框架，分别是自适应剪枝模块、本地训练模块和异构模型的聚合模块，其中自适应剪枝模块中的剪枝率决策算法将在第三章中单独介绍。

第三章为剪枝率决策算法。首先对优化的问题进行定义，通过测量实验引入了问题场景和方法中的困难和挑战，例如效率与准确率的权衡、系统异构性和数据价值异构，最后为解决挑战提出剪枝率决策算法。

第四章为实验与结果。对本文提出的模型剪枝算法与FedMP、PruneFL、SynFL、UP-FL、FedProx、FlexCom等基线算法进行比较，比较的指标为测试集准确率和达到指定准确率所需要的训练时间。



## 基于剪枝优化的大模型联邦训练算法

### 大模型联邦训练框架



### 基于动态拓扑的剪枝算法

#### 动态剪枝

In-Time Over-Parameterization【Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training】

dropout的子采样和聚合效应：正则化效果【Improving neural networks by preventing co-adaptation of feature detectors  】

从头训练一个具有固定稀疏度（静态）的网络会导致性能下降，训练相同参数数量的密集网络会获得更好的结果【The lottery ticket hypothesis at scale】【Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization  】观察到静态稀疏训练收敛到比动态稀疏训练损失更高的解决方案

ERK的有效性【RigL：Rigging the Lottery: Making All Tickets Winners】

SET随机增长新连接，提高了small-dense的性能，但饱和度为75%，表明随即增长新连接的局限性

因为移除这些连接已被证明对损失的影响最小【Learning both weights and connections for efficient neural network】【Detecting dead weights and units in neural networks  】。 接下来，它激活具有高梯度的连接，因为这些连接有望最快地减少损失

与稀疏基元相结合，可以训练非常大的稀疏模型，否则是不可能的。

允许在整个训练时间内进行连续参数探索，而不是从密集和预训练的模型中继承权重，从而在时空流形中执行过参数化，这可以显着提高稀疏训练的可表达性。【Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training】

最近，彩票假设 (LTH) (Frankle & Carbin, 2019) 显示了从头开始训练子网络（稀疏训练）以匹配密集网络性能的可能性。 然而，这些“中奖彩票”是在完全密集的超参数化过程（迭代修剪完全融合网络）的指导下找到的，以及通过部分密集超参数化（初始化时修剪（Lee 等人， 2019；Wang 等人，2020；de Jorge 等人，2020)）或没有过度参数化（随机初始化的静态稀疏训练（Mocanu 等人，2016；Evci 等人，2019）））通常无法 以匹配其密集对应物所达到的准确性。 一个常识性的解释是，与密集训练相比，稀疏训练，尤其是在极高稀疏度下，不具有过参数化特性，因此可表达性较差。 解决这个问题的一种方法是利用从密集训练中学到的知识，例如 LTH（Frankle & Carbin，2019）。 虽然有效，但过度参数化密集训练所附带的计算成本和内存要求令人望而却步。

#### 非结构化剪枝

#### 按层剪枝

bias bn层不剪

#### 剪枝标准





### 基于贡献评估的聚合算法

夏普利值【Contributions to the Theory of Games】

已经应用于FL的公平性和激励机制：需要验证数据集或开销太大

用有界误差有效地近似【Gradient-Driven Rewards to Guarantee Fairness in Collaborative Machine Learning  】

贡献的定义：余弦梯度夏普利值

不一定有助于提升全局大模型的性能，因此实验验证

余弦值的含义

然而这种情况下低价值或者恶意参与方持有大量数据时会影响联邦模型训练效果 

非独立同分布时加速训练

带有噪声标签的参与者，降低噪声标签对联邦学习性能的影响





## 实验与结果

### 实验设置

#### 数据集

#### 数据划分方式

#### 模型

#### 分布式训练结构

#### 超参数



## 参考文献

On the Opportunities and Risks of Foundation Models

Improving language understanding by generative pre-training

Language models are unsupervised multitask learners

Language models are few-shot learners

机器学习

Reconciling modern machine learning practice and the bias-variance trade-off

Communication-Efficient Learning of Deep Networks from Decentralized Data

Learning both weights and connections for efficient neural network

Distilling the Knowledge in a Neural Network

BinaryConnect: Training Deep Neural Networks with binary weights during propagations

Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1

Shufflenet: An extremely efficient convolutional neural network for mobile devices

The state of sparsity in deep neural networks

The lottery ticket hypothesis: Finding sparse, trainable neural networks

The difficulty of training sparse neural networks

A topological insight into restricted boltzmann machines

## 附录A

## 致谢

