# [Survey] Privacy and Robustness in Federated Learning: Attacks and Defenses

![image-20221101204345196](D:\github\assets\20221208_1670498830.png)



## Privacy Attacks

- the gradients are derived from the participantsâ€™ private training data
- a learning model can be considered as a representation of the high-level statistics of the dataset
- target at **data** privacy

<img src="D:\github\assets\20221208_1670498837.png" alt="image-20221101204411118" style="zoom:50%;" />



## Defenses Against Privacy Attacks

<img src="D:\github\assets\20221208_1670498846.png" alt="image-20221101204453158" style="zoom:50%;" />

<img src="D:\github\assets\20221208_1670498852.png" alt="image-20221101205140960" style="zoom:67%;" />



## Poisoning Attacks

- compromise the **system** robustness
- untargeted poisoning attacks
  - data poisoning
  - model poisoning
- targeted poisoning attacks
- data poisoning attacks are generally less effective than model poisoning attacks   

<img src="D:\github\assets\20221208_1670498859.png" alt="image-20221101205159792" style="zoom:50%;" />



## Defenses Against Poisoning Attacks

![image-20221101205319092](D:\github\assets\20221208_1670498867.png)