ReLU减轻了困扰以往神经网络的梯度消失问题

当我们想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数。

sigmoid在隐藏层中已经较少使用， 它在大部分时候被更简单、更容易训练的ReLU所取代。

糟糕的初始化可能会导致我们在训练时遇到梯度爆炸或梯度消失。

当sigmoid函数的输入很大或是很小时，它的梯度都会消失。此外，当反向传播通过许多层时，除非我们在sigmoid函数的输入接近于零的地方，否则整个乘积的梯度可能会消失。

除了局部最小值之外，鞍点也是梯度消失的另一个原因。

梯度消失可能会导致优化停滞，重参数化通常会有所帮助。对参数进行良好的初始化也可能是有益的。

数值稳定性：模型初始化 正则化

分布偏移：标准化 Dropout

过拟合：正则化
