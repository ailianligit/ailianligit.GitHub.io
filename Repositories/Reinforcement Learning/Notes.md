on-policy 不使用旧数据，这使得它们的样本效率较弱。但这是有充分理由的：这些算法直接优化了你关心的目标：策略性能，并且从数学上计算出你需要策略数据来计算更新。因此，这一系列算法权衡了样本效率以支持稳定性。但您可以看到技术的进步（从 VPG 到 TRPO 再到 PPO）正在努力弥补样本效率的不足。

DDPG 和 Q-Learning 等算法是 off-policy 的，因此它们能够非常有效地重用旧数据。他们通过利用 Bellman 的最优方程来获得这种好处，可以使用任何环境交互数据训练 Q 函数以满足该方程（只要环境中的高回报区域有足够的经验）。但有问题的是，不能保证很好地满足贝尔曼方程会导致策略表现出色。从经验上讲，样本效率好，但缺乏保证使得这类算法可能变得脆弱和不稳定。TD3 和 SAC 是 DDPG 的后代，它们利用各种见解来缓解这些问题。

虽然两种回报公式之间的界限在 RL 形式主义中非常明显，但深度 RL 实践往往会模糊界限——例如，我们经常设置算法来优化未贴现回报，但在估计**价值函数**时使用折扣因子。

策略优化方法的主要优势在于它们是有原则的，从某种意义上说，您可以直接针对您想要的东西进行优化。这往往使它们稳定可靠。相比之下，Q-learning 方法只是通过训练满足自洽方程来间接优化代理性能，往往不太稳定。但是，Q-learning 方法在实际工作时获得了样本效率更高的优势，因为它们可以比策略优化技术更有效地重用数据。

尽管我们将其描述为损失函数，但它**并不是**监督学习中典型意义上的损失函数。与标准损失函数有两个主要区别。**1.数据分布取决于参数。**损失函数通常定义在与我们要优化的参数无关的固定数据分布上。这里不是这样，必须根据最新策略对数据进行采样。**2. 它不衡量性能。**损失函数通常评估我们关心的性能指标。在这里，我们关心的是预期回报，但我们的“损失”函数根本不近似这个，即使在预期中也是如此。这个“损失”函数只对我们有用，因为在当前参数下评估时，使用当前参数生成的数据，它具有负梯度的性能。但是在梯度下降的第一步之后，与性能没有更多的联系。这意味着对于给定的一批数据，最小化这个“损失”函数并不能保证提高预期回报。您可以将降低损失，但策略的效果可能会下降；事实上，它通常会。有时，深度 RL 研究人员可能会将这一结果描述为对一批数据的“过度拟合”策略。这是描述性的，但不应按字面意思理解，因为它不涉及泛化错误。我们提出这一点是因为 ML 从业者通常将损失函数解释为训练期间的有用信号——“如果损失下降，一切都很好。” 在策略梯度中，这种直觉是错误的，你应该只关心平均回报。损失函数没有任何意义。

策略梯度的一个关键问题是需要多少样本轨迹才能获得低方差样本估计。我们开始使用的公式包括与过去奖励成比例的强化动作的术语，所有这些都具有零均值但非零方差：因此，它们只会在策略梯度的样本估计中添加噪声。通过移除它们，我们减少了所需样本轨迹的数量。

最常见的baseline选择是on-policy value function。根据经验，该选择具有减少策略梯度样本估计方差的理想效果。这导致更快、更稳定的策略学习。从概念的角度来看，它也很有吸引力：它编码了这样一种直觉，即如果智能体得到了预期的结果，它应该对此“感到”中立。

动作的优势描述了它比其他动作平均好或差多少（相对于当前策略）。