## 介绍

- 自然语言处理的概念：自然语言处理是语言学、计算机科学和人工智能的交叉领域，涉及计算机与人类语言之间的交互，特别是如何对计算机进行编程以处理和分析自然语言数据。（目标是让计算机能够理解文档中的内容，包括文档中语言的上下文细微差别）

- 自然语言处理的挑战：
  - 歧义：词汇Lexical歧义、词性Part-of-speech歧义、结构歧义
  - 未知的语言现象：新词、人名、地名、新的含义、新的语言用法

- 自然语言处理的常见应用：机器翻译、文本生成、信息检索、问答和对话系统、知识图谱、自动汇总、情感分析

- 文本预处理：文本清洗（去除噪声）、分词、文本规范化、去除停用词、词干提取、词形还原、词性标注



## 文本分类

- 文本分类任务的定义和意义：给模型输入一个文档和一个固定的类别集合，需要该模型输出文档对应的类别。
- 朴素贝叶斯分类器的原理：词袋模型、类别的概率（语料库中的相对频率）、词袋假设、条件独立
- 朴素贝叶斯分类器的训练方法：先验概率和似然计算方法、未知词、停用词
- 文本分类的性能评估指标：
  - 精度：预测正例中真实正例的比例
  - 召回率：真实正例中预测正例的比例
  - 准确率：所有例子中预测正确的比例
  - F值



## N-gram 语言模型

- 语言模型的概念和目的：一个计算完整句子（单词序列）或即将出现的词的概率的模型。
- 基于n-gram和极大似然估计的语言模型计算方法：
  - 计算完整句子（单词序列）的概率：链式法则、马尔可夫假设



## 神经网络

- 神经网络的原理：非线性变换激活函数
- 神经网络的训练方法：前向传递、反向传播和链式法则、交叉熵损失函数
- 基于神经网络实现文本分类：
  - 输入层是情感Sentiment二元特征，输出层是0或1
  - 表示学习：使用词嵌入作为输入特征（投影层）
  - 文本的长度不同：填充和截断/句子嵌入（与词嵌入的维度相同）
- 神经语言模型的概念：在给定一些历史的情况下计算序列中下一个词的概率
- 神经语言模型的原理：投影层输入滑动窗口中先前的词嵌入，预测下一个词的概率
- 循环神经网络的原理：RNN单元共享参数
- 循环神经网络的训练方法：
  - 逐词往RNN中输入每个序列，并在每个时间步计算输出概率
  - 在每个时间步使用交叉熵计算损失函数
  - 在所有时间步计算平均损失
  - 反向传播计算梯度并更新网络参数
- 循环神经网络的优点：
  - 能够处理任意长度的序列
  - 能够获得任意先前时间步的信息
  - 对于更长的输入，模型的大小保持不变
  - RNN单元共享参数
- 循环神经网络的缺点：
  - 循环计算过程很慢
  - 实际上，获取远离现在时刻的信息是困难的



## 向量语义Semantic和嵌入

- 词向量的动机与意义：建立词义的计算模型；使得两个相似的词在语义Semantic空间中相邻；测试集中允许出现相似但与训练集中不同的词。
- 用TF-IDF获取词向量和文本向量的方法：文档d中词t的tf-idf值由文档d中词t的词频与词t的逆文档频率（集合中的文档总数除以出现词t的文档数）相乘得到。
- 基于TF-IDF计算词语和文本相似度的方法：两个词向量的余弦值越接近1表示两个词语的相似度越高。
- Word2Vec模型计算词向量的原理：预测而不是计数；自监督；训练一个分类器预测一个词是否有可能在附近出现，而不是计算一个词在另一个词附近出现的频率
- Word2Vec模型计算词向量的优点：使用短且密的词向量（机器学习中特征少避免过拟合、便于泛化、更容易捕获同义词）；维度小，训练速度快；考虑上下文，训练效果好；通用性强，适用于各种NLP任务
- Word2Vec模型的训练方法：
  - V个d维随机向量作为初始的词嵌入
  - 基于词嵌入相似度训练一个分类器
    - 从语料库中取同时出现的词对作为正例
    - 将不同时出现的词对作为负例
    - 通过缓慢调整所有的词嵌入来训练分类器去区分这些词以提升分类器的性能：
      - 输出：两个词向量的点积作为sigmoid函数的输入，得到这两个词向量为正样本或负样本的概率，估计中心词出现在上下文窗口中的概率。
      - 训练：调整词嵌入让正样本的相似度最大化，让负样本的相似度最小化。
    - 保留分类器的权重，将中心词嵌入和上下文词嵌入相加，作为学习得到的词嵌入
- Word2Vec中负采样的原理：从语料库中取同时出现的词对作为正例，将不同时出现的词对作为负例。
- Word2Vec中负采样的目的：加速了模型的计算，因为模型每次只需要更新采样的词的权重，不用更新所有的权重。同时保证了模型训练的效果，因为中心词只与它周围的词有关系，与遥远的词没有关系。



## 词性Part-of-speech和命名实体

- 词性标注的含义：
  - 用词性标记注释句子中的每个单词
  - 最低级别的句法Syntactic分析
- 词性标注的意义：
  - 能够用于其他NLP任务：句法Syntactic解析、机器翻译、情感分析、文本转语言
  - 能够用于语言学或语言分析的计算任务中：研究语言学上的变化、测量相似度或差异
- 命名实体识别的含义：
  - 命名实体是指可以用专有名词引用的任何事物，包括人名、地名、组织名、地缘政治实体
  - 查找构成专有名词的文本范围并标记实体的类型
- 命名实体识别的意义：用于情感分析、问答、信息提取等NLP任务中



## 神经机器翻译

- 神经机器翻译的原理：
  - 一种使用单个端到端神经网络做机器翻译的方法。
  - 序列到序列的模型是神经机器翻译的架构，编码器产生原句的语义编码，解码器基于编码生成目标句子。
  - 这个序列到序列的模型是一个条件语言模型：解码器在原句和目前的目标词的条件上对目标句子的下一个词做预测。
- 和统计机器翻译相比的优点：
  - 因为更流畅、更好地利用上下文，因此有更好的性能
  - 只需要端到端优化一个神经网络，而没有要单独优化的子组件
  - 因为不需要做特征工程，且对所有的语言对的方法相同，因此只需要更少的人力工作
- 和统计机器翻译相比的缺点：
  - 解释性差，难以维护
  - 难以控制，无法轻松指定翻译的规则，可能会产生安全问题
- 神经机器翻译的训练方法：
  - 得到一个大型的并行语料库
  - 序列到序列的模型以单个系统的形式被优化
  - 目标函数是所有时间步生成词的负对数概率的平均值
  - 反向传播是端到端的
- Free-running方法的原理：
  - 训练：将预测词传递到下一步
  - 测试：传递预测词到下一个时间步
- Teacher-forcing方法的原理：
  - 训练：将基本事实词传递到下一步
  - 测试：传递预测词到下一个时间步
- Free-running和teacher-forcing方法的区别：
  - Free-running模式的问题在于收敛速度慢，teacher-forcing收敛速度快
  - Free-running模式会导致模型不稳定，teacher-forcing模式下模型始终是稳定的
  - teacher-forcing模式下模型从未针对自己的错误进行训练，可能会对自己的错误不具备鲁棒性
- 贪婪：
  - 通过在解码器的每个步骤上对目标函数最大化生成目标句子，在每个步骤上采用最有可能的词
  - 贪婪策略解码无法撤销已经生成的词
- 束搜索：
  - 在解码器的每个步骤上，跟踪k个最有可能的部分翻译
  - 一个假设有一个对数概率的得分，得分都是负数，越高分越好
  - 不保证能够找到最优解，但比完全搜索更高效
- 注意力机制：
  - 信息瓶颈：编码RNN最后一个隐藏层的编码需要包含原句的所有信息
  - 在解码器的每一个步骤使用与编码器的直接连接来关注源序列的特定部分
  - 先对解码器的当前状态与编码器各个隐藏状态做点积得到注意力得分，然后做softmax生成注意力分布，使用注意力分布得到编码器隐藏状态的加权和，注意力输出主要包含了来自高度关注的隐藏状态的信息，最后将注意力输出与解码器的隐藏状态连接起来，用于计算最终的输出。
  - 优点：
    - 提升了神经机器翻译的性能；
    - 解决了神经机器翻译的信息瓶颈；
    - 因为提供了到遥远状态的捷径，因此减缓了梯度消失的问题
    - 提供了一些可解释性：通过观察注意力分布，可以发现解码器关注的重点



## 成分Constituency解析

- 句法结构Syntactic structure的含义：
  - 句子中的短语结构以及短语之间的层次句法关系
  - 两种语言结构：成分结构（短语结构语法）和依存结构
- 成分结构Constituency structure的含义：使用短语结构语法将词组织成嵌套的成分
- 句法解析的含义：识别一个句子并为其分配一个结构
- 成分解析的含义：识别一个句子并为其分配一个成分结构
- 上下文无关文法的含义：产生式左端总可以被终止符号或非终止符号自由替换，而无需考虑产生式左端出现的上下文。
- 上下文无关文法的作用：用于成分结构建模，可以表示大多数程序设计语言的语法。
- 概率上下文无关文法的意义：语法的解析存在二义性。除了常规的语法规则以外，还对每一条规则赋予了一个概率，对于每一棵生成的语法树，将其中所有规则的概率的乘积作为语法树的出现概率。从多种可能的语法树种找出最可能的一棵树，避免了语法解析的二义性。