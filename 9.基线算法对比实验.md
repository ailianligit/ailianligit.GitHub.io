## \section{大模型联邦训练算法的对比实验}

\label{sec:baseline}

cifar10_05_iid_grad_10_08_read_03.csv

0.64：python fedavg.py --dataset cifar10 --eta 0.01 --clients 10 --total-clients 10 --rounds 200 --distribution iid --sparsity 0.5 --grad-agg --Gamma 1 --tradeoff 0.8 --need-readjust --readjustment-ratio 0.3 --outfile test2.csv --device 2

cifar10_05_dirichlet_01_grad_10_08_read_03.csv

0.28：python fedavg.py --dataset cifar10 --eta 0.01 --clients 10 --total-clients 10 --rounds 200 --distribution dirichlet --sparsity 0.5 --beta 0.1 --grad-agg --Gamma 1 --tradeoff 0.8 --need-readjust --readjustment-ratio 0.3 --outfile test3.csv --device

cifar10_05_dirichlet_05_grad_10_08_read_03.csv

0.61：python fedavg.py --dataset cifar10 --eta 0.01 --clients 10 --total-clients 10 --rounds 200 --distribution dirichlet --sparsity 0.5 --beta 0.5 --grad-agg --Gamma 1 --tradeoff 0.8 --need-readjust --readjustment-ratio 0.3 --outfile test4.csv --device



介绍基线算法：

FedAvg、PruneFL、FedDST、FedProx

四种数据划分方式

三个数据集

三个指标



表格（准确率）：MNIST、CIFAR-10、CIFAR-100

图片：CIFAR-10 0.1 upload、CIFAR-10 0.1 flops、CIFAR-10 噪声 upload、CIFAR-10 噪声 flops



数据分布：iid 0.1 0.5 噪声

剪枝算法：是

剪枝率：0.5

动态剪枝：是

剪枝间隔：10

初始比例：0.5

聚合算法：是

权衡系数：0.5

统计指标：FLOPs和上传消耗

收敛速度：

曲线稳定：

最终精度：



FedAvg算法不对模型进行剪枝，服务器将模型发送给客户端后，客户端进行本地训练，然后在服务器进行加权聚合，聚合的权重为客户端的数据量占总数据量的比例。为了改善FedAvg算法在非独立同分布数据上的表现，FedProx算法对FedAvg算法进行了改进，在本地训练的目标函数上加上一个正则项，使得客户端的模型更新方向与所有客户端的平均更新方向尽可能接近，以此减轻数据分布不平衡给联邦训练带来的影响。PruneFL算法是较早将剪枝技术引入联邦学习的算法之一，PruneFL的框架分为两个部分，首先要对完整的模型进行初始剪枝，然后在训练过程中进一步动态剪枝，通过最大化单位时间经验误差来动态选择要对哪些参数进行剪枝。FedDST将动态稀疏训练的思想引入联邦学习中，根据客户端本地的非独立同分布数据自适应调整拓扑结构，希望训练出一个能够在所有客户端的测试数据集中表现良好的联邦模型。

表\ref{tab:final}中列出了本文提出的基于剪枝优化的大模型联邦训练算法与上述基线算法的对比结果。从表\ref{tab:final}中可以看出，在独立同分布的情况下，所有算法的测试准确率是接近的，然而，当样本标签变得不再平衡时，本文算法的优势开始出现，并在样本标签非常不平衡（$\beta=0.1$的狄利克雷分布）和带有不同比例的噪声时具有最大的优势。