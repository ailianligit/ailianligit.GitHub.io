# [FedSCR@TPDS 2021] FedSCR: Structure-Based Communication Reduction for Federated Learning

- 背景：Non-IID 属性会导致局部loss和权重差异很大，从而导致**精度下降**，以及聚合参数的巨大**通信开销**
- 实验：揭示在 IID 和 Non-IID 数据上训练时参数更新的演变
  - 实验结果表明大量参数在训练过程中**保持不变**
  - 位于**相同 filters 和 channels** 中的参数更有可能获得**相似的更新模式**
  - **Non-IID** 数据集的参数更新矩阵比 IID 设置的参数更新矩阵**稀疏**得多
  - 可以从 Non-IID 训练数据中**排除更多参数更新**，且对模型性能的影响轻微

![image-20221227134738001](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202212/20221227_1672120065.png)

![image-20221227134927011](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202212/20221227_1672130558.png)

![image-20221227135008659](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202212/20221227_1672130562.png)

- FedSCR：基于结构的减少通信的联邦学习算法
  - FedSCR 通过**聚合特定结构**（例如过滤器和通道）上的参数更新来识别重要的本地更新
  - 然后，它将聚合值**与阈值进行比较**：如果值超过阈值，我们将这些更新上传到全局服务器进行聚合
  - 与传统的结构化剪枝方法不同，FedSCR 保留了**完整的模型**，但减少了上传到全局服务器的**参数数量**，因此不需要重新训练和微调
  - 减少了通过网络传输的**参数数量**，同时保持了模型的**准确性**
- 自适应 FedSCR：为了解决由**非独立同分布**数据引起的性能下降
  - 非独立同分布：不同边缘设备的**参数更新幅度**和局部模型的**性能**差异很大
  - 测量全局模型和局部模型之间的权重差异，然后**动态**调整每个边缘学习器的有界阈值
  - 鼓励那些表现更好的边缘客户端向全局服务器上传更多参数，同时减少与表现不佳者同步的参数数量



## Introduction

- 一般来说，有三种方法可以减少通信开销：（1）减少边缘学习器和全局服务器之间传输的参数总数； （2）网络压缩，如量化；（3）降低沟通频率。 
- [12] 设计了 SkewScout，它使用模型旅行来估计由非 IID 数据分区引起的精度损失，并根据精度损失来控制通信的紧密度。 然而，为处理非独立同分布数据而引入的现有方法总是带来更多的通信开销或计算复杂性。 例如，当数据分区数量很多时，模型旅行会导致巨大的通信成本。



## Background & Challenges

- 相同通道/过滤器中的参数有更高的概率获得相似的参数更新模式。
- 相似性可以通过以下方式体现：1）在整个训练过程中，同一通道/过滤器中的大多数更新幅度几乎相等； 2）当一个通道/滤波器中的一个参数要收敛时，同一个通道/滤波器中的其他参数有更高的收敛概率。
- 图 2 显示了 VGG19 模型第二个卷积层在不同训练时期的参数更新幅度。 在图 2 中，同一列中的内核（3×3 块）在同一通道中。在相同的通道中发现较大的更新值（深色），并且几个通道的颜色在运行时慢慢变成白色。这些发现展示了在一个特定通道中这些参数的更新模式的相似性。相比之下，我们发现同一过滤器中的参数也有更高的可能性获得相似的参数更新模式。
- 在查看每个参数中更新值的变化时，我们观察到大量参数保持不变，导致更新矩阵稀疏，特别是对于非 IID 数据。主要原因之一是对于非 IID 数据，边缘设备可能只包含来自几个类别的图像，导致只有一小部分隐藏元素和相关特征提取被更新，而其他部分保持不变。 
- 这些发现表明，可以使用基于结构的聚合方法来识别无关紧要的更新。
- 在 Non-IID 数据上训练时，不同 worker 之间的参数更新模式是不同的；
- 而参数更新模式对于那些在 IID 数据集上训练的工人来说更可预测。
- 图 3 显示，使用 IID 数据集训练的模型获得了相似的参数更新模式，例如，对于 VGG19-CIFAR10-IID 示例（图 3a）的可视化图像，worker1 和 worker2 的更新矩阵几乎相同。当一个 worker 的通道/过滤器中的参数收敛时，使用相同 IID 数据集训练相同模型的其他 worker 将有更高的概率使相同参数收敛。
- 然而，使用 Non-IID 数据集训练的模型在这种现象中并不常见；不同worker之间的参数更新模式是多种多样的。
- 此外，我们发现在这些实验中，非 IID 数据集的更新矩阵比 IID 数据集稀疏得多。