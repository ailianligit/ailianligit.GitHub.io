# Federated Dynamic Sparse Training: Computing Less, Communicating Less, Yet Learning Better

## Abstract

联合学习 (FL) 可以将机器学习工作负载从云端分配到资源有限的边缘设备。 不幸的是，当前的深度网络不仅对于边缘设备上的推理和训练来说计算量太大，而且对于通过带宽受限的网络进行更新通信来说也太大了。 在本文中，我们开发、实施并通过实验验证了一种称为联合动态稀疏训练 (FedDST) 的新型 FL 框架，通过该框架可以部署和训练复杂的神经网络，并显着提高设备上计算和网络内通信的效率。 FedDST 的核心是一个从目标全网络中提取和训练稀疏子网络的动态过程。 有了这个方案，“一箭双雕：”而不是完整的模型，每个客户端都对自己的稀疏网络进行高效训练，并且只有稀疏网络在设备和云端之间传输。 此外，我们的结果表明，与固定的共享稀疏掩码相比，FL 训练期间的动态稀疏性更灵活地适应 FL 代理中的局部异质性。 此外，动态稀疏性自然地将“及时自集成效应”引入训练动态中，甚至在密集训练中也能提高 FL 性能。 在现实且具有挑战性的非 i.i.d. 在 FL 设置下，FedDST 在我们的实验中始终优于竞争算法：例如，在非 iid CIFAR-10 上的任何固定上传数据上限下，在给定相同的上传数据上限时，它比 FedAvgM 获得了 10% 的令人印象深刻的准确性优势； 即使 FedAvgM 的上传数据上限是 2 倍，准确度差距仍为 3%，进一步证明了 FedDST 的有效性。 代码位于：https://github.com/bibikar/feddst。



## Introduction

为了生成用于边缘设备推理的轻量级模型，人们在优化稀疏神经网络 (NN) 方面做出了重大努力（Gale、Elsen 和 Hooker，2019 年；Chen 等人，2020 年、2021 年；Ma 等人，2021 年）。 这些方法显着减少了推理延迟，但严重影响了训练所需的计算和内存资源。 彩票假说 (Frankle and Carbin 2018) 表明**密集 NN 包含稀疏匹配子网络**，这些子网络能够独立训练以达到完全准确度（Frankle 等人 2020）。 更多作品表明稀疏性可以在初始化时出现（Lee、Ajanthan 和 Torr 2019；Wang、Zhang 和 Grosse 2020）或者可以在训练期间以动态形式被利用（Evci 等人 2020）。

本文的总体目标是开发、实施和实验验证一种称为联合动态稀疏训练 (FedDST) 的新型 FL 框架，通过该框架可以部署和训练复杂的神经网络，同时显着提高设备上计算和内部计算的效率。 网络通讯。 FedDST 的核心是一种精心设计的动态稀疏训练联邦方法（Evci 等人，2020 年）。 FedDST 传输客户端的高度稀疏匹配子网络而不是完整模型，并允许每个客户端插入高效的稀疏分布式训练——从而“一石二鸟”。 更重要的是，我们发现 FL 训练期间的动态稀疏性比最先进的算法更能适应 FL 中的局部异质性。 动态稀疏性本身会导致及时的自集成效应 (Liu et al. 2021c) 并提高 FL 性能，甚至超过密集训练对应物，这与独立训练中的观察结果相呼应 (Liu et al. 2021c)。 我们将我们的贡献总结如下：

我们首次将动态稀疏训练引入联邦学习，从而无缝集成稀疏 NN 和 FL 范式。 我们的框架名为联合动态稀疏训练 (FedDST)，它利用稀疏性作为统一工具来节省通信和本地培训成本。

通过使用灵活的聚合方法，我们在 FedAvg 之上部署了 FedDST（McMahan 等人，2017 年），没有来自客户端的额外传输开销。 作为一般设计原则，我们的方法很容易扩展到其他 FL 框架，例如 FedProx (Li et al. 2018)。 此外，动态稀疏性的概念被发现可以适应局部异质性，并产生及时自集成的额外效果，即使在密集基线上也能提高 FL 性能。

大量实验表明，FedDST 显着提高了在病理非独立同分布数据分布的难题上的通信效率。 即使在这些非 iid 设置中，FedDST 在 CIFAR-10 上也比 FedAvgM（Hsu、Qi 和 Brown 2019）提高了 3% 的准确性，同时只需要一半的上传带宽。 我们还提供广泛的消融研究，显示 FedDST 对其参数的合理变化的稳健性。 这些结果表明稀疏训练是未来 FL 的“首选”选项。



## Related Work

据我们所知，只有两个作品解决了整个 FL 过程中的修剪问题。 PruneFL (Jiang et al. 2020) 依赖于在特定客户端选择的初始掩码，然后是类似 FedAvg 的算法，该算法每 ΔR 轮执行掩码重新调整。 然后通过稀疏矩阵操作进行训练。 在掩码调整轮次中，客户端需要上传完整的密集梯度，服务器使用这些梯度来形成聚合梯度 g。 选择掩码时，对应于可修剪权重的索引 j 按 gj2=tj 排序，其中 tj 是在网络中保留连接 j 的时间成本的估计。 通过测量具有各种稀疏性的一轮 FL 的时间成本，通过实验确定估计值 tj。 其次，与 PruneFL 不同，FedDST 是为具有挑战性的、现实的非 iid FL 设置而设计的。 出于这个原因，FedDST 的聚合和动态稀疏训练允许在客户端重新调整掩码，以 PruneFL 的自适应修剪标准无法实现的方式为非 iid 数据提供弹性。 特别是，PruneFL 的客户不会在第一轮之后重新调整掩模； 相反，它们会在某些回合将梯度传输到服务器，服务器使用梯度幅度和层时间来决定下一轮的掩码。 **由于数据异质性意味着无法在客户端之间直接比较梯度大小，因此梯度聚合本质上是不稳定的。** FedDST 通过仅使用客户端提交的权重大小和掩码“投票”来决定服务器上的掩码，从而提供稳定的更新。 我们的实验证明了 FedDST 与 PruneFL 在泛化能力方面的优势。

LotteryFL (Li et al. 2020) 从 LGFedAvg (Liang et al. 2020) 中汲取灵感，并允许客户端通过选择全球网络的本地子集来维护本地表示。 它也可以描述为 FedAvg 的扩展，其中每个客户端 c 维护一个单独的掩码 mc。 在每一轮 r 中，选定的客户端使用本地验证集评估他们的子网络 θr ⊙ mr c。 如果验证精度超过预定义的阈值并且客户端当前的稀疏度 kmr ck0 小于目标稀疏度，则执行幅度剪枝以产生新的掩码 mr c+1 并将相应的权重重置为其初始值。 将 FedDST 与 FL 中的先前修剪工作进行比较。 首先，与 **LotteryFL 生成的稀疏模型系统仅在本地数据集上表现良好**不同，FedDST 生成一个全局稀疏模型，随着时间的推移动态变化，在任何地方都表现良好。 FedDST 在客户端和服务器上执行掩码重新调整，但这些都是相对低开销的操作（逐层幅度修剪和梯度幅度增长）。 而且，FedDST 既不传输密集模型，也不传输梯度，甚至在最开始也不训练密集模型：这与 LotteryFL 形成鲜明对比，使 FedDST 明显更轻。

由于整个训练过程中固定的稀疏预算，FedDST 更新需要非常少的网络带宽，即使在最坏的情况下也是如此。 尽管 PruneFL 也为大多数轮次传输稀疏更新，但它每隔几轮就将全密集梯度传输到服务器以促进掩码重新调整。 LotteryFL 要求客户端传输密集模型，除非其精度达到一定阈值，因此密集传输发生得更频繁。



## Methodology

FedDST provides a fully federated approach to dynamic sparse training of NNs. In this method, we aim to learn a single model that provides good accuracy to all clients, while also consuming minimal compute, memory, and communication resources. Our method is designed to perform well even in pathologically non-iid settings.

