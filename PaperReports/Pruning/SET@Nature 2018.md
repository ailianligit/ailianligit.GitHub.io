# [SET@Nature 2018] Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science

## Abstract

通过深度学习在各个领域的成功，人工神经网络是目前最常用的人工智能方法之一。 从生物神经网络的网络特性（例如稀疏性、无标度性）中汲取灵感，我们认为（与一般做法相反）人工神经网络也不应具有全连接层。 在这里，我们提出了人工神经网络的稀疏进化训练，该算法在学习过程中将两个连续神经元层的初始稀疏拓扑（Erdős–Rényi 随机图）演化为无标度拓扑。 我们的方法在训练前用稀疏层代替人工神经网络的全连接层，以二次方式减少参数数量，而精度没有降低。 我们展示了我们对受限玻尔兹曼机、多层感知器和卷积神经网络的主张，用于在 15 个数据集上进行无监督和监督学习。 我们的方法有可能使人工神经网络扩展到超出目前可能的范围。



## Introduction

人工神经网络 (ANN) 是当今最成功的人工智能方法之一。 人工神经网络已在各个领域取得重大突破，例如粒子物理学1、深度强化学习2、语音识别、计算机视觉等3。 通常，人工神经网络具有完全连接的神经元层 3，其中包含大部分网络参数（即加权连接），导致连接数量与其神经元数量成二次方。 反过来，由于计算限制，网络大小受到严重限制。

与 ANN 相比，生物神经网络已被证明具有稀疏（而不是密集）拓扑4,5，并且还具有有助于提高学习效率的其他重要属性。 这些已在参考文献中进行了广泛研究。 6 并包括无标度 7（详见方法部分）和小世界 8。 尽管如此，人工神经网络还没有进化到模仿这些拓扑特征 9、10，这就是为什么它们在实践中会导致非常大的模型。 先前的研究表明，在训练阶段之后，ANN 模型最终得到的权重直方图在零 11、12、13 附近达到峰值。 此外，在我们之前的工作 14 中，我们观察到类似的事实。 然而，在最先进的机器学习中，稀疏拓扑连接仅作为训练阶段的结果进行追求，它仅在推理阶段才有好处。

在最近的一篇论文中，我们介绍了复杂玻尔兹曼机 (XBM)，它是受限玻尔兹曼机 (RBM) 的一种稀疏变体，采用稀疏无标度拓扑构想 10。 XBM 优于其完全连接的 RBM 对应物，并且在训练和推理阶段都更快。 然而，基于固定的稀疏模式，XBM 可能无法正确建模数据分布。 为了克服这一限制，在本文中，我们引入了一种稀疏进化训练 (SET) 程序，该程序考虑了数据分布并创建了适合替换任何类型 ANN 中的完全连接的二分层的稀疏二分层。

SET 受到进化方法的自然简单性的广泛启发，这些方法在我们之前关于进化函数近似的工作中得到了成功探索。 参考文献中已经为网络连接探索了相同的进化方法。 16，以及深度神经网络的层架构 17。 通常，在生物大脑中，进化过程分为四个层次：世代时间尺度的系统发育、每日（或每年）时间尺度的个体发育、秒到天尺度的表观遗传以及毫秒到秒尺度的推理18。 解决所有这些级别的经典示例是增强拓扑的神经进化 (NEAT)19。 简而言之，NEAT 是一种进化算法，旨在针对给定任务优化 ANN 的参数（权重）和拓扑结构。 它从节点和链接很少的小型 ANN 开始，逐渐考虑添加新的节点和链接以生成更复杂的结构，以提高性能。 虽然 NEAT 已经显示出一些令人印象深刻的实证结果 20，但在实践中，NEAT 及其大多数直接变体由于其非常大的搜索空间而难以扩展。 据我们所知，它们只能解决比目前最先进的深度学习技术解决的问题小得多的问题，例如 从大图像的原始像素数据进行对象识别。 在参考。 21 日，由于搜索空间仍然很大，Miconi 尝试结合使用 NEAT 类原则（例如添加、删除）和随机梯度下降 (SGD) 来针对小问题训练循环神经网络。 最近在参考文献中。 22,23，已经表明，进化策略和遗传算法分别可以成功训练具有多达 400 万个参数的 ANN，作为 DQN2 的可行替代方案，用于强化学习任务，但它们需要 700 多个 CPU 才能做到这一点。 为了避免陷入相同类型的可扩展性问题，在 SET 中，我们专注于使用两个世界中最好的（即传统的神经进化和深度学习）。 例如，仅在连接的表观遗传尺度上进化以产生稀疏的自适应连接、具有固定数量的层和神经元的结构化多层架构以获得易于通过标准训练算法训练的 ANN 模型，例如 新加坡元等。

在这里，我们声称必须从 ANN 设计阶段开始追求拓扑稀疏性，这会导致连接大幅减少，进而提高内存和计算效率。 我们展示了人工神经网络如何在稀疏连接层上表现出色。 我们发现，使用 SET 训练的稀疏连接层可以替换 ANN 中的任何完全连接层，而不会降低准确性，同时即使在 ANN 设计阶段（训练前）也具有二次方更少的参数。 这导致内存需求减少，并可能导致两个阶段（即训练和推理）的计算时间以二次方更快。 我们展示了我们对三种流行的 ANN 类型（RBM、多层感知器 (MLP) 和卷积神经网络 (CNN)）、两种类型的任务（监督和无监督学习）以及 15 个基准数据集的主张。 我们希望我们的方法将使拥有数十亿个神经元和进化拓扑结构的 ANN 能够处理复杂的现实世界任务，而这些任务使用最先进的方法是难以处理的。



## Results

### SET method

使用 SET，二分 ANN 层从随机稀疏拓扑（即 Erdös–Rényi 随机图 24）开始，在训练阶段通过随机过程演变为无标度拓扑。 值得注意的是，这个过程不必结合任何约束来强制无标度拓扑。 但我们的进化算法不是任意的：它遵循现实世界复杂网络（如生物神经网络和蛋白质相互作用网络）中发生的现象。 从 Erdős–Rényi 随机图拓扑开始，经过几千年的自然演化，网络最终具有更结构化的连通性，即无标度 7 或小世界 8 拓扑。

然而，这种随机生成的拓扑结构可能不适合 ANN 模型试图学习的数据的特殊性。 为了克服这种情况，在训练过程中，在每个训练时期之后，移除 SCk 的最小正权重和最大负权重的分数 ζ。 这些移除的权重是最接近零的权重，因此我们预计它们的移除不会显着改变模型性能。 例如，这已在参考文献中显示。 13,25 使用更复杂的方法去除不重要的权重。 接下来，为了让 SCk 的拓扑演化以适应数据，将等于先前移除的权重的数量的新随机连接数量添加到 SCk。 这样，SCk中的连接数在训练过程中保持不变。 训练结束后，我们保持 SCk 的拓扑结构为最后一步去除权重后得到的拓扑结构，不添加新的随机连接。 为了更好地说明这些过程，我们做以下类比。 如果我们假设连接是随时间演化的实体，那么最不重要的连接的移除大致对应于自然进化的选择阶段，而新连接的随机添加大致对应于自然进化的变异阶段 .

值得强调的是，在构思 SET 过程的初始阶段，我们根据自己的直觉引入了每个训练时期后的权重移除和权重添加步骤。 然而，在准备本文的最后阶段，我们发现 SET 与生物大脑中发生的一种现象有相似之处，称为睡眠期间的突触收缩。 这种现象已在最近的两篇论文 26、27 中得到证实。 简而言之，研究发现，在睡眠期间大脑中最弱的突触收缩，而最强的突触保持不变，这支持了睡眠的核心功能之一是重新正常化清醒时整体突触强度增加的假设 27。 通过类比，在某种程度上，这就是 SET 过程中 ANN 也会发生的情况。

我们在三种类型的 ANNs、RBMs28、MLPs 和 CNNs3 中评估 SET（所有这三种在方法部分都有详细说明），以试验无监督和监督学习。 总的来说，我们在 15 个基准数据集上评估了 SET，如表 1 所详述，涵盖了使用 ANN 的广泛领域，例如生物学、物理学、计算机视觉、数据挖掘和经济学。 我们还结合两种不同的训练方法来评估 SET，即对比散度 29 和 SGD3。