# [RigL@ICML 2020] Rigging the Lottery: Making All Tickets Winners

## Abstract

由于空间或推理时间限制，许多应用程序需要稀疏神经网络。 关于训练密集网络以产生用于推理的稀疏网络的大量工作，但这将最大可训练稀疏模型的大小限制为最大可训练密集模型的大小。 在本文中，我们介绍了一种在整个训练过程**中使用固定参数计数和固定计算成本**训练稀疏神经网络的方法，而不会牺牲相对于现有密集到稀疏训练方法的准确性。 我们的方法通过使**用参数大小和不频繁的梯度计算**在训练期间更新稀疏网络的拓扑结构。 我们表明，与现有技术相比，这种方法需要更少的浮点运算 (FLOP) 来达到给定的精度水平。 我们在各种网络和数据集上展示了最先进的稀疏训练结果，包括 ResNet-50、Imagenet-2012 上的 MobileNet 和 WikiText-103 上的 RNN。 最后，我们提供了一些见解，说明为什么允许拓扑在优化过程中发生变化可以克服拓扑保持静态*时遇到的局部最小值。



## Introduction

稀疏神经网络的参数和浮点运算 (FLOP) 效率现已在各种问题上得到很好的证明（Han 等人，2015 年；Srinivas 等人，2017 年）。 多项工作表明，对于递归神经网络 (RNN)（Kalchbrenner 等人，2018 年）和卷积神经网络 (ConvNets)（Park 等人，2016 年；Elsen 等人，2019 年），使用稀疏性可以加快推理时间。 目前，最准确的稀疏模型是通过至少需要在内存和 FLOP 方面训练密集模型的成本的技术获得的（Zhu & Gupta，2018 年；Guo 等人，2016 年），有时甚至更多 （Molchanov 等人，2017 年）。

这种范式有两个主要限制。 首先，稀疏模型的最大尺寸被限制为可以训练的最大密集模型； 即使稀疏模型的参数效率更高，我们也不能使用剪枝来训练比最大可能的密集模型更大、更准确的模型。 二是效率低下； 必须对零值参数或在推理过程中将为零的参数执行大量计算。 此外，目前最好的修剪算法的性能是否是稀疏模型质量的上限仍然未知。 大风等。 (2019) 发现三种不同的密集到稀疏训练算法都实现了大致相同的稀疏性/准确性权衡。 然而，这远不能证明没有更好的性能是可能的。

彩票假说 (Frankle & Carbin, 2019) 假设如果我们能找到一个具有迭代修剪功能的稀疏神经网络，那么我们就可以通过从原始初始条件开始，从头开始训练该稀疏网络，达到相同的准确度水平。 在本文中，我们介绍了一种无需“幸运”初始化即可训练稀疏模型的新方法； 出于这个原因，我们将我们的方法称为“The Rigged Lottery”或 RigL†。 我们做出以下具体贡献：

我们介绍了 RigL——一种用于训练稀疏神经网络同时保持与网络密度成正比的内存和计算成本的算法。

我们对 RigL 在计算机视觉和自然语言任务上进行了广泛的实证评估。 我们表明，对于给定的计算成本，RigL 实现了比以前所有技术更高的质量。

我们展示了令人惊讶的结果，即 RigL 可以找到比当前最好的密集到稀疏训练算法更准确的模型。

我们研究了稀疏神经网络的损失情况，并深入了解了为什么允许非零权重的拓扑结构在训练辅助优化过程中发生变化。



## Related Work

寻找稀疏神经网络的研究可以追溯到几十年前，至少可以追溯到 Thimm 和 Fiesler（1995 年），他们得出的结论是，基于幅度的修剪权重是一种简单而强大的技术。 Strom ¨ (1997) 后来引入了重新训练先前修剪过的网络以提高准确性的想法。 韩等。 (2016b) 更进一步，引入了多轮幅度修剪和再训练。 然而，这是相对低效的，在删除 20% 的连接以达到 90% 的最终稀疏度时需要十轮再训练。 为了克服这个问题，Narang 等人。 (2017) 引入了渐进修剪，在单轮训练过程中慢慢移除连接。 Zhu & Gupta (2018) 改进了该技术，以最大限度地减少所需的超参数选择量。

还提出了多种不基于幅度修剪的方法。 Mozer & Smolensky (1989), LeCun 等人。 (1990) 和 Hassibi & Stork (1993) 是一些早期的例子，但对于现代神经网络来说是不切实际的，因为它们使用来自 Hessian 的信息来修剪训练有素的网络。 最近的工作包括 L0 正则化 (Christos Louizos, 2018)、Variational Dropout (Molchanov et al., 2017)、动态网络手术 (Guo et al., 2016)、Discovering Neural Wirings (Wortsman et al., 2019)、灵敏度驱动 正则化（Tartaglione 等人，2018 年）。 大风等。 (2019) 检查了幅度修剪、L0 正则化和变分丢弃，并得出结论，它们在 ResNet-50 和 Transformer 架构上都实现了大致相同的精度与稀疏性权衡。

还有一些结构化剪枝方法试图移除通道或神经元，从而使生成的网络更加密集并且可以轻松加速（Dai 等人，2018 年；Neklyudov 等人，2017 年；Christos Louizos，2018 年）。 我们在附录 B 中将 RigL 与这些最先进的结构化修剪方法进行了比较。我们表明，我们的方法需要的资源要少得多，并且可以找到需要更少 FLOPs 运行的更小的网络。

据我们所知，第一种在整个训练过程中允许稀疏性的训练技术首先在稀疏进化训练 (SET) 中引入（Mocanu 等人，2018 年）。 权重根据修剪中使用的标准幅度标准进行修剪，并随机添加回去。 该方法简单，在实践中取得了合理的性能。 在此之后，Deep Rewiring (DeepR)（Bellec 等人，2018 年）通过在参数空间中随机游走来增强随机梯度下降 (SGD)。 此外，在初始化连接时随机分配一个预定义的符号； 当优化器通常会翻转符号时，权重设置为 0，并且随机激活新权重

动态稀疏重新参数化 (DSR)（Mostafa & Wang，2019）引入了允许参数预算在模型的不同层之间移动的想法，从而允许非均匀稀疏性。 这允许模型将参数分布在最有效的地方。 不幸的是，所考虑的模型大多是卷积网络，因此这种参数重新分配的结果（即降低早期层的稀疏性并增加后期层的稀疏性）具有增加 FLOP 计数的总体效果，因为空间大小是 在早期层中最大。 Sparse Networks from Scratch (SNFS)（Dettmers & Zettlemoyer，2019）介绍了使用每个参数的动量作为用于增加权重的标准的想法，并证明它可以提高测试准确性。 与 DSR 一样，它们允许每一层的稀疏性发生变化，并专注于一个常数参数，而不是 FLOP、预算。 重要的是，该方法需要在每次迭代时计算梯度并更新模型中每个参数的动量，即使是那些为零的参数。 这会导致大量的整体计算。 此外，根据模型和训练设置，完整动量张量所需的存储空间可能过高。 Single-Shot Network Pruning (SNIP)（Lee 等人，2019）试图通过一次剪枝找到一个初始掩码，并使用参数的显着性得分来决定保留哪些参数。 修剪后，训练继续使用这个静态稀疏网络。 表 1 总结了不同稀疏训练技术的特性。

还有一系列调查彩票假说的工作（Frankle & Carbin，2019）。 弗兰克等人。 (2019) 表明，必须削弱该公式才能应用于更大的网络，例如 ResNet-50（He 等人，2015）。 在大型网络中，必须使用经过数千次优化步骤后的值来代替原始初始化。 (Zhou et al., 2019) 表明“中彩票”甚至在训练开始之前就获得了非随机的准确度。 尽管使用彩票训练具有固定稀疏掩码的稀疏神经网络的可能性很有趣，但尚不清楚是否有可能从头生成此类初始化（对于掩码和参数）。



## Rigging The Lottery

我们的方法 RigL 在图 1 中进行了说明，并在算法 1 中进行了详细说明。RigL 从一个随机稀疏网络开始，并且在规则间隔的间隔内，它根据连接的大小删除一部分连接，并使用瞬时梯度信息激活新连接。 更新连接后，继续使用更新后的网络进行训练，直到下一次更新。 我们算法的主要部分，稀疏分布，更新计划，丢弃标准，增长标准，以及为每个考虑的各种选项，在下面解释。



## Empirical Evaluation

我们的实验包括在 ImageNet-2012（Russakovsky 等人，2015 年）和 CIFAR-10（Krizhevsky，2009 年）数据集上使用 CNN 进行图像分类，以及在 WikiText-103 数据集（Merity 等人，2016 年）中使用 RNN 进行基于字符的语言建模 ). 我们将所有实验重复 3 次并报告平均值和标准偏差。 我们使用 TensorFlow 模型修剪库 (Zhu & Gupta, 2018) 作为我们的修剪基线。 可以在 github.com/googleresearch/rigl 上找到我们方法的 Tensorflow（Abadi 等人，2015 年）实现以及我们模型的其他三个基线（SET、SNFS、SNIP）和检查点。 您还可以在 https://github.com/varun19299/riglreproducibility（Sundar & Dwaraknath，2021 年）上查看关于我们工作的再现性报告。

对于所有动态稀疏训练方法（SET、SNFS、RigL），除非另有说明，否则我们使用相同的更新计划，其中 ΔT = 100 和 α = 0:3。 相应的超参数扫描可以在第 4.4 节中找到。 我们将 SNFS 的动量值设置为 0.9 并研究附录 D 中的其他值。我们观察到在训练结束之前停止掩码更新会产生更好的性能； 因此，我们将 ImageNet-2012 的 Tend 设置为 25k，将 CIFAR-10 训练的 Tend 设置为 75k，这大约相当于完整训练的 3/4。

用于训练密集网络的默认训练步骤数对于具有动态连接的稀疏训练可能不是最佳的。 在我们的实验中，我们观察到稀疏训练方法从增加的训练步骤中受益匪浅。 当将训练步数增加一个因子 M 时，学习率计划的锚时期和掩码更新计划的结束迭代也按相同的因子缩放； 我们用下标表示这种缩放比例（例如 RigLM×）。

此外，在附录 B 中，我们将 RigL 与结构化修剪算法进行了比较，在附录 E 中，我们表明 RigL 找到的解决方案不是彩票。

### 4.1. ImageNet-2012 Dataset

在本节的所有实验中，我们使用带有动量的 SGD 作为我们的优化器。 我们将优化器的动量系数设置为 0.9，L2 正则化系数设置为 0.0001，标签平滑（Szegedy 等人，2016）设置为 0.1。 学习率计划从线性预热开始，在第 5 轮达到最大值 1.6，然后在第 30、70 和 90 轮下降 10 倍。我们以 4096 的批量大小训练网络 32000 步，这 大致对应于 100 个训练周期。 我们的训练管道使用标准数据增强，其中包括随机翻转和裁剪。

#### 4.1.1. RESNET-50

图 2-右上总结了各种方法在训练 80% 稀疏 ResNet-50 时的性能。 我们还训练具有等效参数数的小型密集网络。 除非另有说明，否则所有稀疏网络都使用均匀的逐层稀疏分布和余弦更新计划（α = 0:3，ΔT = 100）。 总的来说，我们观察到所有方法的性能都随着训练时间的增加而提高； 因此，对于每种方法，我们都使用原始训练步骤的 5 倍进行扩展训练。

正如 Gale 等人所指出的。 (2019)，Evci 等人。 (2019)，Frankle 等人。 (2019) 和 Mostafa & Wang (2019)，从头开始训练具有固定稀疏度（**静态**）的网络会导致性能下降。 训练具有相同参数数量的密集网络（**Small-Dense**）比 Static 获得更好的结果，但无法匹配动态稀疏模型的性能。 **SET** 提高了 Small-Dense 的性能，但饱和度约为 75%，表明随机增长新连接的限制。 使用梯度信息来增加新连接的方法（RigL 和 SNFS）获得了更高的准确度，但 **RigL** 达到了最高的准确度，并且始终需要比其他方法更少的 FLOP。

鉴于不同的应用程序或场景可能需要限制用于推理的 FLOP 数量，我们研究了我们的方法在各种稀疏级别的性能。 如前所述，我们方法的一个优势是它的资源需求在整个训练过程中是恒定的，我们可以选择适合我们的训练和/或推理约束的稀疏程度。 在图 2 的右下角，我们展示了我们的方法在不同稀疏度下的性能，并将它们与 (Gale et al., 2019) 的修剪结果进行比较，后者使用 1.5 倍的训练步骤，相对于原始的 32k 次迭代。 为了对 FLOPs 进行公平比较，我们将所有其他方法的学习计划扩展了 5 倍。 请注意，即使在扩展训练之后，与修剪方法相比，使用 RigL 训练稀疏网络所需的 FLOPs 更少‡。

RigL，我们的具有恒定稀疏分布的方法，在所有稀疏级别上都超过了基于幅度的迭代修剪的性能，同时需要更少的 FLOPs 来训练。 使用 Erdos-Renyi-Kernel (ERK) 稀疏分布的稀疏网络获得了更高的性能。 例如，稀疏度为 96.5% 的 ResNet-50 达到了惊人的 72.75% Top-1 准确度，比 (Gale et al., 2019) 报告的扩展幅度修剪结果高出约 3.5%。 如前所述，较小的密集模型（具有相同数量的参数）或具有静态连接的稀疏模型无法在可比较的水平上执行。

图 2-left 显示了稀疏训练方法的更细粒度比较。 使用均匀稀疏分布且其 FLOP/内存足迹直接与 (1-S) 成比例的方法被放置在表的第一个子组中。 第二个子组包括 DSR 和具有 ERK 稀疏分布的网络，它们需要更多的 FLOPs 来使用相同的参数计数进行推理。 最后一个子组包括需要空间和工作量与训练密集模型成比例的方法。

#### 4.1.2. MOBILENET

MobileNet 是一种紧凑的架构，在资源受限的环境中表现非常出色。 由于其具有可分离卷积的紧凑性，众所周知，在不影响性能的情况下很难进行稀疏化（Zhu & Gupta，2018）。 在本节中，我们将我们的方法应用于 MobileNet-v1（Howard 等人，2017 年）和 MobileNet-v2（Sandler 等人，2018 年）。 由于其参数数量少，我们保持第一层和深度卷积密集。 我们使用 ERK 或均匀稀疏分布来稀疏化剩余层。 我们在本节中计算修剪层的稀疏度分数，实际稀疏度（当包括第一层和深度卷积时）略低于报告值（即 75、85、90、95% 的 74.2、84.1、89、94） 稀疏性）。

图 3 显示了使用 RigL 训练的稀疏 MobileNet 的性能以及基线。我们扩展了本节中所有运行的训练（原始步数的 5 倍）。 RigL 训练 75% 的稀疏 MobileNets 而没有性能损失。 在这一点之后性能开始下降，尽管 RigL 始终以很大的优势获得最好的结果。

图 2-右上和图 3-左显示稀疏模型比具有相同参数数量的密集模型更准确，证实了 Kalchbrenner 等人的结果。 (2018)。 为了进一步验证这一点，我们训练了一个宽度乘数为 1.98 且稀疏度为 75% 的稀疏 MobileNet-v1，它具有与密集基线相同的 FLOP 和参数计数。 使用 RigL 训练该网络可将 Top-1 准确度绝对提高 4.3%，这证明了稀疏网络在提高广泛使用的密集模型性能方面的令人兴奋的潜力。

### 4.2. Character Level Language Modeling

大多数先前的工作只检查了视觉网络的稀疏训练§。 为了充分理解这些技术，检查不同数据集上的不同架构非常重要。 Kalchbrenner 等人。 (2018) 发现稀疏 GRU（Cho 等人，2014）在建模语音方面非常有效，但是他们使用的数据集不可用。 我们选择具有相似特征的代理任务（数据集大小和词汇量大小大致相同）——在公开可用的 WikiText-103（Merity 等人，2016 年）数据集上进行字符级语言建模。

我们的网络包含维度为 128 的共享嵌入，词汇量为 256，状态大小为 512 的 GRU，GRU 状态的读出由宽度分别为 256 和 128 的两个线性层组成。 我们使用 Adam 优化器训练具有交叉熵损失的下一步预测任务。 其余超参数在附录 I 中报告。

在图 4-left 中，我们在训练结束时报告了各种解决方案每步的验证位。 对于每种方法，我们执行扩展运行以查看每种方法的性能如何随着训练时间的增加而扩展。 如前所述，SET 的性能比其他动态训练方法差，并且其性能仅随着训练时间的增加而略有提高。 另一方面，随着训练步骤的增加，RigL 和 SNFS 的性能不断提高。 尽管 RigL 超过了其他稀疏训练方法的性能，但它无法匹配此设置中修剪的性能，突出了未来工作的重要方向。

### 4.3. WideResNet-22-2 on CIFAR-10

我们还评估了 RigL 在 CIFAR-10 图像分类基准上的性能。 我们使用宽度乘数 2 训练 22 层的 Wide Residual Network (Zagoruyko & Komodakis, 2016) 250 个时期（97656 步）。 学习率从 0.1 开始，每 30,000 次迭代按比例缩小 5 倍。 我们使用 5e-4 的 L2 正则化系数、128 的批量大小和 0.9 的动量系数。 我们使用 RigL (ΔT = 100) 的默认掩码更新间隔和默认的 ERK 稀疏分布。 其他掩码更新间隔和稀疏分布的结果产生类似的结果。 这些可以在附录 J 中找到。

RigL 对于各种稀疏度级别的最终精度如图 4 右侧所示。 密集基线获得94.1%的测试准确率； 令人惊讶的是，一些 50% 的稀疏网络比密集基线具有更好的泛化能力，证明了稀疏性的正则化方面。 随着稀疏性的增加，我们看到**静态和修剪解决方案之间存在性能差距**。 训练静态网络的时间更长似乎对最终性能的影响有限。 另一方面，RigL 只用训练所需资源的一小部分来匹配修剪的性能。

### 4.4. Analyzing the performance of RigL

在本节中，我们研究了稀疏分布和更新计划对我们方法性能的影响。 SET 和 SNFS 的结果相似，并在附录 C 和 F 中进行了讨论。此外，我们研究了稀疏 ResNet-50s 的能量景观，并表明 RigL 提供的动态连接有助于避免静态训练发现的次优解决方案。

稀疏分布的影响：图 5-left 显示了稀疏分布如何影响使用 RigL 训练的稀疏 ResNet-50s 的最终测试精度。 **Erdos-R ˝ enyi-Kernel ´ (ERK) 的性能始终优于其他两个分布**。 ERK 通过降低稀疏度自动将更多参数分配给参数较少的层¶。 这种重新分配似乎对于在高稀疏度水平上保持网络容量至关重要，其中 ERK 的性能优于其他分布。 尽管性能更好，**但与均匀分布相比，ERK 分布需要大约两倍的 FLOP**。 这突出了准确性和计算效率之间有趣的权衡，其中通过增加评估模型所需的 FLOP 数量来获得更好的性能。 当比较两个具有相同稀疏性（参数计数）的网络时，这种权衡还强调了报告非均匀稀疏性以及各自的 FLOP 的重要性。

更新计划和频率的影响：在图 5-右侧，我们评估了我们的方法在更新间隔 ∆T 2 [50; 100； 500; 1000] 和初始下降分数 α 2 [0:1; 0:3； 0:5]。 当掩码每 100 次迭代更新一次且初始下降分数为 0.3 或 0.5 时，可获得最佳精度。 值得注意的是，即使更新间隔不频繁（例如每 1000 次迭代），RigL 的性能也超过 73.5%。

动态连接的影响：Frankle 等。 (2019) 和 Mostafa & Wang (2019) **观察到静态稀疏训练收敛到比动态稀疏训练损失更高的解决方案**。 在图 6-left 中，我们检查了位于通过静态稀疏训练找到的解决方案和通过修剪找到的解决方案之间的损失情况，以了解前者是否位于与后者隔离的盆地中。 在两者之间执行线性插值揭示了预期的结果——一个高损失障碍——表明损失情况并非微不足道。 然而，这只是两点之间无限多条路径中的一条（Garipov 等人，2018 年；Draxler 等人，2018 年），并不意味着不存在这样一条路径。 例如 Garipov 等人。 (2018) 通过在两个解决方案之间找到低能量的二阶贝塞尔曲线，表明不同的密集解决方案位于同一盆地中。 按照他们的方法，我们尝试在两个稀疏 ´ 解之间找到二次和三次贝塞尔曲线。 令人惊讶的是，即使使用三次曲线，我们也无法找到没有高损耗障碍的路径。 这些结果表明，静态稀疏训练可能会陷入与更好的解决方案隔离的局部最小值。 另一方面，当我们优化整个密集空间的二次贝塞尔曲线时，我们发现了一条通向改进解决方案的近单调路径，这表明允许新的连接增长在导航损失景观方面产生更大的灵活性。

在图 6-right 中，我们从静态稀疏训练找到的次优解开始训练 RigL，证明它能够逃脱局部最小值，而使用静态稀疏训练进行重新训练则不能。 RigL 首先移除幅度最小的连接，因为移除这些连接已被证明对损失的影响最小 (Han et al., 2015; Evci, 2018)。 接下来，它激活具有高梯度的连接，因为这些连接有望最快地减少损失。 在附录 A 中，我们讨论了 RigL 更新对能源格局的影响。

## Discussion & Conclusion

在这项工作中，我们介绍了 RigL，这是一种有效训练稀疏神经网络的算法。 对于给定的计算预算，RigL 实现了比现有的密集到稀疏和稀疏到稀疏训练算法更高的精度。 RigL 在三种不同的场景中很有用：（1）提高用于部署的稀疏模型的准确性； (2) 提高只能训练有限迭代次数的大型稀疏模型的精度； (3) **与稀疏基元相结合，可以训练非常大的稀疏模型，否则是不可能的。**

由于缺乏对稀疏性的硬件和软件支持，第三种情况尚未探索。 尽管如此，当前硬件上稀疏网络性能的改进工作仍在继续（Hong et al., 2019; Merrill & Garland, 2016），新型硬件加速器将更好地支持参数稀疏性（Wang et al., 2018; Mike Ashby，2019 年；Liu 等人，2018 年；Han 等人，2016a；Chen 等人，2019 年）。 RigL 提供了利用这些进步的工具和动力。
