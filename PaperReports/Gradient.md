在协作机器学习 (CML) 中，多个代理将它们的资源（例如数据）集中在一起以完成共同的学习任务。 在代理人自利而非无私的现实 CML 设置中，他们可能不愿意在没有足够奖励的情况下共享数据或模型信息。 此外，由于代理共享的数据/模型信息可能质量不同，因此设计对他们公平的奖励很重要，这样他们就不会感到被剥削或不愿分享。 在本文中，我们采用联邦学习作为 CML 范式，提出了一种新颖的余弦梯度沙普利值（CGSV）来公平地评估每个代理上传的模型参数更新/梯度的预期边际贡献，而不需要辅助验证数据集，并且基于 CGSV，通过将从服务器下载的聚合参数更新/梯度稀疏化作为对每个代理的奖励，设计一种具有公平性保证的新型训练时间梯度奖励机制，使其产生的质量与代理上传的参数更新/梯度的质量相当。 我们在公平性、预测性能和时间开销方面凭经验证明了我们的公平梯度奖励机制在多个基准数据集上的有效性。



在协作机器学习 (CML) 中，多个代理（例如，研究人员、组织、公司）将他们的资源（例如，数据）集中在一起以完成共同的学习任务。 它涵盖了各种现实世界的应用，例如数字医疗 [49]、临床试验研究 [13、23]、智能语音助手的唤醒词检测 [27] 以及移动设备上的下一个词预测 [15]。

联邦学习 (FL) 提供了 CML 的自然范例 [18、29、41、43、57、62]。 在 FL 中，代理执行本地模型训练（例如，使用随机梯度下降）并通过可信服务器共享其生成的模型参数更新/梯度 [40、56、59]。 我们这里的工作与标准 FL 文献的一个重要区别是代理是自利的，因此不一定像分布式学习中的工作节点那样合作。 这意味着，为了实现学习任务的竞争性预测性能，必须激励/奖励以模型参数更新/梯度形式贡献/共享高质量信息的代理 [47、48、52]。

我们在这里的工作采用 FL 作为 CML 范式来设计一个公平的奖励机制，这样贡献更多的（自利的）代理人不会感到被剥削，而是得到相应的奖励。 这在合作博弈论 [42]、机制设计 [4] 和计算社会选择 [11] 中通常被视为公平。 要设计这样一个公平的奖励机制，我们需要解决三个主要问题：

首先，什么是合适的公平概念？ 来自合作博弈论的 Shapley 值 (SV) [50] 是一个有吸引力的选择，并已用于 ML [14] 和 FL [54、56]。 然而，现有的基于 SV 的工作 [19、37、54、56] 通常需要辅助验证数据集的可用性（并且所有代理都同意）和评估代理以 SV 形式的贡献的大量时间开销和 结果模型训练。 为了克服这些困难，我们建议改为利用代理上传/贡献的模型参数更新/梯度向量（或在某些代理上聚合）与在所有代理上聚合的对齐（特别是余弦相似性）（从而衡量其质量/ 值并绕过对验证数据集 [12、52] 的需要）来设计我们提出的余弦梯度 Shapley 值 (CGSV)（第 3.2 节），它可以用有界误差有效地近似（第 3.3 节）。

其次，奖励的选择是什么？ 已经提出了各种选择，例如预分配预算 [65, 66] 的货币奖励或通过 FL [9, 10] 的合作产生的总收入。 尽管考虑金钱奖励似乎很自然，但金钱和数据/梯度 [1, 46] 之间的通用名称如何容易建立并不明显，这使得在实践中应用这些工作具有挑战性。 相反，我们建议将从服务器下载的聚合参数更新/梯度视为对代理的奖励。

最后，梯度奖励机制如何保证公平性？ 我们提出的机制利用稀疏梯度技巧（第 3.4 节）来控制从服务器下载的聚合参数更新/梯度的质量，作为训练时对每个代理的奖励（而不是事后 [48、52、65]），例如 它的质量与代理上传/贡献的参数更新/梯度 [2, 7] 的质量相当。 因此，在整个训练过程中上传/贡献更高质量参数更新/梯度的代理最终应该得到收敛模型参数的奖励，其产生的训练损失（以及预测性能）更接近服务器的损失，如我们在 公平保证（第 3.5 节）[52]。

总之，我们在这里的工作对 CML 和 FL 的贡献包括：

我们提出了一种新颖的余弦梯度沙普利值 (CGSV)（第 3.2 节），以公平地评估每个智能体上传的模型参数更新/梯度的预期边际贡献，而无需辅助验证数据集，并提出具有有限误差的 CGSV 的有效近似值（ 第 3.3 节）。

基于近似 CGSV，我们设计了一种具有公平性保证（第 3.5 节）的新型训练时间梯度奖励机制（第 3.4 节），利用稀疏化从服务器下载的聚合参数更新/梯度作为对每个人的奖励的技巧 agent 使得其结果质量与 agent 上传/贡献的参数更新/梯度的质量相称。

我们凭经验证明了公平梯度奖励机制在多个基准数据集上的公平性、预测性能和时间开销方面的有效性（第 4 节）。



CML 中的奖励设计和选择。 在 FL [30、36、38、47、59、63、66]、贝叶斯 CML [52]、协作生成建模 [55] 和数据共享 [13、23、48] 等相关主题中，设计适当的奖励以 鼓励协作（例如，共享真实或合成数据、梯度或其他信息）是一个非常重要的问题。 一个有用的解决方案概念应该提供正式的公平概念、合适的奖励形式/名称，以及通过精心设计的奖励机制保证公平的原则性方式。 以前的工作考虑了来自预先分配的预算 [65、66] 或合作产生的总收入 [9、10] 的货币奖励，或者只是一种抽象但可量化的奖励形式 [47、48]。 尽管考虑金钱奖励似乎很自然，但金钱和数据/梯度 [1, 46] 之间的通用名称如何容易建立并不明显，这使得在实践中应用这些工作具有挑战性。 [66] 的工作探索了一种不同的途径，即使用反向拍卖来保证其机制的真实性而不是公平性。

公平观念。 合作博弈论中的 Shapley 值 (SV) [50] 被广泛认为是公平的原则概念 [4, 11, 42]，因为它有几个理想的属性，例如对称性和空玩家。 现有的基于 SV 的作品已经根据他们的贡献在奖励代理人的意义上考虑了公平性 [19,54,56]。 然而，它们通常需要辅助验证数据集的可用性（并且所有代理都同意）[37、52]，并且需要大量的时间开销来评估代理以 SV 形式的贡献和由此产生的模型训练 [14、19， 56]。 相比之下，[31] 的工作采用了均等主义的公平概念，旨在使代理之间的最终个人表现均等，这与 SV 有根本不同。

与 [31] 中的公平定义不同，我们采用由 SV [14、19、52、54、56] 形式化的公平概念。 我们提出的工作在 SV 的应用中是新颖的：虽然以前的工作使用验证准确性 [14、19、54、56]，但我们利用模型参数更新/梯度向量之间的余弦相似性 [12] 来设计我们提出的余弦梯度 Shapley 值 (CGSV)（第 3.2 节）以公平评估每个智能体上传的模型参数更新/梯度的预期边际贡献。 基于 CGSV，我们设计了一种具有公平性保证（第 3.5 节）的新型训练时间梯度奖励机制（第 3.4 节），并根据经验表明它在预测性能、公平性和时间开销方面优于几个现有的 FL 基线 （第 4.2 节）。



梯度下载步骤。 回想原始 FL 问题设置（第 3.1 节），在每次迭代 t 中，此步骤涉及所有代理从服务器下载相同的聚合参数更新/梯度 uN;t (1)（作为奖励）以将其模型参数更新为 相同的 wt（作为服务器），预计会收敛以产生有竞争力的预测性能 [8, 32]。 然而，这种对所有代理人的平等奖励是不公平的，并且会阻止任何代理人在负担得起的情况下上传/贡献更高质量的参数更新/梯度 [37、63]。 为了确保公平，每个代理都应下载某种形式的聚合参数更新/梯度作为与其上传/贡献的参数更新/梯度的质量/价值相称的奖励。 因此，在整个训练过程中上传/贡献更高质量参数更新/梯度的代理最终应该得到收敛模型参数的奖励，其产生的训练损失（以及预测性能）更接近服务器（定理 2）。

其中 mask(u; q) 保留 u 的最大 max(0; q) 分量（量级）并将其所有其他分量 [2, 61] 归零，并且 β ≥ 1 指定利他主义的程度：更大的利他主义 β 给任何具有较小 ri;t 的代理更大的梯度奖励质量改进，即，更大程度地减少其作为奖励的下载 vi;t 的稀疏性。 在 β = 1 的极端情况下，我们恢复普通 FL 问题设置（第 3.1 节），其中所有代理人都得到 uN;t 的同等奖励（即，所有 i 2 的最佳质量梯度奖励 vi;t = uN;t N 没有稀疏化），尽管重要系数 ri;t 可能因代理 i 2 N 而不同，并且在迭代 t 2 Z+ 中动态更新。 因此，将 β 从 1 增加到 1 通过对任何 ri;t; 我们凭经验显示了不同 β 对训练损失的影响，见图 7 节。 4.2. 注意代理 i∗ := argmaxi02N tanh(β ri0;t) 具有最大可能的 ri∗;t 不会从这种利他主义中受益，因为它已经根据 (5) 下载了最佳质量的梯度奖励（即 uN;t） ).