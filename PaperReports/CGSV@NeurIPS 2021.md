# [CGSV@NeurIPS 2021] Gradient-Driven Rewards to Guarantee Fairness in Collaborative Machine Learning

## Abstract

在协作机器学习 (CML) 中，多个代理将它们的资源（例如数据）集中在一起以完成共同的学习任务。 在代理人自利而非无私的现实 CML 设置中，他们可能不愿意在没有足够奖励的情况下共享数据或模型信息。 此外，由于代理共享的数据/模型信息可能质量不同，因此设计对他们公平的奖励很重要，这样他们就不会感到被剥削或不愿分享。 在本文中，我们采用联邦学习作为 CML 范式，提出了一种新颖的余弦梯度沙普利值（CGSV）来公平地评估每个代理上传的模型参数更新/梯度的预期边际贡献，而不需要辅助验证数据集，并且基于 CGSV，通过将从服务器下载的聚合参数更新/梯度稀疏化作为对每个代理的奖励，设计一种具有公平性保证的新型训练时间梯度奖励机制，使其产生的质量与代理上传的参数更新/梯度的质量相当。 我们在公平性、预测性能和时间开销方面凭经验证明了我们的公平梯度奖励机制在多个基准数据集上的有效性。



## Introduction

在协作机器学习 (CML) 中，多个代理（例如，研究人员、组织、公司）将他们的资源（例如，数据）集中在一起以完成共同的学习任务。 它涵盖了各种现实世界的应用，例如数字医疗 [49]、临床试验研究 [13、23]、智能语音助手的唤醒词检测 [27] 以及移动设备上的下一个词预测 [15]。

联邦学习 (FL) 提供了 CML 的自然范例 [18、29、41、43、57、62]。 在 FL 中，代理执行本地模型训练（例如，使用随机梯度下降）并通过可信服务器共享其生成的模型参数更新/梯度 [40、56、59]。 我们这里的工作与标准 FL 文献的一个重要区别是代理是自利的，因此不一定像分布式学习中的工作节点那样合作。 这意味着，为了实现学习任务的竞争性预测性能，必须激励/奖励以模型参数更新/梯度形式贡献/共享高质量信息的代理 [47、48、52]。

我们在这里的工作采用 FL 作为 CML 范式来设计一个公平的奖励机制，这样贡献更多的（自利的）代理人不会感到被剥削，而是得到相应的奖励。 这在合作博弈论 [42]、机制设计 [4] 和计算社会选择 [11] 中通常被视为公平。 要设计这样一个公平的奖励机制，我们需要解决三个主要问题：

首先，什么是合适的公平概念？ 来自合作博弈论的 **Shapley 值 (SV)** [50] 是一个有吸引力的选择，并已用于 ML [14] 和 FL [54、56]。 然而，现有的基于 SV 的工作 [19、37、54、56] 通常需要辅助**验证数据集**的可用性（并且所有代理都同意）和评估代理以 SV 形式的贡献的**大量时间开销**和 结果模型训练。 为了克服这些困难，我们建议改为利用代理上传/贡献的模型参数更新/梯度向量（或在某些代理上聚合）与在所有代理上聚合的对齐（特别是余弦相似性）（从而衡量其质量/ 值并绕过对验证数据集 [12、52] 的需要）来设计我们提出的余弦梯度 Shapley 值 (CGSV)（第 3.2 节），它可以**用有界误差有效地近似**（第 3.3 节）。

其次，奖励的选择是什么？ 已经提出了各种选择，例如预分配预算 [65, 66] 的货币奖励或通过 FL [9, 10] 的合作产生的总收入。 尽管考虑金钱奖励似乎很自然，但金钱和数据/梯度 [1, 46] 之间的通用名称如何容易建立并不明显，这使得在实践中应用这些工作具有挑战性。 相反，我们建议将从服务器下载的聚合参数更新/梯度视为对代理的奖励。

最后，梯度奖励机制如何保证公平性？ 我们提出的机制利用稀疏梯度技巧（第 3.4 节）来控制从服务器下载的聚合参数更新/梯度的质量，作为训练时对每个代理的奖励（而不是事后 [48、52、65]），例如 它的质量与代理上传/贡献的参数更新/梯度 [2, 7] 的质量相当。 因此，在整个训练过程中上传/贡献更高质量参数更新/梯度的代理最终应该得到收敛模型参数的奖励，其产生的训练损失（以及预测性能）更接近服务器的损失，如我们在 公平保证（第 3.5 节）[52]。

总之，我们在这里的工作对 CML 和 FL 的贡献包括：

我们提出了一种新颖的余弦梯度沙普利值 (CGSV)（第 3.2 节），以公平地评估每个智能体上传的模型参数更新/梯度的预期边际贡献，而无需辅助验证数据集，并提出具有有限误差的 CGSV 的有效近似值（ 第 3.3 节）。

基于近似 CGSV，我们设计了一种具有公平性保证（第 3.5 节）的新型训练时间梯度奖励机制（第 3.4 节），利用稀疏化从服务器下载的聚合参数更新/梯度作为对每个人的奖励的技巧 agent 使得其结果质量与 agent 上传/贡献的参数更新/梯度的质量相称。

我们凭经验证明了公平梯度奖励机制在多个基准数据集上的公平性、预测性能和时间开销方面的有效性（第 4 节）。



## Related Work

CML 中的奖励设计和选择。 在 FL [30、36、38、47、59、63、66]、贝叶斯 CML [52]、协作生成建模 [55] 和数据共享 [13、23、48] 等相关主题中，设计适当的奖励以 鼓励协作（例如，共享真实或合成数据、梯度或其他信息）是一个非常重要的问题。 一个有用的解决方案概念应该提供正式的公平概念、合适的奖励形式/名称，以及通过精心设计的奖励机制保证公平的原则性方式。 以前的工作考虑了来自预先分配的预算 [65、66] 或合作产生的总收入 [9、10] 的货币奖励，或者只是一种抽象但可量化的奖励形式 [47、48]。 尽管考虑金钱奖励似乎很自然，但金钱和数据/梯度 [1, 46] 之间的通用名称如何容易建立并不明显，这使得在实践中应用这些工作具有挑战性。 [66] 的工作探索了一种不同的途径，即使用反向拍卖来保证其机制的真实性而不是公平性。

公平观念。 合作博弈论中的 Shapley 值 (SV) [50] 被广泛认为是公平的原则概念 [4, 11, 42]，因为它有几个理想的属性，例如对称性和空玩家。 现有的基于 SV 的作品已经根据他们的贡献在奖励代理人的意义上考虑了公平性 [19,54,56]。 然而，它们通常需要辅助验证数据集的可用性（并且所有代理都同意）[37、52]，并且需要大量的时间开销来评估代理以 SV 形式的贡献和由此产生的模型训练 [14、19， 56]。 相比之下，[31] 的工作采用了均等主义的公平概念，旨在使代理之间的最终个人表现均等，这与 SV 有根本不同。

与 [31] 中的公平定义不同，我们采用由 SV [14、19、52、54、56] 形式化的公平概念。 我们提出的工作在 SV 的应用中是新颖的：虽然以前的工作使用验证准确性 [14、19、54、56]，但我们利用模型参数更新/梯度向量之间的余弦相似性 [12] 来设计我们提出的余弦梯度 Shapley 值 (CGSV)（第 3.2 节）以公平评估每个智能体上传的模型参数更新/梯度的预期边际贡献。 基于 CGSV，我们设计了一种具有公平性保证（第 3.5 节）的新型训练时间梯度奖励机制（第 3.4 节），并根据经验表明它在预测性能、公平性和时间开销方面优于几个现有的 FL 基线 （第 4.2 节）。



## Fair Gradient Reward Mechanism

### Cosine Gradient Shapley Value (CGSV) for Fairness

梯度下载步骤。 回想原始 FL 问题设置（第 3.1 节），在每次迭代 t 中，此步骤涉及所有代理从服务器下载相同的聚合参数更新/梯度 uN;t (1)（作为奖励）以将其模型参数更新为 相同的 wt（作为服务器），预计会收敛以产生有竞争力的预测性能 [8, 32]。 然而，这种对所有代理人的平等奖励是不公平的，并且会阻止任何代理人在负担得起的情况下上传/贡献更高质量的参数更新/梯度 [37、63]。 为了确保公平，每个代理都应下载某种形式的聚合参数更新/梯度作为与其上传/贡献的参数更新/梯度的质量/价值相称的奖励。 因此，在整个训练过程中上传/贡献更高质量参数更新/梯度的代理最终应该得到收敛模型参数的奖励，其产生的训练损失（以及预测性能）更接近服务器（定理 2）。

如果 φi 为负，则根据 (1) 中参数更新/梯度的加权和，ui 指向与某些其他参数更新/梯度相反的方向，因此与它们具有负余弦相似性。 在实践中，由于使用随机梯度下降 (SGD) 和/或高度非凸损失函数会产生嘈杂的训练，即使对于诚实的智能体 i，φi 有时也可能为负。 当此类案例的数量有限时，通过 SGD 进行的训练仍然可以收敛以产生具有竞争力的预测性能，正如 [12] 中的经验验证的那样。

### Efficient Approximation of CGSV

其证明见附录 A.2。 根据定理 1，近似误差是有界的，并随归一化系数 Γ 呈二次方下降。 但是，Γ 不能减小到任意小，这可能会导致 jhui； uNij ≥ 1=I 不持有。 当 ui 与 uN 正交或接近零向量时，它也不成立，因此暗示代理 i 的参数更新/梯度的质量不够高。 因此，鼓励每个代理提供足够高质量的参数更新/梯度，以确保近似值 i 的质量（定理 1）

### Server-Side Training-Time Gradient Reward Mechanism

其中 mask(u; q) 保留 u 的最大 max(0; q) 分量（量级）并将其所有其他分量 [2, 61] 归零，并且 β ≥ 1 指定利他主义的程度：更大的利他主义 β 给任何具有较小 ri;t 的代理更大的梯度奖励质量改进，即，更大程度地减少其作为奖励的下载 vi;t 的稀疏性。 在 β = 1 的极端情况下，我们恢复普通 FL 问题设置（第 3.1 节），其中所有代理人都得到 uN;t 的同等奖励（即，所有 i 2 的最佳质量梯度奖励 vi;t = uN;t N 没有稀疏化），尽管重要系数 ri;t 可能因代理 i 2 N 而不同，并且在迭代 t 2 Z+ 中动态更新。 因此，将 β 从 1 增加到 1 通过对任何 ri;t; 我们凭经验显示了不同 β 对训练损失的影响，见图 7 节。 4.2. 注意代理 i∗ := argmaxi02N tanh(β ri0;t) 具有最大可能的 ri∗;t 不会从这种利他主义中受益，因为它已经根据 (5) 下载了最佳质量的梯度奖励（即 uN;t） ).



## Experiments and Discussion

### Experimental Results

预测性能。 表 1 显示了代理通过我们的公平梯度奖励机制与所有数据集上的测试基线进行协作所取得的平均和最高测试准确度的结果。 我们的公平梯度奖励机制通常在两个指标上都优于测试基线，特别是对于异构数据分区和 MR 数据集。 在 MNIST 上，对于 10 个智能体之间的 CLA 数据分区，我们的公平梯度奖励机制在 β = 1:5 时实现了平均（最高）75%（95%）的测试准确率，而表现最佳的 ECI 基线仅达到 53 % (94%)。 在 CIFAR-10 上，对于 10 个智能体之间的 CLA 数据划分，我们的公平梯度奖励机制在 β = 2 时实现了平均（最高）36%（54%）的测试精度，而表现最佳的 DW 基线仅达到 32 % (47%)。 在 MR 数据集上，我们的公平梯度奖励机制在 β = 1 时达到平均（最高）测试准确率 62%（76%），而表现最佳的 RR 基线达到 63%（65%）。 **其更好的性能可能归因于梯度聚合步骤 (1) 中通过 ri;t 的自适应重新加权，它可以动态地解释代理本地数据集中的异质性** [32]。 虽然 EU 的性能与 FedAvg 和 ECI 相当（即，EU 与 FedAvg/ECI 之间的平均测试精度差异小于 3%），但它的性能并不比我们的公平梯度奖励机制（例如，在 MNIST 上，对于 CLA 数据在 10 个智能体之间划分，EU 与我们的公平梯度奖励机制在 β = 1:5 时的平均测试精度差异超过 20%，因为与余弦相似度不同，欧氏距离无法捕获梯度之间的方向差异， 这很重要，因为负梯度指向较低损失的方向。 重要的是，q-FFL 旨在均衡局部训练损失 w.r.t。 代理的本地数据集，这对于 POW 和 CLA 等异构数据分区可能不是最优的。 我们在附录 B.5 中提供了进一步的结果，通过经验比较我们的公平梯度奖励机制与 q-FFL 的预测性能。



## Conclusion and Future Work

在本文中，我们描述了一种新颖的余弦梯度沙普利值 (CGSV)（第 3.2 节），以公平地评估每个智能体在 FL 中上传的模型参数更新/梯度的预期边际贡献，而无需辅助验证数据集，并提供了一个有效的近似值 具有有限误差的 CGSV（第 3.3 节）。 基于近似的 CGSV，我们设计了一种新颖的训练时间公平梯度奖励机制（第 3.4 节），利用稀疏化从服务器下载的聚合参数更新/梯度作为对每个代理的奖励的技巧，使得其产生的质量与 代理上传/贡献的参数更新/梯度。 因此，在整个训练过程中上传/贡献更高质量参数更新/梯度的代理最终应该得到收敛模型参数的奖励，其产生的训练损失（以及预测性能）更接近服务器的损失，如我们在 公平保证（第 3.5 节）。 我们已经凭经验证明了我们的公平梯度奖励机制在多个基准数据集上的公平性、预测性能和时间开销方面的有效性（第 4 节）。 特别是，我们的公平梯度奖励机制比几个现有的 FL 基线更有效，因为它只需要服务器进行少量计算。

我们提出的公平梯度奖励机制还为从业者提供了灵活性，可以通过控制利他主义程度的超参数 β 在梯度奖励的公平性和平等性之间进行权衡（第 3.4 节）。 对于未来的工作，当有一些对手时考虑公平的概念会很有趣。 我们还会考虑将我们的工作和公平性保证推广到其他类型的 CML（例如，模型融合 [16、17、24]）和协作贝叶斯优化 [53]。