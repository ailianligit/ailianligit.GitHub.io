# [ITOP@ICML 2021] Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training

## Abstract

在本文中，我们通过在稀疏训练中提出实时过参数化 (ITOP) 的概念，引入了一种新的视角来训练能够获得最先进性能的深度神经网络，而无需进行昂贵的过参数化。 **通过从随机稀疏网络开始并在训练过程中不断探索稀疏连接性，我们可以在训练过程中执行过参数化，缩小稀疏训练和密集训练之间的可表达性差距。** 我们进一步使用 ITOP 来了解动态稀疏训练 (DST) 的底层机制，**并发现 DST 的好处来自于它在搜索最佳稀疏连接时跨时间考虑所有可能参数的能力**。 只要可靠地探索了足够的参数，DST 就可以大大优于密集神经网络。 我们提出了一系列实验来支持我们的猜想，并在 ImageNet 上使用 ResNet-50 实现最先进的稀疏训练性能。 更令人印象深刻的是，ITOP 在极端稀疏情况下取得了优于基于过参数化的稀疏方法的优势性能。 当在 CIFAR-100 上使用 ResNet-34 进行训练时，ITOP 可以在 98% 的极端稀疏度下匹配密集模型的性能。



## Introduction

尽管事实上训练目标函数通常是非凸的和非光滑的（Goodfellow et al., 2015; Brutzkus et al., 2017；Li & Liang，2018；Safran & Shamir，2018；Soudry & Carmon，2016；Allen-Zhu 等，2019；Du 等，2019；Zou 等，2020；Zou & Gu，2019）。 同时，先进的深度模型（Simonyan & Zisserman，2014；He 等人，2016；Devlin 等人，2018；Brown 等人，2020；Dosovitskiy 等人，2021）不断实现最先进的水平 结果导致许多机器学习任务。 在实现令人印象深刻的性能的同时，最先进模型的尺寸也在爆炸式增长。 训练和部署那些高度过度参数化的模型所需的资源令人望而却步。

出于推理的动机，大量研究（Mozer & Smolensky，1989 年；Han 等人，2015 年）试图发现一种稀疏模型，该模型可以充分匹配相应密集模型的性能，同时大幅减少参数数量。 这些技术虽然有效，但涉及对高度过度参数化的模型进行至少一个完整的收敛训练时间（完全密集的过度参数化）的预训练（Janowsky，1989 年；LeCun 等人，1990 年；Hassibi 和 Stork，1993 年；Molchanov 等人 ., 2017; Han et al., 2016; Gomez et al., 2019; Dai et al., 2018a) 或部分收敛训练时间（部分密集过参数化）（Louizos et al., 2017; Zhu & Gupta, 2017 年；Gale 等人，2019 年；Savarese 等人，2019 年；Kusupati 等人，2020 年；You 等人，2019 年）。

鉴于最先进的模型（例如 GPT-3（Brown 等人，2020 年）和 Vision Transformer（Dosovitskiy 等人，2020 年））的训练成本越来越大，这种严重过度参数化 依赖性导致大多数机器学习社区无法获得最先进的模型。

最近，彩票假设 (LTH) (Frankle & Carbin, 2019) 显示了从头开始训练子网络（稀疏训练）以匹配密集网络性能的可能性。 然而，这些“中奖彩票”是在完全密集的超参数化过程（迭代修剪完全融合网络）的指导下找到的，以及通过部分密集超参数化（初始化时修剪（Lee 等人， 2019；Wang 等人，2020；de Jorge 等人，2020)）或没有过度参数化（随机初始化的静态稀疏训练（Mocanu 等人，2016；Evci 等人，2019）））通常无法 以匹配其密集对应物所达到的准确性。 一个常识性的解释是，与密集训练相比，稀疏训练，尤其是在极高稀疏度下，不具有过参数化特性，因此可表达性较差。 解决这个问题的一种方法是利用从密集训练中学到的知识，例如 LTH（Frankle & Carbin，2019）。 虽然有效，但过度参数化密集训练所附带的计算成本和内存要求令人望而却步。

### Our Contribution

在本文中，我们提出了一个称为 InTime 过参数化 1 的概念，以缩小过参数化方面的差距以及稀疏训练和密集训练之间的可表达性，如图 1 所示。不是从密集和预训练模型继承权重，而是允许 跨训练时间的连续参数探索在时空流形中执行过参数化，这可以显着提高稀疏训练的可表达性。

我们发现实时过参数化的概念有助于 (1) 探索稀疏训练的可表达性，特别是对于极端稀疏性，(2) 降低训练和推理成本 (3) 理解动态稀疏训练的潜在机制 ( DST) (Mocanu et al., 2018; Evci et al., 2020a), (4) **在防止过度拟合和提高泛化方面**。

基于实时过参数化，我们在 ImageNet 上使用 ResNet50 提高了最先进的稀疏训练性能。 我们通过将 ITOP 概念应用于稀疏训练方法的主要类别 DST，与基于过参数化的稀疏方法（包括 LTH、渐进幅度剪枝 (GMP) 和初始化剪枝 (PI)）进行比较，进一步评估了 ITOP 概念。 我们的结果表明，当达到充分且可靠的参数探索时（根据 ITOP 的要求），DST 始终优于那些基于过度参数化的方法。 由于 ITOP 在整个训练过程中消除了密集的过参数化，因此它可以以更少的训练 FLOPs 匹配相应的密集网络的性能，如图 2 所示。



## Related Work

### Dense Over-Parameterization

依赖于密集过度参数化（密集到稀疏训练）的稀疏诱导技术已被广泛研究。 我们根据它们对密集超参数化的依赖程度将它们分为三类。

**全密集过参数化。** 寻求从完全预训练的密集模型继承权重的技术历史悠久，由 Janowsky (1989) 和 Mozer & Smolensky (1989) 首次引入，作为迭代修剪和保留方法自主发展。 迭代剪枝和保留的基本思想包括三个步骤：（1）完全预训练密集模型直到收敛，（2）剪枝对性能影响最小的权重或神经元，以及（3） 重新训练修剪后的模型以进一步提高性能。 修剪和再训练周期至少需要一次 (Liu et al., 2019)，通常需要多次 (Han et al., 2016; Guo et al., 2016; Frankle & Carbin, 2019)。 用于修剪的标准包括但不限于大小 (Mozer & Smolensky, 1989; Han et al., 2016; Guo et al., 2016)、Hessian (LeCun et al., 1990; Hassibi & Stork, 1993)、 互信息 (Dai et al., 2018a)，泰勒展开 (Molchanov et al., 2016; 2019)。 除剪枝外，其他技术包括变分 dropout（Molchanov 等人，2017 年）、目标 dropout（Gomez 等人，2019 年）、强化学习（Lin 等人，2017 年）也从预训练的密集模型中生成稀疏模型 模型。

**部分密集过参数化。** 另一类方法从密集网络开始，并在训练过程中不断地稀疏化模型。 Gradual magnitude pruning (GMP) (Narang et al., 2017; Zhu & Gupta, 2017; Gale et al., 2019) 被提议通过在 培训课程。 Louizos 等人有一些例子。 (2017) 和 Wen 等人。 (2016) 利用 L0 和 L1 正则化通过显式惩罚不同于零的参数来逐渐学习稀疏性。 最近，Srinivas 等人。 (2017); 刘等人。 (2020a)； 萨瓦雷斯等人。 (2019); 肖等。 (2019); 库苏帕蒂等人。 (2020); 周等。 (2021) 通过引入可训练掩码以在训练过程中学习理想的稀疏连接，进一步推进。 由于这些技术是从密集模型开始的，因此训练成本比训练密集网络要小，具体取决于学习最终稀疏模型的阶段。

**一次性密集过参数化。** 最近，出现了初始化修剪 (PI) 的工作（Lee 等人，2019 年；2020 年；Wang 等人，2020 年；Tanaka 等人，2020 年；de Jorge 等人，2021 年）以获得可训练的稀疏神经网络 基于一些显着性标准的主要训练过程之前的网络。 这些方法属于密集过参数化的范畴，主要是因为密集模型需要训练至少一次迭代才能获得那些可训练的稀疏网络。

### In-Time Over-Parameterization

动态稀疏训练。 与 LTH 并行发展的 DST 是一类不断增长的方法，用于在整个训练过程中使用固定参数计数从头开始训练稀疏网络（稀疏到稀疏训练）。 该范例从（随机）稀疏神经网络开始，并允许稀疏连接在训练期间动态演变。 它首先在 Mocanu (2017) 中引入，并在 Mocanu 等人中得到完善。 **(2018) 通过提出稀疏进化训练 (SET) 算法，该算法比静态稀疏神经网络具有更好的性能。** 除了适当的分类性能外，它还有助于检测重要的输入特征（Atashgahi 等人，2020 年）。 贝莱克等人。 (2018) 提出深度重新布线，通过从后验分布中采样稀疏配置和权重来训练具有严格连接约束的稀疏神经网络。 后续工作进一步引入了权重再分配（Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019; Liu et al., 2021）、**基于梯度的权重增长（Dettmers & Zettlemoyer, 2019; Evci et al., 2020a）**，以及 反向传递中的额外权重更新（Raihan & Aamodt，2020；Jayakumar 等人，2020）以提高稀疏训练性能。 通过放宽固定参数计数的约束，Dai 等人。 (2019; 2018b) 提出了一种基于基于梯度的增长和基于幅度的修剪的增长和修剪策略，以产生准确但非常紧凑的稀疏网络。 最近，刘等人。 (2020b) 首次说明了使用动态稀疏训练的真正潜力。 通过开发一个独立的框架，他们可以在典型的笔记本电脑上训练真正的稀疏神经网络，而无需掩码，神经元数超过一百万。

了解动态稀疏训练。 同时，一些作品试图理解动态稀疏训练。 刘等人。 (2020c) 发现 DST 逐渐将初始稀疏拓扑优化为一个完全不同的拓扑。 尽管存在许多可以实现类似损失的低损失稀疏解决方案，但它们在拓扑空间上有很大不同。 Evci 等人。 (2020b) 发现由密集初始化初始化的稀疏神经网络，例如，He 等人。 (2015)，梯度流较差，而 DST 可以显着改善训练期间的梯度流。 尽管很有前途，但稀疏训练的能力尚未得到充分探索，DST 的潜在机制尚不清楚。 像这样的问题：为什么 Dynamic Sparse Training 可以提高稀疏训练的性能？ Dynamic Sparse Training 如何使稀疏神经网络模型能够匹配 - 甚至优于 - 密集模型？ 必须回答



## In-Time Over-Parameterization

在本节中，我们详细描述了实时过参数化，这是我们提出的一种概念，可以作为训练深度神经网络而无需昂贵的过参数化的替代方法。 我们将 In-Time OverParameterization 称为密集过度参数化的变体，这可以通过鼓励在整个训练时间内进行连续参数探索来实现。 需要注意的是，不同于稠密模型的过参数化指的是参数空间的空间维度，实时过参数化指的是在时空流形中探索的整体维度

### In-Time Over-Parameterization Hypothesis

基于实时过参数化，我们提出以下假设来理解动态稀疏训练：

假设。 动态稀疏训练的好处在于它能够在搜索最佳稀疏神经网络连接时跨时间考虑所有可能的参数。 具体来说，这个假设可以分为三个主要支柱来解释 DST 的性能：

1. 动态稀疏训练可以显着提高稀疏训练的性能，这主要归功于跨训练时间的参数探索。
2. 动态稀疏训练的性能与整个训练过程中可靠探索参数的总数高度相关。 可靠探索的参数是指那些新探索的（新生长的）权重，这些权重已经更新了足够长的时间以超过修剪阈值。
3. 只要有足够的可靠探索的参数，通过 Dynamic Sparse Training 训练的稀疏神经网络模型可以在很大程度上匹配甚至优于它们的密集模型，即使在极高的稀疏度水平下也是如此。

为了方便起见，我们将我们的假设命名为及时超参数化假设。

### Hypothesis Evaluation

在本节中，我们将研究实时过度参数化假设，并研究实时过度参数化对 DST 性能的影响。 我们选择稀疏进化训练 (SET) 作为我们的 DST 方法，因为 SET 以随机方式激活新权重，自然会考虑所有可能的参数进行探索。 它还有助于避免基于梯度的方法引入的密集过度参数化偏差，例如 Rigged Lottery (RigL)（Evci 等人，2020a）和 Sparse Networks from Scratch (SNFS)（Dettmers & Zettlemoyer，2019）， 因为后者在向后传递中利用密集梯度来探索新的权重。 为了验证提出的假设，我们进行了一组带有图像分类的逐步时尚实验。 我们在 CIFAR-10 上研究多层感知器 (MLP)，在 CIFAR-10 上研究 VGG-16，在 CIFAR-100 上研究 ResNet-34，在 ImageNet 上研究 ResNet-50。 我们使用 PyTorch 作为我们的库。 所有结果均取自三个不同运行的平均值，并用平均值和标准偏差报告。 实验细节见附录 A。