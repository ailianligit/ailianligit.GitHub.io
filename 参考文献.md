FedMP: Federated Learning through Adaptive Model Pruning in Heterogeneous Edge Computing

联邦学习 (FL) 已被广泛用于在边缘计算的海量分布式数据源上训练机器学习模型。 然而，现有的 FL 框架通常存在资源限制和边缘异质性的困难。 在此，我们设计并实现了 FedMP，这是一种通过自适应模型修剪的高效 FL 框架。 我们从理论上分析了剪枝率对模型训练性能的影响，并提出采用基于多臂强盗的在线学习算法来自适应地确定异构边缘节点的不同剪枝率，即使事先不知道它们的计算和通信能力。 通过自适应模型剪枝，FedMP 不仅可以减少资源消耗，还可以实现有希望的准确性。 为了防止剪枝模型的不同结构影响训练收敛，我们进一步提出了一种新的参数同步方案，称为残差恢复同步并行（R2SP），并提供了理论上的收敛保证。 在经典模型和数据集上进行的大量实验表明，FedMP 对于不同的异构场景和数据分布是有效的，并且与现有的 FL 方法相比可以提供高达 4.1 倍的加速。

![image-20230308194836590](D:\GitHub\assets\image-20230308194836590.png)