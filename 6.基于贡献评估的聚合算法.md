## \section{基于贡献评估的聚合算法}

\label{sec:contribution-aggregation}

### \subsection{基于余弦梯度夏普利值的贡献评估方法}

\label{subsec:shapley-contribution}



### \subsection{贡献评估方法的有效性}

\label{subsec:contribution-effective}

**噪声数据**



### \subsection{聚合算法的设计}

\label{subsec:aggregation-design}

最后，在每个通信轮次$t=1,2,\cdots,T$，服务器需要将客户端训练好的模型聚合起来，融合成为一个全局模型。在FedAvg算法\cite{mcmahan2017communication}中，模型以加权的方式进行聚合：$\theta^{t+1}\leftarrow\sum_{n=1}^N r_n^t\theta_n^t$。与之稍有不同的是，本文使用参数更新进行聚合，可以通过简单的推导证明两种聚合方式是等效的。$\Delta\theta^{t}_n$表示客户端$n$在通信轮次$t$训练前后模型参数的变化，通过训练后的模型参数与训练前的模型参数相减得到，并通过归一化防止梯度爆炸（gradient explosion）：$u_n^t\leftarrow\frac{\Delta\theta_n^{t}}{\lVert\Delta\theta_n^{t}\rVert}$。服务器接收到客户端发送的参数更新后，加权聚合得到聚合参数更新：$U^t\leftarrow\sum_{n=1}^Nr^{t}_n u_n^t$，最后更新联邦模型参数：
$$
\theta^{t+1}\leftarrow\theta^{t}+U^t
$$
\begin{equation}
    \label{eq:param-update}
	\theta^{t+1}\leftarrow\theta^{t}+U^t
\end{equation}



夏普利值【Contributions to the Theory of Games】

已经应用于FL的公平性和激励机制：需要验证数据集或开销太大

用有界误差有效地近似【Gradient-Driven Rewards to Guarantee Fairness in Collaborative Machine Learning】

贡献的定义：余弦梯度夏普利值

不一定有助于提升全局大模型的性能，因此实验验证

余弦值的含义

然而这种情况下低价值或者恶意参与方持有大量数据时会影响联邦模型训练效果 

非独立同分布时加速训练

带有噪声标签的参与者，降低噪声标签对联邦学习性能的影响

![image-20230314225313619](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805682.png)

![image-20230314215815586](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678802299.png)

![image-20230314215830449](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678802313.png)



## \section{聚合算法的消融实验和超参数实验}

\label{subsec:exp-aggregation}

## \subsection{消融实验}

\label{subsec:exp-aggregation-ablation}



## \subsection{超参数实验}

\label{subsec:exp-aggregation-superparameters}

超参数变化 **噪声数据**

--distribution iid dirichlet
--beta 0.1 0.5

--sparsity 0.5

--grad-agg
--tradeoff 0.0 0.2 0.4 0.6 0.8 1.0

--outfile
--device

稳定性与探索之间的权衡

| 调整率 | non           | 0.0       | 0.2       | 0.4           | 0.6           | 0.8       | 1.0           |
| ------ | ------------- | --------- | --------- | ------------- | ------------- | --------- | ------------- |
| 0.1    | **0.52653** a | 0.38625 c | 0.45494 a | 0.3252 b+c    | 0.31122 a     | 0.39197 b | 0.416720003 b |
| 0.5    | **0.61985** a | 0.53255 a | 0.56927 a | 0.56358 a     | 0.531100005 a | 0.56498 a | 0.626300001 a |
| iid    | **0.66816** a | 0.6558 a  | 0.65311 a | 0.660710007 a | 0.65535 a     | 0.64995 a | 0.65926 a     |