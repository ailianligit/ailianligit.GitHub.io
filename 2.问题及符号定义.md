# \chapter{基于剪枝优化的大模型联邦训练算法}

\label{chap:algorithm}

## \section{问题及符号定义}

\label{sec:definition}

本文提出的大模型联邦训练算法基于联邦学习的框架，利用分布在众多客户端上的本地数据集，以协作的方式训练一个性能良好的联邦模型。在联邦学习的框架中，共有$N$个客户端和一个与所有客户端直接相连的服务器。在每个通信轮次$t=1,2,\cdots,T$，服务器将联邦模型$\theta^t$分发给$N$个客户端。每个客户端$n\in\{1,2,\cdots,N\}$都拥有自己的本地数据集$\mathcal{D}_{n}$，每个迭代轮次$e=1,2,\cdots,E$，客户端$n$利用本地数据集对模型参数进行更新。损失函数（loss function）$f(\theta)$表示$\mathcal{D}_{n}$中的特征输入模型后的预测输出和实际输出之间的差距。对于回归任务，损失函数一般为均方误差（mean square error，MSE）。对于分类任务，损失函数一般为交叉熵（cross-entropy）函数。为了导出优化目标，本地经验风险（empirical risk）函数可以表示为$F_n(\theta):=\frac{1}{\left|\mathcal{D}_{n}\right|}\sum_{i\in\mathcal{D}_{n}}f_i(\theta)$，因此，联邦学习优化的目标是全局（联邦）经验风险最小化，在FedAvg算法\cite{mcmahan2017communication}中，聚合的权重表示为$r_n\leftarrow\frac{\left|\mathcal{D}_{n}\right|}{\sum_{n\in\{1,2,\cdots,N\}}\left|\mathcal{D}_{n}\right|}$，而本文将基于贡献评估对这一聚合权重进行改进。联邦学习的目标函数可以表示为：
$$
\min_{\theta}F(\theta)\triangleq\sum_{n=1}^N r_nF_n(\theta)
$$
\begin{equation}
    \label{eq:optimization-goal}
	\min_{\theta}F(\theta)\triangleq\sum_{n=1}^N r_nF_n(\theta)
\end{equation}

为了改进模型的参数使得全局经验风险最小化，常用且有效的方法是随机梯度下降（stochastic gradient descent，SGD）算法。首先，从$\mathcal{D}_{n}$中随机采样小批量数据，然后，基于小批量数据的预测输出和实际输出计算损失函数$f_n(\theta^{t,e}_n)$及其梯度$\nabla f_n(\theta^{t,e}_n)$，最后，基于随机梯度下降对参数进行更新：
$$
\theta^{t,e}_n\leftarrow\theta^{t,e-1}_n-\eta^t\nabla f_n(\theta^{t,e-1}_n)
$$
\begin{equation}
    \label{eq:sgd}
	\theta^{t,e}_n\leftarrow\theta^{t,e-1}_n-\eta^t\nabla f_n(\theta^{t,e-1}_n)
\end{equation}

其中$\eta^t$表示学习率，学习率太大，模型难以收敛，学习率太小，模型收敛慢或根本无法学习。带有动量（momentum）的随机梯度下降使用指数加权平均后的梯度进行参数更新，因为动量带有之前梯度的信息，因此可以加快收敛的速度，同时跳过一些局部的极小值点。此外，随机梯度下降算法中还可以加入权重衰减（weight decay）项，这相当于在目标函数中加上了正则（regularization）项，通过惩罚过大的权重，限制模型的复杂度，防止过拟合现象的发生。

最后，在每个通信轮次$t=1,2,\cdots,T$，服务器需要将客户端训练好的模型聚合起来，融合成为一个全局模型。在FedAvg算法\cite{mcmahan2017communication}中，模型以加权的方式进行聚合：$\theta^{t+1}\leftarrow\sum_{n=1}^N r_n^t\theta_n^t$。与之稍有不同的是，本文使用参数更新进行聚合，可以通过简单的推导证明两种聚合方式是等效的，聚合算法的设计和实现细节详见\label{sec:contribution-aggregation}节。

除了对联邦模型的性能进行优化，本文的优化目标还包括对通信开销和计算成本的优化，同时需要减轻不同分布且带有噪声的样本对模型性能的影响，而针对这两部分的优化将分别在\label{sec:dynamic-pruning}节和\label{sec:contribution-aggregation}节进行阐述，基于剪枝优化的大模型联邦训练的框架将在\label{sec:framework}节进行概述。本文中用到的大部分符号及其含义都在表\ref{tab:superparameters}中列出。

| 符号                         | 定义                 | 符号                 | 定义                 |
| ---------------------------- | -------------------- | -------------------- | -------------------- |
| $n$，$N$                     | 客户端索引和总数     | $\mathcal{D}_{n}$    | 本地数据集           |
| $t$，$T$                     | 通信轮次和总轮数     | $e$，$E$             | 本地迭代轮次和总轮数 |
| $\alpha$                     | 整体剪枝率           | $\Delta\alpha$       | 逐层剪枝率           |
| $\theta^t$，$\theta^{t}_n$   | 联邦和客户端模型参数 | $r_n^{t}$            | 客户端聚合权重       |
| $\nabla f_n(\theta^{t,e}_n)$ | 随机梯度             | $\Delta\theta^{t}_n$ | 客户端参数更新       |
| $u_n^t$                      | 归一化参数更新       | $U^t$                | 联邦参数更新         |
| $f(\theta)$                  | 损失函数             | $F(\theta)$          | 经验风险函数         |
| $\psi_n^t$                   | 余弦梯度夏普利值     | $\gamma$             | 聚合算法权衡系数     |
| $\eta$                       | 学习率               | $p^t$                | 调整比例系数         |
| $T_{end}$                    | 结束动态剪枝通信轮次 | $\Delta{T}$          | 动态剪枝间隔通信轮数 |

