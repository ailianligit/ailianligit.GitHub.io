# 基于动态拓扑的剪枝算法

## 动态剪枝

In-Time Over-Parameterization【Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training】

dropout的子采样和聚合效应：正则化效果【Improving neural networks by preventing co-adaptation of feature detectors  】

从头训练一个具有固定稀疏度（静态）的网络会导致性能下降，训练相同参数数量的密集网络会获得更好的结果【The lottery ticket hypothesis at scale】【Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization】观察到静态稀疏训练收敛到比动态稀疏训练损失更高的解决方案

ERK的有效性【RigL：Rigging the Lottery: Making All Tickets Winners】

SET随机增长新连接，提高了small-dense的性能，但饱和度为75%，表明随即增长新连接的局限性

因为移除这些连接已被证明对损失的影响最小【Learning both weights and connections for efficient neural network】【Detecting dead weights and units in neural networks  】。 接下来，它激活具有高梯度的连接，因为这些连接有望最快地减少损失

与稀疏基元相结合，可以训练非常大的稀疏模型，否则是不可能的。

允许在整个训练时间内进行连续参数探索，而不是从密集和预训练的模型中继承权重，从而在时空流形中执行过参数化，这可以显着提高稀疏训练的可表达性。【Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training】

最近，彩票假设 (LTH) (Frankle & Carbin, 2019) 显示了从头开始训练子网络（稀疏训练）以匹配密集网络性能的可能性。 然而，这些“中奖彩票”是在完全密集的超参数化过程（迭代修剪完全融合网络）的指导下找到的，以及通过部分密集超参数化（初始化时修剪（Lee 等人， 2019；Wang 等人，2020；de Jorge 等人，2020)）或没有过度参数化（随机初始化的静态稀疏训练（Mocanu 等人，2016；Evci 等人，2019）））通常无法 以匹配其密集对应物所达到的准确性。 一个常识性的解释是，与密集训练相比，稀疏训练，尤其是在极高稀疏度下，不具有过参数化特性，因此可表达性较差。 解决这个问题的一种方法是利用从密集训练中学到的知识，例如 LTH（Frankle & Carbin，2019）。 虽然有效，但过度参数化密集训练所附带的计算成本和内存要求令人望而却步。

![image-20230314225325737](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805659.png)

![image-20230314225337382](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805661.png)



## 非结构化剪枝



## 按层剪枝

bias bn层不剪

![image-20230314225218737](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805664.png)

![image-20230314225237558](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805666.png)

![image-20230314225247744](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805670.png)





## 剪枝标准



## 消融实验

动态剪枝 静态剪枝 剪枝率为0 小型密集模型？



## 超参数实验

超参数变化：$\Delta T$ 和 R

--distribution iid dirichlet
--beta 0.1 0.5

--sparsity 0.5

--need-readjust
--readjustment-ratio 0.0 0.1 0.3 0.5 0.7 0.9

--outfile
--device

| 调整率 | 0.0           | 0.01         | 0.1       | 0.3         | 0.5         | 0.7          | 0.9          |
| ------ | ------------- | ------------ | --------- | ----------- | ----------- | ------------ | ------------ |
| 0.1    | **0.52653** a | 0.438265 a+b | 0.44275 b | 0.40205 a+b | 0.4853 b    | 0.428515 a+b | 0.416295 a+b |
| 0.5    | **0.61985** a | 0.62811      | 0.61379   | 0.60474     | 0.60608     | 0.61489      | 0.61472      |
| iid    | **0.66816** a | 0.65834      | 0.65589   | 0.64009     | 0.661259997 | 0.66066      | 0.65725      |

| 调整率 | 0.0           | 0.01    | 0.1     | 0.3        | 0.5        | 0.7     | 0.9     |
| ------ | ------------- | ------- | ------- | ---------- | ---------- | ------- | ------- |
| 0.1    | **0.52653** a | 0.44886 | 0.49334 | 0.53636    | **0.4562** | 0.45227 | 0.42387 |
| 0.5    | **0.61985** a | 0.60639 | 0.62149 | **0.5841** | 0.62862    | 0.62361 | 0.5961  |
| iid    | **0.66816** a | 0.66017 | 0.66177 | 0.65668    | 0.6482     | 0.64705 | 0.65354 |

| 调整间隔 | 0.0           | 1             | 4             | 7             | 10        | 20        | 50        |
| -------- | ------------- | ------------- | ------------- | ------------- | --------- | --------- | --------- |
| 0.1      | **0.52653** a | 0.506880009 b | 0.449759996 b | 0.501229995 a | 0.42935 a | 0.48242 b | 0.44115 b |
| 0.5      | **0.61985** a | 0.60612       | 0.58966       | 0.62455       | 0.63958   | 0.60875   | 0.58828   |
| iid      | **0.66816** a | 0.63779       | 0.65832       | 0.66487       | 0.64539   | 0.65741   | 0.64983   |

| 调整间隔 | 0.0           | 1    | 4    | 7    | 10   | 20   | 40   |
| -------- | ------------- | ---- | ---- | ---- | ---- | ---- | ---- |
| 0.1      | **0.52653** a |      |      |      |      |      |      |
| 0.5      | **0.61985** a |      |      |      |      |      |      |
| iid      | **0.66816** a |      |      |      |      |      |      |







