## \section{基于动态拓扑的剪枝算法}

\label{sec:dynamic-pruning}

对于一种网络剪枝算法，需要对剪枝的方法、结构和标准等方面进行设计。本章将首先介绍剪枝的方法，\subsection{动态剪枝算法的设计}节对基于动态拓扑的剪枝算法的设计细节进行了阐述，\subsction{剪枝结构与剪枝标准}节对剪枝的结构和标准进行了介绍，最后\subsection{逐层剪枝算法}节阐述了逐层剪枝算法，对剪枝的效果进行了改善。



### \subsection{动态剪枝算法的设计}

\label{subsec:dynamic-pruning-design}

In-Time Over-Parameterization【Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training】

dropout的子采样和聚合效应：正则化效果【Improving neural networks by preventing co-adaptation of feature detectors  】

从头训练一个具有固定稀疏度（静态）的网络会导致性能下降，训练相同参数数量的密集网络会获得更好的结果【The lottery ticket hypothesis at scale】【Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization】观察到静态稀疏训练收敛到比动态稀疏训练损失更高的解决方案

ERK的有效性【RigL：Rigging the Lottery: Making All Tickets Winners】

SET随机增长新连接，提高了small-dense的性能，但饱和度为75%，表明随即增长新连接的局限性

因为移除这些连接已被证明对损失的影响最小【Learning both weights and connections for efficient neural network】【Detecting dead weights and units in neural networks  】。 接下来，它激活具有高梯度的连接，因为这些连接有望最快地减少损失

与稀疏基元相结合，可以训练非常大的稀疏模型，否则是不可能的。

允许在整个训练时间内进行连续参数探索，而不是从密集和预训练的模型中继承权重，从而在时空流形中执行过参数化，这可以显着提高稀疏训练的可表达性。【Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training】

最近，彩票假设 (LTH) (Frankle & Carbin, 2019) 显示了从头开始训练子网络（稀疏训练）以匹配密集网络性能的可能性。 然而，这些“中奖彩票”是在完全密集的超参数化过程（迭代修剪完全融合网络）的指导下找到的，以及通过部分密集超参数化（初始化时修剪（Lee 等人， 2019；Wang 等人，2020；de Jorge 等人，2020)）或没有过度参数化（随机初始化的静态稀疏训练（Mocanu 等人，2016；Evci 等人，2019）））通常无法 以匹配其密集对应物所达到的准确性。 一个常识性的解释是，与密集训练相比，稀疏训练，尤其是在极高稀疏度下，不具有过参数化特性，因此可表达性较差。 解决这个问题的一种方法是利用从密集训练中学到的知识，例如 LTH（Frankle & Carbin，2019）。 虽然有效，但过度参数化密集训练所附带的计算成本和内存要求令人望而却步。

拓扑在优化过程中发生变化可以克服拓扑保持静态时遇到的局部最小值

![image-20230314225325737](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805659.png)

![image-20230314225337382](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805661.png)



### \subsction{剪枝结构与剪枝标准}

\label{subsec:structure-standard}

深度学习模型从卷积层到全连接层存在着大量冗余的参数，网络剪枝将模型中不重要的权重、神经元、滤波器甚至整个网络层剪去，这些被剪掉的权重或结构会在计算时被忽略。网络剪枝在剪枝结构上可以分为结构化剪枝和非结构化剪枝。结构化剪枝\cite{kruschke1991benefits}的对象通常是整个滤波器或网络层，能够通过常见的GPU或其他硬件得到加速，但对滤波器进行剪枝时，滤波器之前和之后的特征图都会发生变化，因此实现起来更加复杂。此外，结构化剪枝是粗粒度的剪枝模式，可能会对原始模型的结构造成破坏性的伤害。非结构化剪枝\cite{han2015learning}的对象是神经元之间的权重连接，虽然这种剪枝会导致剪枝后的模型变得稀疏，只有部分能够加速稀疏矩阵计算的硬件架构能够降低实际上的计算量，但非结构化剪枝的实现更加简单、直接，并且这种细粒度剪枝的模式相比结构化剪枝会更加精准，因此在通常情况下，模型的训练效果更好。由于本文的实验建立在仿真的基础上，通过估计的方式对客户端的计算量进行度量，因此本文提出的剪枝算法只会对卷积层和全连接层中的权重参数进行修剪，不会修剪偏差（bias），也不会修剪更大粒度的模型结构。

在非结构化剪枝的算法中，剪枝的标准决定了哪些权重会被剪去，以及哪些权重会得到保留。对于网络剪枝，一个广泛使用的标准是剪去绝对值较小的权重\cite{gale2019state}，另外，还有一些方法\cite{blalock2020state}会利用训练过程中产生的梯度信息，根据梯度与相应权重之间的乘积大小进行修剪。本文提出的剪枝算法采用简单且有效的模型权重大小作为剪枝的标准。因为随机梯度下降算法中引入了权重衰减项，相当于在目标函数上加上了L2正则项，那些对模型训练没有明显贡献的模型权重会在训练过程中逐渐缩小。因此，在这一标准下，绝对值偏大的权重相比绝对值偏小的权重的重要性更高，在模型剪枝中将会被优先保留。



### \subsection{逐层剪枝算法}

\label{subsec:layer-prune}

bias bn层不剪

![image-20230314225218737](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805664.png)

![image-20230314225237558](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805666.png)

![image-20230314225247744](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230314_1678805670.png)



## \section{剪枝算法的消融实验和超参数实验}

\label{sec:exp-pruning}

## \subsection{消融实验}

\label{subsec:exp-pruning-ablation}

动态剪枝 静态剪枝 剪枝率为0 小型密集模型？



## \subsection{超参数实验}

\label{subsec:exp-pruning-superparameters}

超参数变化：$\Delta T$ 和 R

--distribution iid dirichlet
--beta 0.1 0.5

--sparsity 0.5

--need-readjust
--readjustment-ratio 0.0 0.1 0.3 0.5 0.7 0.9

--outfile
--device

| 调整率 | 0.0           | 0.01         | 0.1       | 0.3         | 0.5         | 0.7          | 0.9          |
| ------ | ------------- | ------------ | --------- | ----------- | ----------- | ------------ | ------------ |
| 0.1    | **0.52653** a | 0.438265 a+b | 0.44275 b | 0.40205 a+b | 0.4853 b    | 0.428515 a+b | 0.416295 a+b |
| 0.5    | **0.61985** a | 0.62811      | 0.61379   | 0.60474     | 0.60608     | 0.61489      | 0.61472      |
| iid    | **0.66816** a | 0.65834      | 0.65589   | 0.64009     | 0.661259997 | 0.66066      | 0.65725      |

| 调整率 | 0.0           | 0.01    | 0.1     | 0.3        | 0.5        | 0.7     | 0.9     |
| ------ | ------------- | ------- | ------- | ---------- | ---------- | ------- | ------- |
| 0.1    | **0.52653** a | 0.44886 | 0.49334 | 0.53636    | **0.4562** | 0.45227 | 0.42387 |
| 0.5    | **0.61985** a | 0.60639 | 0.62149 | **0.5841** | 0.62862    | 0.62361 | 0.5961  |
| iid    | **0.66816** a | 0.66017 | 0.66177 | 0.65668    | 0.6482     | 0.64705 | 0.65354 |

| 调整间隔 | 0.0           | 1             | 4             | 7             | 10        | 20        | 50        |
| -------- | ------------- | ------------- | ------------- | ------------- | --------- | --------- | --------- |
| 0.1      | **0.52653** a | 0.506880009 b | 0.449759996 b | 0.501229995 a | 0.42935 a | 0.48242 b | 0.44115 b |
| 0.5      | **0.61985** a | 0.60612       | 0.58966       | 0.62455       | 0.63958   | 0.60875   | 0.58828   |
| iid      | **0.66816** a | 0.63779       | 0.65832       | 0.66487       | 0.64539   | 0.65741   | 0.64983   |

| 调整间隔 | 0.0           | 1    | 4    | 7    | 10   | 20   | 40   |
| -------- | ------------- | ---- | ---- | ---- | ---- | ---- | ---- |
| 0.1      | **0.52653** a |      |      |      |      |      |      |
| 0.5      | **0.61985** a |      |      |      |      |      |      |
| iid      | **0.66816** a |      |      |      |      |      |      |







