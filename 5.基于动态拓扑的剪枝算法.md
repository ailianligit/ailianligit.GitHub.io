## \section{基于动态拓扑的剪枝算法}

\label{sec:dynamic-pruning}

对于一种网络剪枝算法，需要对剪枝的方法、结构和标准等方面进行设计。本章将首先介绍剪枝的方法，\ref{subsec:dynamic-pruning-design}节对基于动态拓扑的剪枝算法的设计细节进行了阐述，\ref{subsec:structure-standard}节对剪枝的结构和标准进行了介绍，最后\ref{subsec:layer-prune}节阐述了逐层剪枝算法，改善了网络剪枝的效果。

### \subsection{动态剪枝算法的设计}

\label{subsec:dynamic-pruning-design}

正如绪论所述，经典框架\cite{han2015learning}、扩展框架\cite{gale2019state}和“彩票假说”\cite{frankle2018lottery}都难以在模型训练开始时就对训练的通信和计算成本进行优化，而基于静态拓扑的剪枝算法由于不具备大规模密集模型的过参数化特性\cite{liu2021we}，导致模型的表达能力和训练效果较差。受到生物学领域神经系统进化原理和计算机科学领域进化算法的启发，Decebal Constantin Mocanu等人\cite{mocanu2018scalable}首次提出了动态稀疏训练的概念和SET算法。SET算法在训练过程中动态调整模型的拓扑结构，在时间的维度上实现了与大模型类似的过参数化的特性，实验也证实了动态剪枝相对于静态剪枝在模型训练效果上的优势。经过简单的对比可以发现，基于动态拓扑的剪枝和基于遗传算法的神经架构搜索（Neural Architecture Search，NAS）\cite{xie2017genetic}的思路是非常相似的，两种技术都以搜索最优子网络为目标，用进化的方式对模型的参数空间进行探索。

算法\ref{algo:dynamic-prune}展示了基于动态拓扑的剪枝算法的流程。当$t<T_{end}$且$t\ \text{mod}\ \Delta{T} == 0$时，算法会对联邦模型的拓扑结构进行调整。调整比例系数$p^t$的含义是对初次剪枝后的模型再次剪枝的参数比例，$p^t$越大，代表着网络拓扑的调整范围越大，反之则越小。为了提升模型训练后期的稳定性，剪枝算法需要在训练过程中逐渐降低动态调整的比例，因此调整比例系数会以等式\ref{eq:f-decay}的形式衰减，Utku Evci和Tim Dettmers等人的实验\cite{dettmers2019sparse,evci2020rigging}表明，余弦衰减的效果略好于常数、负幂等形式的衰减，并且在训练结束前停止对模型拓扑的调整会有轻微的性能提升，因此停止调整的通信轮数$T_{end}$设置为总通信轮数$T$的3/4。此外，在剪枝过后，还需要恢复联邦模型的整体剪枝率，因此算法将以随机的方式逐层恢复相同数量的参数。

![image-20230317104258010](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230317_1679020979.png)

基于动态拓扑的剪枝算法结合了网络剪枝技术和过参数化特性的优势，在优化模型训练成本的同时，提升了模型的训练效果。动态剪枝算法从训练开始就对联邦模型进行剪枝，并在训练过程中保持一定的整体剪枝率，减少了训练所需的通信和计算资源。此外，动态剪枝算法在训练过程中对模型的参数空间进行探索，将重要的参数保留下来，在时间维度上实现了过参数化的特性。因此，动态剪枝可以突破静态剪枝中常常遇到的局部最优解，提升联邦模型的表达能力和训练效果。\ref{subsec:exp-pruning-ablation}节和\ref{subsec:exp-pruning-superparameters}节的消融实验可以证明动态剪枝算法在这两方面的优势，与此同时，这两节的超参数实验对剪枝算法中的整体剪枝率$\alpha$、初始调整比例系数$p^0$和间隔通信轮数$\Delta T$的设置进行了讨论。

此外，根据\ref{subsec:exp-distribution-ratio}节的探究性实验结果可以发现，进行适当的剪枝有助于联邦模型适应客户端样本标签非独立同分布的情况，使得联邦模型具备一定的鲁棒性。一众联邦学习领域的研究\cite{zhao2018federated,ozdayi2020improving}认为异构数据会导致客户端之间的模型或梯度差异过大，从而使得联邦模型的训练效果很差。从这个角度看，适当的剪枝能在尽可能保持模型训练效果的同时，减少客户端之间模型参数的距离。



### \subsction{剪枝结构与剪枝标准}

\label{subsec:structure-standard}

深度学习模型从卷积层到全连接层存在着大量冗余的参数，网络剪枝将模型中不重要的权重、神经元、滤波器甚至整个网络层剪去，这些被剪掉的权重或结构会在计算时被忽略。网络剪枝在剪枝结构上可以分为结构化剪枝和非结构化剪枝两类。结构化剪枝\cite{kruschke1991benefits}的对象通常是整个滤波器或网络层，能够通过常见的GPU或其他硬件得到加速，但对滤波器进行剪枝时，滤波器之前和之后的特征图都会发生变化，因此实现起来更加复杂。此外，结构化剪枝是粗粒度的剪枝模式，可能会对原始模型的结构造成破坏性的伤害。非结构化剪枝\cite{han2015learning}的对象是神经元之间的权重连接，虽然这种剪枝会导致剪枝后的模型高度稀疏，只有部分能够加速稀疏矩阵计算的硬件架构能够降低实际上的计算量，但非结构化剪枝的实现更加简单、直接，并且这种细粒度的剪枝模式相比结构化剪枝会更加精准，因此在通常情况下，模型的训练效果更好。由于本文的实验建立在仿真的基础上，通过估计的方式对客户端的计算量进行度量，因此本文提出的剪枝算法只会对卷积层和全连接层中的权重进行修剪，不会修剪偏差（bias），也不会修剪更大粒度的模型结构。

在非结构化剪枝的算法中，剪枝的标准决定了哪些权重会被剪去，以及哪些权重会得到保留。对于网络剪枝，一个广泛使用的标准是剪去绝对值较小的权重\cite{gale2019state}，另外，还有一些方法\cite{blalock2020state}会利用训练过程中产生的梯度信息，根据梯度与相应权重之间的乘积大小进行修剪。本文提出的剪枝算法采用简单且有效的模型权重大小作为剪枝的标准，移除绝对值较小的权重已被证明对模型训练效果的影响最小\cite{han2015learning,evci2018detecting}。因为随机梯度下降算法中引入了权重衰减项，相当于在目标函数上加上了L2正则项，那些对模型训练没有明显贡献的权重会在训练过程中逐渐缩小。因此，在这一标准下，绝对值大的权重相比绝对值小的权重的重要性更高，在模型剪枝中将会优先得到保留。



### \subsection{逐层剪枝算法}

\label{subsec:layer-prune}

在剪枝算法中，给定目标剪枝率$\alpha$，需要将模型参数$\theta^{in}$中比例为$\alpha$的权重剪去。在具体的实现方法上，可以分为全局剪枝和逐层剪枝两类。其中，全局剪枝对网络中所有参数视为一个整体，剪去绝对值最小的百分之$\alpha$个参数，虽然能够取得不错的效果，但可能导致层崩溃（layer-collapse）\cite{tanaka2020pruning}的现象发生。逐层剪枝的方法对于网络中的每一层进行单独剪枝，各层的剪枝率可以与全局剪枝率$\alpha$相同，也可以有自己的剪枝率$\Delta\alpha_l$。本文提出的逐层剪枝算法采用了Erdős–Rényi随机图拓扑\cite{renyi1959random}的想法，让网络层$l$中剩余的权重数与该层前后连接的神经元数$m_{l-1}+m_{l}$成线性的关系，ER公式\cite{mocanu2018scalable}\ref{eq:layer-ratio-normalize-ER}对全连接层的剪枝率进行了计算，其中分母$m_{l-1}\times m_{l}$表示未剪枝的情况下该全连接层拥有的总权重数。因为ER公式无法适应卷积层剪枝率的计算，所以ERK（Erdős–Rényi-Kernel）公式\cite{evci2020rigging}\ref{eq:layer-ratio-normalize-ERK}将内核的大小也考虑进来，其中$w_l$和$h_l$分别表示卷积内核的宽度和高度。为了让层剪枝率匹配整体的剪枝率，即让逐层剪枝后的整个模型实现$\alpha$的剪枝率，还需要对层剪枝率进行归一化。逐层剪枝算法的完整流程展示在算法\ref{algo:prune}中。

![image-20230317104241642](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202303/20230317_1679020962.png)

此前一些工作\cite{evci2020rigging}将Erdős–Rényi随机图拓扑运用到深度学习模型的网络剪枝中，通过实验验证了算法的有效性和对于剪枝效果的提升作用。本文将逐层剪枝算法视为基于动态拓扑的剪枝算法的具体剪枝细节，并最终帮助大模型在联邦学习的框架中进行训练。



## \section{剪枝算法的消融和超参数实验}

\label{sec:exp-pruning}

在基于动态拓扑的剪枝算法中，有三个超参数的设置是值得讨论的，分别是联邦模型的整体剪枝率$\alpha$、初始的动态调整比例系数$p^0$和动态调整的间隔轮数$\Delta T$。当$\alpha$设置为0时，整个联邦学习的框架退化为不剪枝的情况，因此，\label{subsec:exp-distribution-ratio}节将讨论不同数据划分情况下，讨论了不同整体剪枝率$\alpha$对联邦训练的影响

当$p^0$设置为0时，由等式\ref{eq:f-decay}可知，训练过程中的动态调整比例系数$p^t$始终为0，因此，\ref{subsec:exp-pruning-ablation}节将对

### \subsection{整体剪枝率$\alpha$对联邦训练的影响}

\label{subsec:exp-distribution-ratio}

#### 超参数

--distribution iid dirichlet
--beta 0.1 0.5

--sparsity 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

--outfile cifar10_iid_00_1_a.csv
--device

#### 实验结果

| 剪枝率 | iid        | $\beta=0.5$ | $\beta=0.1$   |
| ------ | ---------- | ----------- | ------------- |
| 0.3    | 0.6544 a+b | 0.61535 a   | 0.43656 b+c   |
| 0.4    | 0.65676 a  | 0.61896 a   | 0.458429998 b |
| 0.5    | 0.66816 a  | 0.61985 a   | 0.52653 a     |
| 0.6    | 0.64791 a  | 0.62733 a   | 0.42421 a     |
| 0.7    | 0.64565 a  | 0.61294 b   | 0.400715 a+b  |
| 0.8    | 0.65044 a  | 0.61669 a   | 0.372395 a+b  |
| 0.85   | 0.62724 a  | 0.59796 a   | 0.36106 b     |
| 0.9    | 0.597009 a | 0.53684 a   | 0.33501 b     |

### \subsection{初始调整比例系数$p^0$对联邦训练的影响}

\label{subsec:exp-pruning-ablation}

动态剪枝 静态剪枝 不剪枝

指标：上传下载、FLOPs

数据分布：iid 0.1 0.5 噪声数据

剪枝率：0.5

剪枝算法：

剪枝间隔：

初始比例：

聚合算法：

权衡系数：

收敛速度：

曲线稳定：

最终精度：



### \subsection{动态调整间隔轮数$\Delta T$对联邦训练的影响}

\label{subsec:exp-pruning-superparameters}

超参数变化：$\Delta T$ 和 R

--distribution iid dirichlet
--beta 0.1 0.5

--sparsity 0.5

--need-readjust
--readjustment-ratio 0.0 0.1 0.3 0.5 0.7 0.9

--outfile
--device

注意相同的超参数

| 调整率 | 0.0           | 0.01         | 0.1       | 0.3         | 0.5         | 0.7          | 0.9          |
| ------ | ------------- | ------------ | --------- | ----------- | ----------- | ------------ | ------------ |
| 0.1    | **0.52653** a | 0.438265 a+b | 0.44275 b | 0.40205 a+b | 0.4853 b    | 0.428515 a+b | 0.416295 a+b |
| 0.5    | **0.61985** a | 0.62811      | 0.61379   | 0.60474     | 0.60608     | 0.61489      | 0.61472      |
| iid    | **0.66816** a | 0.65834      | 0.65589   | 0.64009     | 0.661259997 | 0.66066      | 0.65725      |

| 调整率 | 0.0           | 0.01    | 0.1     | 0.3           | 0.5       | 0.7     | 0.9     |
| ------ | ------------- | ------- | ------- | ------------- | --------- | ------- | ------- |
| 0.1    | **0.52653** a | 0.44886 | 0.49334 | 0.53636 x     | 0.47661 z | 0.45227 | 0.42387 |
| 0.5    | **0.61985** a | 0.60639 | 0.62149 | 0.623909998 k | 0.62862 x | 0.62361 | 0.5961  |
| iid    | **0.66816** a | 0.66017 | 0.66177 | 0.65668       | 0.6482    | 0.64705 | 0.65354 |

| 调整间隔 | 0.0           | 1             | 4             | 7             | 10        | 20        | 50        |
| -------- | ------------- | ------------- | ------------- | ------------- | --------- | --------- | --------- |
| 0.1      | **0.52653** a | 0.506880009 b | 0.449759996 b | 0.501229995 a | 0.42935 a | 0.48242 b | 0.44115 b |
| 0.5      | **0.61985** a | 0.60612       | 0.58966       | 0.62455       | 0.63958   | 0.60875   | 0.58828   |
| iid      | **0.66816** a | 0.63779       | 0.65832       | 0.66487       | 0.64539   | 0.65741   | 0.64983   |

| 调整间隔 | 0.0           | 1               | 4            | 7         | 10            | 20            | 40        |
| -------- | ------------- | --------------- | ------------ | --------- | ------------- | ------------- | --------- |
| 0.1      | **0.52653** a | 0.38415 z       | 0.472735 x+z | 0.47327 k | 0.48978 x+z   | 0.41252 z+j   | 0.51071 a |
| 0.5      | **0.61985** a | 0.609295001 a+b | 0.620975 x+z | 0.63119 z | 0.61144 a+b   | 0.613469988 a | 0.61186 a |
| iid      | **0.66816** a | 0.64863 a       |              |           | 0.666059995 a |               |           |
