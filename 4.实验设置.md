# \chapter{实验与结果}

\label{chap:experiment}

## \section{实验设置}

\label{sec:setting}

### \subsection{实验环境}

\label{subsec:environment}

本章的所有实验程序使用Python程序设计语言及其开源的机器学习库Pytorch进行开发，实验使用的操作系统平台是Ubuntu 20.04.1，Nvidia驱动版本是510.54，并行计算平台是CUDA 11.0。在程序运行过程中，整个联邦学习的框架在单个NVIDIA GeForce RTX 3090 GPU上加载和计算。



### \subsection{数据集及其划分方式}

\label{subsec:dataset}

本章的实验将基于MNIST\cite{lecun1998gradient}、CIFAR-10、CIFAR-100\cite{krizhevsky2009learning}这三个数据集进行联邦模型的训练，并记录相关的实验结果。MNIST是一个黑白的手写数字数据集，训练集有60000张图片，测试集则有10000张图片，每张图片的大小是28*28，标签共有10类。CIFAR-10和CIFAR-100是大小为32*32的彩色图片数据集，其中CIFAR-10分为10个类别，训练集中每个类别有5000张图片，测试集中每个类别有1000张图片，而CIFAR-100分为100个类别，训练集中每个类别有500张图片，测试集中每个类别有100张图片。后续的实验将会按需求选择不同的数据集。

在本章的实验中，数据集的划分方式将会主要有三种。第一种是样本标签独立同分布，假设所有客户端的数据都是从同一个分布中独立采样，因此所有客户端的样本标签的分布是相同的，并且客户端之间的数据量是相同的。第二种是样本标签非独立同分布，划分方式是尽量让每个客户端上的样本标签分布不同，因此某个类别在不同客户端上的概率分布随机向量采样自狄利克雷（Dirichlet）分布，狄利克雷分布中有个分布参数$\beta$，$\beta$的值越大，则随机向量接近于从均匀分布中采样，而$\beta$越小，随机向量接近于从一个非常集中的分布中采样。第三种是在样本标签独立同分布的基础上，对不同的客户端，将不同比例的正确的样本标签换成错误的样本标签。后续的实验将会按需求选择不同的数据集划分方式。



### \subsection{模型}

\label{subsec:model}

本章所有实验所采用的联邦模型都基于卷积神经网络（Convolutional Neural Networks，CNN）\cite{lecun1998gradient}，虽然这些模型的规模远远小于现今最为前沿的一些大规模预训练模型，但这些超大规模的模型即使经过压缩，其训练所需的通信、计算和存储资源仍然远远超出一些边缘设备的能力。本章实验采用的卷积神经网络在MNIST和CIFAR-10数据集上已经可以取得不错的精度，因此，将这些卷积神经网络作为联邦模型进行训练已经可以满足实验的需求。表\ref{tab:mnist-model}和表\ref{tab:cifar-model}展示了本章实验在MNIST和CIFAR-10两类数据集上采用的模型结构、每层的输出和参数量。其中，CIFAR-10和CIFAR-100网络结构的区别只在于CIFAR-10网络的最后端用两个全连接层将输出向量的维度从120先降至84，再降至10，而CIFAR-100网络的最后端只用一个全连接层直接将输出向量的维度从120降至100。模型中采用最大池化层的原因是在保留主要特征的同时减少参数和计算量，防止过拟合，并且相比平均池化层的效果更好。采用ReLU作为激活函数的目的是使网络训练更快，因为相比sigmoid和tanh，ReLU的导数更加好求，另外当参数数值过大或过小时，sigmoid和tanh的导数接近于0，容易出现梯度消失的现象，而ReLU作为非饱和激活函数则不会出现这种现象。

| 网络层     | 参数                  | 输出大小     | 参数量 |
| ---------- | --------------------- | ------------ | ------ |
| 卷积层     | 内核大小5，输出通道10 | [10, 24, 24] | 260    |
| 最大池化层 | 内核大小3，步幅1      | [10, 22, 22] | 0      |
| 卷积层     | 内核大小5，输出通道20 | [20, 18, 18] | 5020   |
| 最大池化层 | 内核大小3，步幅1      | [20, 16, 16] | 0      |
| 全连接层   | -                     | [50]         | 256050 |
| 全连接层   | -                     | [10]         | 510    |

| 网络层     | 参数                  | 输出大小     | 参数量 |
| ---------- | --------------------- | ------------ | ------ |
| 卷积层     | 内核大小5，输出通道6  | [6, 28, 28]  | 456    |
| 最大池化层 | 内核大小3，步幅1      | [6, 26, 26]  | 0      |
| 卷积层     | 内核大小5，输出通道16 | [16, 22, 22] | 2416   |
| 最大池化层 | 内核大小3，步幅1      | [16, 20, 20] | 0      |
| 全连接层   | -                     | [120]        | 768120 |
| 全连接层   | -                     | [84]         | 10164  |
| 全连接层   | -                     | [10]         | 850    |



### \subsection{实验指标}

\label{subsec:model}

在模型训练过程中，一些重要的实验指标会被记录下来。首先是验证集准确率，在本章的实验设置中，服务器将有一个预留的验证集，用于在每轮通信后对聚合的联邦模型的性能进行测试。所有客户端的最大训练时间也会被记录，因为在理想情况下，所有客户端是以并行的方式训练模型的，因此在同步通信的场景中，联邦模型的总训练时间受限于每个通信轮次中花费时间最多的客户端。除此之外，实验过程中也会记录累计的上传和下载开销，下载的开销来自于服务器将联邦模型分发给所有的客户端，而上传的开销来自于客户端将训练后的模型发回给服务器，因此上传和下载的开销与模型的参数量是直接相关的。最后，为了更准确地对客户端的计算量进行估计，实验过程中会计算累计的模型浮点运算数（Floating point operations，FLOPs）\cite{tang2018flops}，这个指标计算模型前向传播和反向传播过程中的加法和乘法的使用次数，可以用来衡量模型的复杂度。对于全连接层的前向传播，FLOPs是$2\left|\mathcal{D}_{n}\right|C_{in}C_{out}\times \Delta\alpha_l$\cite{jiang2022model}，其中$C_{in}$表示输入神经元数，$C_{out}$表示输出神经元数。对于全连接层的反向传播，FLOPs是$2\left|\mathcal{D}_{n}\right|C_{in}C_{out}\times (1+\Delta\alpha_l)$，因为参数的梯度计算量与输入的稀疏度有关，而输入向量并不是稀疏的，相反输入的梯度计算量与参数的稀疏度有关。对于卷积层的前向传播，FLOPs是$2\left|\mathcal{D}_{n}\right|C_{in}C_{out}K^2HW\times \Delta\alpha_l$，其中$C_{in}$表示输入通道数，$C_{out}$表示输出通道数，$K$表示内核大小，$H$和$W$分别表示输出特征图的高度和宽度。对于卷积层的反向传播，FLOPs的计算与全连接层类似，此处不再赘述。后续的实验将会按需求选择不同的实验指标。



### \subsection{超参数}

\label{subsec:superparameter}

在实验过程中，一些超参数的设置将是固定不变的，而另一些超参数的设置是需要讨论的。固定的参数列在表\ref{tab:super-setting}中，其中需要说明的是，简单的实验证明，在训练结束前停止对模型拓扑的调整会有轻微的性能提升，因此结束动态剪枝通信轮次$T_{end}$被固定为通信总轮数$T$的3/4。剪枝算法中的间隔通信轮数$\Delta{T}$和初始剪枝比例系数$p^0$，以及聚合算法中的权衡系数$\gamma$的设置将在后续的超参数实验中进行讨论。

| 超参数                    | 固定值 | 超参数          | 固定值 |
| ------------------------- | ------ | --------------- | ------ |
| 客户端数量$N$             | 10     | 本地迭代次数$E$ | 10     |
| 学习率$\eta$              | 0.01   | 小批量的大小    | 32     |
| L2正则化强度              | 0.001  | 动量参数        | 0.9    |
| 动态剪枝通信轮次$T_{end}$ | 3/4$T$ |                 |        |

