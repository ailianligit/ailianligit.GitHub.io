联邦学习是一种新型的分布式机器学习范式，能够在保护隐私的基础上，利用众多边缘设备的本地数据和计算资源进行联邦大模型的训练。然而，在联邦学习的场景中，通信带宽和边缘设备的计算资源通常是受限的，因此联邦大模型需要经过模型压缩才能在边缘设备上进行训练和推理。此外，联邦学习场景下客户端之间的数据分布通常是非独立同分布的，并且可能会带有标签噪声，导致客户端之间样本标签质量存在差异，影响联邦大模型的训练效果。本文提出了一个新颖的基于剪枝优化的大模型联邦训练算法，该算法利用基于动态拓扑的模型剪枝对联邦大模型进行压缩，在客户端本地进行训练后，基于贡献评估对所有的剪枝模型进行聚合，最后基于模型残差将剪枝模型恢复为联邦大模型。在客户端样本标签存在差异的情况下，相比其他基线算法，本文提出的基于剪枝优化的大模型联邦训练算法能够取得最优的性能，同时减少通信和计算资源的开销。



Federated learning is a new type of distributed machine learning paradigm, which can use the local data and computing resources of many edge devices to train large federated model on the basis of privacy protection. However, in the scenario of federated learning, the communication bandwidth and computing resources of edge devices are usually limited, so the federated large model needs to be trained and reasoned on edge devices after model compression. In addition, the data distribution between clients in a federated learning scenario is usually non-independent and identically distributed, and may contain label noise, resulting in differences in the quality of sample labels between clients, which affects the training effect of the federated large model. This paper proposes a novel large model federation training algorithm based on pruning optimization. This algorithm uses model pruning based on dynamic topology to compress the federated large model. After local training on the client, all pruned models are aggregated based on contribution evaluation, and finally the pruned model is restored to a federated large model based on model residuals. In the case of differences in client sample labels, compared with other baseline algorithms, the large model federated training algorithm based on pruning optimization proposed in this paper can achieve optimal performance while reducing the overhead of communication and computing resources.



本文提出了一个新颖的基于剪枝优化的大模型联邦训练算法。该算法的目标是训练一个性能良好的全局大模型，减少通信和计算资源的消耗，同时对客户端样本标签质量存在差异的情况具备鲁棒性。

首先，本文利用基于动态拓扑的模型剪枝算法减少联邦大模型训练所需的通信量和计算量。模型剪枝算法使用非结构化的剪枝结构和权重绝对值大小的剪枝标准，使用逐层剪枝的方法，使得剪枝模型的网络拓扑在时间的维度上动态变化。此外，模型剪枝算法还利用动态稀疏训练的思想对剪枝模型的参数空间进行深入且有效的探索，充分发挥了联邦模型的过参数化特性。本文通过消融实验证实了模型剪枝算法的有效性，通过超参数实验说明选择合适的剪枝率和初始调整系数有助于训练出一个性能良好的联邦模型，同时减少对通信和计算资源的消耗。

其次，本文将基于贡献评估的模型聚合算法应用于联邦学习的框架中，使得联邦大模型在客户端样本标签存在差异时仍然保持一定的精度。模型聚合算法首先利用余弦梯度夏普利值对客户端的贡献进行评估，然后将其与客户端的聚合权重联系起来，通过贡献评估调整客户端在联邦训练中的参与程度。本文通过实验验证了基于余弦梯度夏普利值的贡献评估方法的有效性，然后通过消融实验证实了基于贡献评估的模型聚合算法在客户端样本标签质量存在差异时的有效性，并通过超参数实验说明选择合适的初始权衡系数有助于探索客户端的最优聚合权重，并在训练后期抑制联邦模型对噪声数据的过度拟合，提升联邦模型的训练效果。

综上所述，在每个通信轮次，基于动态拓扑的模型剪枝算法首先将联邦大模型修剪为稀疏的剪枝模型，客户端本地训练完成后，基于贡献评估对分布在不同客户端上的剪枝模型进行聚合，最后基于模型残差将剪枝模型恢复为联邦大模型。本文通过对比实验说明，相比其他现有的联邦学习和联邦剪枝的基线算法，本文提出的大模型联邦训练算法能够在固定的通信和计算开销下实现最优的准确率，同时在客户端样本标签质量存在差异的情况下损失最少的精度。

最后，本文提出的基于剪枝优化的大模型联邦训练算法有改进的空间。例如，可以根据客户端的计算能力自适应地选择剪枝率，使得大模型联邦训练算法能够适应边缘计算资源异构的场景。此外，本文还探究了在客户端样本标签质量存在差异时，根据客户端的贡献大小自适应剪枝对联邦大模型训练效果的影响，并通过实验证实了基于贡献评估自适应剪枝方法的可行性。
