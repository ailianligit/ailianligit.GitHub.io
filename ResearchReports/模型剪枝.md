## 剪枝的原因

- 神经网络网络通常是过度参数化的（存在大量冗余权重或神经元）

## 剪枝的方式

- 衡量权重、神经元、filter的重要性
  - 权重：绝对值、寿命（lifelong）
  - 神经元：在给定数据集上不为零的次数、连接的权重绝对值之和
  - filter：kernel权重绝对值之和
- 平衡模型**大小**与**性能**
  - 剪枝率的选取
  - 微调fine-tune

## 剪枝分类1：非结构化剪枝

- 删除不重要的**权重**（即神经元之间的连接）
  - 没有有效地减少计算时间，因为大多数移除的权重来自全连接层，但计算成本集中在卷积层
- 高度的**参数稀疏性**
   - 难以压缩内存中的参数
   - 需要专门的硬件/库来加速模型训练

<img src="https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202212/20221213_1670912176.png" alt="image-20221213132928209" style="zoom:33%;" />

## 剪枝分类2：结构化剪枝

- 删除一些不重要的**模型结构**（例如，卷积filter）而不引入**稀疏性**
- 得到原始神经网络的子图或子模型
- 包含**更少的参数**并有助于**减少通信和计算开销**

<img src="https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202212/20221213_1670912181.png" alt="image-20221213133508941" style="zoom:33%;" />

## 模型剪枝的合理性

- **The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks**

<img src="https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202212/20221213_1670912184.png" alt="image-20221213135604576" style="zoom:33%;" />

- **Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask**
  - 正负号是能否训练的关键
  - 直接从具有随机权重的网络中剪枝

<img src="https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202212/20221213_1670912189.png" alt="image-20221213141509767" style="zoom: 50%;" />

- **Rethinking the Value of Network Pruning**
  - 训练一个大型的、过度参数化的模型不是必要的
  - 学习到的大型模型的“重要”权重对修剪后的小型模型是没有用的
  - 修剪后的架构本身对最终模型的效率更为重要

<img src="https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202212/20221213_1670912194.png" alt="image-20221213140154219" style="zoom: 50%;" />