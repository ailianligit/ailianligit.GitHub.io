# \chapter{实验与结果}

\label{chap:experiment}

## \section{实验设置}

\label{sec:setting}

### \subsection{实验环境}

\label{subsec:environment}

本章的所有仿真实验程序使用Python程序设计语言及其开源的机器学习库Pytorch进行开发，实验使用的操作系统平台是Ubuntu 20.04.1，并行计算平台是CUDA 11.0，Nvidia驱动版本是510.54。在程序运行过程中，整个联邦学习的框架和训练数据集在单个NVIDIA GeForce RTX 3090 GPU上加载和计算。



### \subsection{数据集及其划分方式}

\label{subsec:dataset}

本章的实验将基于MNIST\cite{lecun1998gradient}和CIFAR-10\cite{krizhevsky2009learning}这两个数据集进行联邦模型的训练，并记录相关的实验结果。MNIST是一个黑白的手写数字数据集，训练集有60000张图片，测试集则有10000张图片，每张图片的大小是28*28，标签共有10类。CIFAR-10是大小为32*32的彩色图片数据集，所有数据分为10个类别，训练集中每个类别有5000张图片，测试集中每个类别有1000张图片。后续的实验将会按需求选择不同的数据集。

在本章的实验中，数据集的划分方式将会主要有三种。第一种是样本标签独立同分布，假设所有客户端的数据都是从同一个分布中独立采样，因此所有客户端的样本标签的分布是相同的，并且客户端之间的数据量是相同的。第二种是样本标签非独立同分布，划分方式是尽量让每个客户端上的样本标签分布不同，因此某个类别在不同客户端上的概率分布的随机向量采样自狄利克雷分布，狄利克雷分布中有个分布参数$\beta$，$\beta$的值越大，则随机向量接近于从均匀分布中采样，而$\beta$越小，随机向量接近于从一个非常集中的分布中采样。若无专门的说明，本章实验在非独立同分布情况下采用的分布参数$\beta=0.5$。第三种是在样本标签独立同分布的基础上，对不同的客户端，将不同比例的正确的样本标签换成错误的样本标签。后续的实验将会按需求选择不同的数据集划分方式。



### \subsection{模型}

\label{subsec:model}

本文中的大模型的含义是相比边缘设备上的经过压缩的模型，在服务器上进行测试的模型规模更大。本章所有实验所采用的联邦模型都基于卷积神经网络（convolutional neural networks，CNN）\cite{lecun1998gradient}，虽然这些模型的规模远远小于现今最为前沿的一些大规模预训练模型，但这些超大规模的模型即使经过压缩，其训练所需的通信、计算和存储资源仍然远远超出一些边缘设备的能力。本章实验采用的卷积神经网络在MNIST和CIFAR-10数据集上已经可以取得不错的精度，因此，将这些卷积神经网络作为联邦模型进行训练已经可以满足实验的需求。表\ref{tab:mnist-model}和表\ref{tab:cifar-model}展示了本章实验在MNIST和CIFAR-10两类数据集上采用的模型结构、每层的输出和参数量。模型中采用最大池化层的原因是在保留主要特征的同时减少参数和计算量，防止过拟合，并且相比平均池化层的效果更好。采用ReLU作为激活函数的目的是使网络训练更快，因为相比sigmoid和tanh，ReLU的导数更加好求，另外当参数数值过大或过小时，sigmoid和tanh的导数接近于0，容易出现梯度消失的现象，而ReLU作为非饱和激活函数则不会出现这种现象。

\begin{table}[h] %voc table result
	\centering
	\caption{MNIST数据集使用的模型架构}
	\begin{tabular}{c|*{20}{c}}
		\toprule
        网络层 & 参数 & 输出大小 & 参数量\\
        \midrule
        卷积层 & 内核大小5，输出通道10 & [10, 24, 24] & 260\\
        最大池化层 & 内核大小3，步幅1 & [10, 22, 22] & 0\\
        卷积层 & 内核大小5，输出通道20 & [20, 18, 18] & 5020\\
        最大池化层 & 内核大小3，步幅1 & [20, 16, 16] & 0\\
        全连接层 & - & [50] & 256050\\
        全连接层 & - & [10] & 510\\
        \bottomrule
    \end{tabular}
\label{tab:mnist-model}
\end{table}

\begin{table}[h] %voc table result
	\centering
	\caption{CIFAR-10数据集使用的模型架构}
	\begin{tabular}{c|*{20}{c}}
		\toprule
        网络层 & 参数 & 输出大小 & 参数量\\
        \midrule
        卷积层 & 内核大小5，输出通道6 & [6, 28, 28] & 456\\
        最大池化层 & 内核大小3，步幅1 & [6, 26, 26] & 0\\
        卷积层 & 内核大小5，输出通道16 & [16, 22, 22] & 2416\\
        最大池化层 & 内核大小3，步幅1 & [16, 20, 20] & 0\\
        全连接层 & - & [120] & 768120\\
        全连接层 & - & [84] & 10164\\
        全连接层 & - & [10] & 850\\
        \bottomrule
    \end{tabular}
\label{tab:cifar-model}
\end{table}



### \subsection{实验指标}

\label{subsec:index}

在模型训练过程中，一些重要的实验指标会被记录下来。首先是验证集准确率，在本章的实验设置中，服务器将有一个预留的验证集，用于在每轮通信后对聚合的联邦模型的性能进行测试。其次，实验过程中也会记录累计的上传开销，这部分开销主要来自于客户端将训练后的模型发回给服务器，因此上传的开销与模型的参数量是直接相关的。最后，为了更准确地对客户端的计算量进行估计，实验过程中会计算累计的浮点运算数（floating point operations，FLOPs）\cite{tang2018flops}，这个指标计算模型前向传播（forward propagation）和反向传播（backward propagation，BP）过程中的加法和乘法的使用次数，可以用来衡量模型的复杂度。对于全连接层的前向传播，FLOPs是$2\left|\mathcal{D}_{n}\right|C_{in}C_{out}\times (1-\boldsymbol{\lambda}_l)$\cite{jiang2022model}，其中$C_{in}$表示输入神经元数，$C_{out}$表示输出神经元数。对于全连接层的反向传播，FLOPs是$2\left|\mathcal{D}_{n}\right|C_{in}C_{out}\times (2-\boldsymbol{\lambda}_l)$，因为参数的梯度计算量与输入的稀疏度有关，而输入向量并不是稀疏的，相反输入的梯度计算量与参数的稀疏度有关。对于卷积层的前向传播，FLOPs是$2\left|\mathcal{D}_{n}\right|C_{in}C_{out}K^2HW\times(1-\boldsymbol{\lambda}_l)$，其中$C_{in}$表示输入通道数，$C_{out}$表示输出通道数，$K$表示内核大小，$H$和$W$分别表示输出特征图的高度和宽度。对于卷积层的反向传播，FLOPs的计算与全连接层类似，此处不再赘述。后续的实验将会按需求选择不同的实验指标。



### \subsection{超参数}

\label{subsec:superparameter}

在实验过程中，一些超参数的设置将是固定不变的，而另一些超参数的设置是需要讨论的。本文参考了PruneFL\cite{jiang2022model}和FedDST算法\cite{bibikar2022federated}的实验设置，将固定的超参数列在表\ref{tab:super-setting}中。基于动态拓扑的模型剪枝算法中的整体剪枝率$\alpha$和初始调整系数$\lambda^0$，以及基于贡献评估的模型聚合算法中的初始权衡系数$\gamma^0$的设置将在\ref{sec:exp-superparameters}节的超参数实验中进行讨论。

\begin{table}[h] %voc table result
	\centering
	\caption{超参数固定值}
	\begin{tabular}{*{2}{c}|*{2}{c}}
		\toprule
        超参数 & 固定值 & 超参数 & 固定值\\
        \midrule
        客户端数量$N$ & 10 & 本地迭代次数$E$ & 10\\
        学习率$\eta$ & 0.01 & 小批量的大小 & 32\\
        L2正则化强度 & 0.001 & 动量参数 & 0.9\\
        \bottomrule
    \end{tabular}
\label{tab:super-setting}
\end{table}
