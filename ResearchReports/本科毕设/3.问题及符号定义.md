# \chapter{基于剪枝优化的大模型联邦训练算法}

\label{chap:algorithm}

## \section{问题及符号定义}

\label{sec:definition}

本文提出的大模型联邦训练算法利用分布在众多客户端上的本地数据集，以协作的方式训练一个性能良好的联邦大模型。在本文的联邦学习框架中，共有$N$个客户端和一个与所有客户端直接相连的服务器。每个客户端$n\in\{1,2,\cdots,N\}$都有一个本地数据集$\mathcal{D}_{n}$和关于模型参数向量$\boldsymbol{\theta}$的本地经验风险$F_n(\boldsymbol{\theta}):=\frac{1}{\left|\mathcal{D}_{n}\right|}\sum_{i\in\mathcal{D}_{n}}f_i(\boldsymbol{\theta})$，其中损失函数（loss function）$f_i(\boldsymbol{\theta})$表示数据样本$i$的模型输出和期望输出之间的差距。对于回归任务，损失函数一般定义为均方误差（mean square error，MSE），对于分类任务，损失函数一般为交叉熵（cross-entropy）函数。联邦学习的目标是尝试寻找一个最优的模型参数向量$\boldsymbol{\theta}$使得全局经验风险最小化，目标函数可以表示为式（\ref{eq:optimization-goal}）的形式。其中，$r_n$表示客户端的聚合权重，且$\sum_{n=1}^{N}r_n=1$。在FedAvg算法\cite{mcmahan2017communication}中，$r_n:=\frac{\left|\mathcal{D}_{n}\right|}{\sum_{n=1}^N\left|\mathcal{D}_{n}\right|}$，而本文将基于贡献评估对这一聚合权重进行改进。

\begin{equation}
    \label{eq:optimization-goal}
	\min_{\boldsymbol{\theta}}F(\boldsymbol{\theta})\triangleq\sum_{n=1}^N r_nF_n(\boldsymbol{\theta})
\end{equation}

在每个通信轮次$t=1,2,\cdots,T$开始时，客户端$n$从服务器端接收到本地模型参数向量$\boldsymbol{\theta}_n^t$，在本地训练完成后，这些客户端的模型聚合定义为$\boldsymbol{\theta}_n:=\sum_{n=1}^N r_n \boldsymbol{\theta}_n^t$。在本地训练的过程中，为了改进模型的参数使得全局经验风险最小化，常用且有效的方法是随机梯度下降（stochastic gradient descent，SGD）算法。在每个本地训练轮次$e=1,2,\cdots,E$开始时，客户端$n$从$\mathcal{D}_{n}$中随机采样小批量数据，然后，基于小批量数据的预测输出和实际输出计算损失函数$f_n(\boldsymbol{\theta}^{t,e}_n)$及其梯度$\nabla f_n(\boldsymbol{\theta}^{t,e}_n)$，最后，基于随机梯度下降对参数进行更新：
\begin{equation}
    \label{eq:sgd}
	\boldsymbol{\theta}^{t,e}_n\leftarrow\boldsymbol{\theta}^{t,e-1}_n-\eta^t\nabla f_n(\boldsymbol{\theta}^{t,e-1}_n)
\end{equation}

其中$\eta^t$表示学习率，学习率太大，模型难以收敛，学习率太小，模型收敛慢或根本无法学习。带有动量（momentum）的随机梯度下降使用指数加权平均后的梯度进行参数更新，因为动量带有之前梯度的信息，因此可以加快收敛的速度，同时跳过一些局部的极小值点。此外，随机梯度下降算法中还可以加入权重衰减（weight decay）项，这相当于在目标函数中加上了正则（regularization）项，通过惩罚过大的权重，限制模型的复杂度，防止过拟合现象的发生。

\newpage

除了对联邦模型的性能进行优化，本文的优化目标还包括对通信开销和计算成本的优化，同时需要减轻样本标签质量差异对模型性能的影响，而针对这两部分的优化将分别在\ref{sec:dynamic-pruning}节和\ref{sec:contribution-aggregation}节进行阐述，基于剪枝优化的大模型联邦训练的框架将在\ref{sec:framework}节进行概述。本文中用到的大部分符号及其含义都在表\ref{tab:superparameters}中列出。

\begin{table}[h] %voc table result
	\centering
	\caption{符号定义}
	\begin{tabular}{*{2}{c}|*{2}{c}}
		\toprule
        符号 & 定义 & 符号 & 定义\\
        \midrule
        $n$，$N$ & 客户端索引和总数 & $\mathcal{D}_{n}$ & 第n个客户端的本地数据集\\
        $t$，$T$ & 通信轮次和总轮数 & $e$，$E$ & 本地训练轮次和总轮数\\
        $\boldsymbol{\theta}$ & 联邦模型参数 & $\hat{\boldsymbol{\theta}}$ & 剪枝模型参数\\
        $f(\boldsymbol{\theta})$ & 损失函数 & $F(\boldsymbol{\theta})$ & 经验风险函数\\
        $\eta^t$ & 学习率 & $\nabla f(\boldsymbol{\theta})$ & 随机梯度\\
        $\Delta\boldsymbol{\theta}^{t}_n$ & 客户端参数更新 & $r_n^{t}$ & 客户端聚合权重\\
        $\boldsymbol{u}_n^t$ & 归一化参数更新 & $\boldsymbol{U}^t$ & 聚合参数更新\\
        $\psi_n^t$ & 余弦梯度夏普利值 & $\gamma^t$ & 聚合算法权衡系数\\
        $\alpha$ & 整体剪枝率 & $\lambda^t$ & 剪枝算法调整系数\\
        \bottomrule
    \end{tabular}
\label{tab:superparameters}
\end{table}
