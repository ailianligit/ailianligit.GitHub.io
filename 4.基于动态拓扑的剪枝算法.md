# 基于动态拓扑的剪枝算法

## 动态剪枝

In-Time Over-Parameterization【Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training】

dropout的子采样和聚合效应：正则化效果【Improving neural networks by preventing co-adaptation of feature detectors  】

从头训练一个具有固定稀疏度（静态）的网络会导致性能下降，训练相同参数数量的密集网络会获得更好的结果【The lottery ticket hypothesis at scale】【Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization】观察到静态稀疏训练收敛到比动态稀疏训练损失更高的解决方案

ERK的有效性【RigL：Rigging the Lottery: Making All Tickets Winners】

SET随机增长新连接，提高了small-dense的性能，但饱和度为75%，表明随即增长新连接的局限性

因为移除这些连接已被证明对损失的影响最小【Learning both weights and connections for efficient neural network】【Detecting dead weights and units in neural networks  】。 接下来，它激活具有高梯度的连接，因为这些连接有望最快地减少损失

与稀疏基元相结合，可以训练非常大的稀疏模型，否则是不可能的。

允许在整个训练时间内进行连续参数探索，而不是从密集和预训练的模型中继承权重，从而在时空流形中执行过参数化，这可以显着提高稀疏训练的可表达性。【Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training】

最近，彩票假设 (LTH) (Frankle & Carbin, 2019) 显示了从头开始训练子网络（稀疏训练）以匹配密集网络性能的可能性。 然而，这些“中奖彩票”是在完全密集的超参数化过程（迭代修剪完全融合网络）的指导下找到的，以及通过部分密集超参数化（初始化时修剪（Lee 等人， 2019；Wang 等人，2020；de Jorge 等人，2020)）或没有过度参数化（随机初始化的静态稀疏训练（Mocanu 等人，2016；Evci 等人，2019）））通常无法 以匹配其密集对应物所达到的准确性。 一个常识性的解释是，与密集训练相比，稀疏训练，尤其是在极高稀疏度下，不具有过参数化特性，因此可表达性较差。 解决这个问题的一种方法是利用从密集训练中学到的知识，例如 LTH（Frankle & Carbin，2019）。 虽然有效，但过度参数化密集训练所附带的计算成本和内存要求令人望而却步。



## 非结构化剪枝



## 按层剪枝

bias bn层不剪



## 剪枝标准



## 消融实验

动态剪枝 静态剪枝 剪枝率为0 小型密集模型？



## 超参数实验

超参数变化：$\Delta T$ 和 R

--distribution iid dirichlet
--beta 0.1 0.5

--sparsity 0.5

--need-readjust
--readjustment-ratio 0.0 0.1 0.3 0.5 0.7 0.9

--outfile
--device

| 调整率 | 0.0           | 0.01          | 0.1           | 0.3       | 0.5           | 0.7       | 0.9           |
| ------ | ------------- | ------------- | ------------- | --------- | ------------- | --------- | ------------- |
| 0.1    | **0.52653** a | 0.139580001 a | **0.42905** a | 0.3969 b  | 0.41441 a     | 0.29787 b | 0.427239993 a |
| 0.5    | **0.61985** a | 0.61612 a     | **0.61665** a | 0.59643 a | 0.600710005 a | 0.60968 a | **0.6187** a  |
| iid    | **0.66816** a | **0.6625** a  | 0.655079997 a |           |               |           |               |

| 调整率 | 0.0           | 0.01         | 0.1       | 0.3         | 0.5         | 0.7          | 0.9          |
| ------ | ------------- | ------------ | --------- | ----------- | ----------- | ------------ | ------------ |
| 0.1    | **0.52653** a | 0.438265 a+b | 0.44275 b | 0.40205 a+b | 0.4853 b    | 0.428515 a+b | 0.416295 a+b |
| 0.5    | **0.61985** a | 0.62811      | 0.61379   | 0.60474     | 0.60608     | 0.61489      | 0.61472      |
| iid    | **0.66816** a | 0.65834      | 0.65589   | 0.64009     | 0.661259997 | 0.66066      | 0.65725      |

| 调整间隔 | 0.0           | 1       | 4       | 7           | 10      | 20      | 50      |
| -------- | ------------- | ------- | ------- | ----------- | ------- | ------- | ------- |
| 0.1      | **0.52653** a | 0.43279 | 0.29347 | 0.501229995 | 0.42935 | 0.13695 | 0.41495 |
| 0.5      | **0.61985** a | 0.60612 | 0.58966 | 0.62455     | 0.63958 | 0.60875 | 0.58828 |
| iid      | **0.66816** a | 0.63779 | 0.65832 | 0.66487     | 0.64539 | 0.65741 | 0.64983 |