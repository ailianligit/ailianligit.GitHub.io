### [Efficient Deep Learning](https://hanlab.mit.edu/)

![image-20230417173556554](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202304/20230417_1681724160.png)

- 模型压缩和加速、TinyML和智能物联网 (IIoT)、高效训练和推理

- Efficient Deep Learning of HAN Lab

  - [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://ofa.mit.edu/)
    - 解耦训练和搜索
    - 渐进式收缩算法、广义剪枝方法
    - 可以获得数量惊人的子网络

  - [MCUNet](https://mcunet.mit.edu/)
    - 在物联网设备上进行微型深度学习
    - 算法-系统协同设计框架
    - 高效的网络架构搜索（TinyNAS）和轻量推理引擎（TinyEngine）
  - [TinyML](https://tinyml.mit.edu/)：嵌入式机器学习
    - TinyML项目旨在通过需要更少的计算、更少的工程师和更少的数据来提高深度学习AI系统的效率，以促进边缘AI和AIoT的巨大市场。
    - 优势：减少延迟时间、节能、减少带宽、数据隐私
    - [TensorFlow Lite for Microcontrollers](https://www.tensorflow.org/lite/microcontrollers)：在微控制器上推理



### 神经架构搜索（NAS）

![img](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202304/20230417_1681722506.png)

- **Search Space**
- **Search Strategy**
  - Reinforcement Learning (RL)
  - Evolutionary Algorithm (EA)
  - Gradient Based (GB)
- **Performance Estimation Strategy**



### [One-shot NAS](https://zhuanlan.zhihu.com/p/74985066)

![img](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202304/20230417_1681722554.png)

- [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://ofa.mit.edu/)
  - ICLR2020：OFA(Once for All): 高效率部署：https://zhuanlan.zhihu.com/p/164695166



### [Ray: Distributed AI Framework](https://thenewstack.io/how-ray-a-distributed-ai-framework-helps-power-chatgpt/)

- distributed computing ecosystem as a service



### [Hyperparameter Optimization](https://speakerdeck.com/richardliaw/a-modern-guide-to-hyperparameter-optimization)

- [Ray Tune](https://docs.ray.io/en/latest/tune/index.html)：可扩展的超参数调整



### 研究方向：

- 对RNN、Transformer、生成模型的压缩
- RNN、Transformer、生成模型等大模型的训练



### 找老师聊的内容：

- 毕设的查重
- 选方向：周老师、优化、系统、AI、大模型训练、模型压缩、分布式机器学习
- 实习理由：
  - 倾向于硕士毕业找工作，实习了解行业需求
  - 大模型时代，工业界更为关注的研究方向
  - 假期不能留在学校
  - 师兄找实习困难
  - 算法岗越来越要求开发能力



### 研究方向：

图联邦学习

**模型压缩：剪枝 蒸馏**

NAS

超参数优化

公平联邦学习

**边缘计算任务调度**

联邦学习模型异构 

个性化联邦学习：联邦学习数据异构？

**大模型训练？**

时间序列？

few-shot？

预训练大模型？

TinyML？

高质量数据、高算力、大模型



### 重点考虑的研究方向：

分布式机器学习、机器学习系统与运筹优化

模型压缩、大模型训练与运筹优化：NAS、超参数优化

面向边云协同网络中分布式机器学习的动态优化与资源调度



### 研究方向应考虑的角度：

好不好发论文

方便找工作：机器学习

热门中的冷门

人生价值：创造性的方向

个人喜好：运筹

数学基础：优化 统计 运筹 博弈

脚踏实地

实体经济

一门手艺

多只脚走路

科研：优化+机器学习

知识储备：MLSys、AI前沿

那一次「真正让我意识到，如果要创造影响力，你应该去写一些基础的东西，或者在工程上有所建树，而不是说在一些 research 方面有所建树。」他说。



### 就业和实习方向：

机器学习算法工程师

运筹优化算法工程师

搜广推算法工程师

风控算法工程师

AI框架开发工程师



### 就业方向应考虑的角度：

热门中的冷门

work life balance

不会被AI取代

人生价值：创造性的方向

实体经济

一门手艺

运筹优化背景+机器学习工具



### 研究生阶段应具备的技能：

模型部署：有开发能力的算法工程师

数据开发

搜广推或风控

分布式机器学习

分布式计算与系统

高性能计算

云计算与边缘计算

计算机体系结构



### 技能获取方式：

科研发论文

上课学习

项目实习经验

竞赛项目？



### 参考资料：

知乎问题

B站视频

B站专栏

prompt tuning lora PEFT

网络经济实验室、陈旭、陈亮、张晓溪

脉脉、求职软件、华为云、阿里云、运筹优化offer

边缘计算论文

NeurIPS/ICML/ICLR2023

剪枝与噪声的关系

联邦学习noniid理论

大模型训练+预训练对比学习论文

联邦NAS

联邦参数优化

参考文献文件夹

MoE、tinyml、fedml

A Coalition Formation Game Approach for Personalized Federated Learning

Resource-Adaptive Federated Learning with All-In-One Neural Composition

Federated Continual Learning with Weighted Inter-client Transfer

Federated Online Learning Based Recommendation Systems for Mobile Social Applications