## [CGSV@NeurIPS'21] Gradient-Driven Rewards to Guarantee Fairness in Collaborative Machine Learning

- 公平联邦学习：
  - 数据/模型的质量不同
  - 公平的奖励
- 贡献评估方法：余弦梯度夏普利值（CGSV）
- 奖励：从服务器下载的梯度
- 实现公平奖励的方式：下载梯度的质量（稀疏度）与上传梯度的质量相当

$$
\boldsymbol{v}_{i, t}:=\operatorname{mask}\left(\boldsymbol{u}_{\mathcal{N}, t}, q_{i, t}\right), \quad q_{i, t}:=\left\lfloor D \tanh \left(\beta r_{i, t}\right) / \max _{i^{\prime} \in \mathcal{N}} \tanh \left(\beta r_{i^{\prime}, t}\right)\right\rfloor
$$

- 为什么性能会更好？
  - 可能归因于梯度聚合步骤中通过$r_{i,t}$自适应加权，它可以动态地代表客户端本地数据集中的异质性。

$$
\boldsymbol{u}_{i, t}:=\Gamma \Delta \boldsymbol{w}_{i, t} /\left\|\Delta \boldsymbol{w}_{i, t}\right\|, \quad \boldsymbol{u}_{\mathcal{N}, t}:=\sum_{i \in \mathcal{N}} r_{i, t-1} \boldsymbol{u}_{i, t}
$$

![image-20230417152109720](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202304/20230417_1681716100.png)



## Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence

![image-20230417152222941](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202304/20230417_1681716144.png)



### [Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)

- 使用强化学习的方式直接优化带有人类反馈的语言模型。
- 步骤
  - 预训练一个语言模型 (LM) ；
  - 聚合问答数据并训练一个奖励模型 (Reward Model，RM) ；
  - 用强化学习 (RL) 方式微调 LM。

