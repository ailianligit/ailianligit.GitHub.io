### 产学研项目：大小模型协同进化

超大规模预训练模型是从弱人工智能向通用人工智能的突破性探索，解决了传统深度学习的应用碎片化难题，但性能与能耗提升不成比例的效率问题限制了参数规模继续扩张。人工智能研究将从大模型参数竞赛走向大小模型的协同进化，大模型向边、端的小模型输出模型能力，小模型负责实际的推理与执行，小模型再向大模型反馈算法与执行成效，让大模型的能力持续强化，形成有机循环的智能体系。

谷歌的BERT、Open AI的GPT-3、智源的悟道、达摩院的M6、AliceMind等大规模预训练模型取得了巨大成果，大模型的性能有了飞跃性的提升，为下游的AI模型提供很好的基础。然而大模型训练对资源消耗过大，GPT-3训练一次需要19万度电，相当于开车从地球到月球往返一圈，参数数量增加所带来的性能提升与消耗提升不成比例，让大模型的效率受到挑战。大模型的规模发展将进入冷静期，大模型与相关联的小模型协同将是未来的发展方向。大模型沉淀的知识与认知推理能力向小模型输出，小模型基于大模型的基础叠加在垂直场景的感知、认知、决策、执行能力，再将执行的结果反馈给大模型，让大模型的知识与能力持续进化，形成一套有机循环的智能系统，参与者越多，受惠者也越多，同时模型进化的速度也越快。

新的智能体系带来三个优势：一是让小模型更容易获取通用的知识与能力，在特定场景做极致优化，提升了性能与效率；二是解决了过去大模型数据集过于单一的问题，小模型在真实场景回收的增量数据，让大模型有再进化的元素；三是全社会不需要重复训练相似的大模型，模型可以被共享，让算力与能源的使用效率最大化。AI是数字经济时代的关键生产工具，给产业或学术的生产方式带来颠覆式的改变，AI基础模型让AI的生产方式极大的简化，可以更灵活的按需开发垂直领域的增量算法模型，提高模型生产的效率。另一方面，复杂系统彼此间可以更有机的融合，如城市治理的场景，云是治理中枢大脑，边端是各路摄像头及边缘设备，其中一路摄像头将看到的数据进行学习，将学习的结果反馈给治理中枢，治理中枢再将学习的成果赋能给其他类似场景的摄像头，形成不断进化的系统。新的智能体系需要克服三个挑战，一是大模型与知识和常识的融合，将以规则存在的知识利用起来，提升模型通用能力的同时也降低训练所需的数据量，让大模型从数据驱动走向**知识与数据融合驱动**；二是大小模型的**协同**机制，包含大模型的知识与能力向小模型降维迁移的有效性挑战、小模型的小样本学习向大模型的升维融合、不同维度数据的清洗与治理等；三是大模型的**可解释性与因果推理**，随着小模型对大模型的依赖上升，对大模型的信任决定是否能被广泛的使用。我们预测在未来的3年内，在个别领域将以多中心的大规模预训练模型为AI基础模型，对协同进化的智能系统进行试点探索。在未来的5年内，运用AI基础模型成为AI模型生产的标准方式，极大幅度改变生产流程及生产所需的技能。



### 产学研项目：全域隐私计算

数据安全保护与数据流通是数字时代的两难问题，破解之道是隐私计算。过去受制于性能瓶颈、技术信任不足、标准不统一等问题，隐私计算尚只能在少量数据的场景下应用。随着专用芯片、加密算法、白盒化、数据信托等技术融合发展，隐私计算有望跨越到海量数据，数据源将扩展到全域，激发数字时代的新生产力。预计未来三年，全域隐私计算技术将在性能和可解释性上有新的突破，或将出现数据信托机构提供基于隐私计算的数据共享服务。

在数字经济时代，数据成为核心生产要素，但与此同时，数据确权、数据法规、隐私保护意识、数据安全保障等因素，已成为跨组织间数据的共享与价值挖掘必须面对的课题。隐私计算融合**密码学、人工智能、芯片设计**等学科，以**多方安全计算、差分隐私、可信计算**为代表技术，可在保证数据隐私不泄露的情况下实现计算分析，为跨组织的数据共享提供可行的模式。然而**性能瓶颈、技术信任不足、标准不统一**等问题，让隐私计算尚只能在少量数据的场景下应用。

隐私计算将迎来三方面的突破，让隐私计算能被大规模应用：一是**性能与效率的跨越式提升**，包含同态加密的算法突破，降低加解密的算力需求、软硬一体的加速芯片，**针对多方安全计算和联邦学习场景进行性能优化**、更多第三方提供可信执行环境（TEE）等。二是隐私计算技术的**白盒化**，提升技术的可解释性进而强化信任度，通过开放集成能力，降低跨技术、跨模型的集成壁垒。三是**数据信托机构**的出现，作为可信第三方提供技术与运营，加速组织间的数据共享。隐私计算的技术突破将推动数据计算由私域走向全域，分析的精度与深度也随着可用的数据量增加而提升，在某些对数据量强依赖的领域效果更显著，如商业分析、风险控制、学术研究、人工智能、精准营销等。另一方面，全域隐私计算技术成熟后，有望成为数据共享的标准，数据流通的风险将大幅降低，数据所有者与数据保管者的责任边界更加明确，安全程度也更加可衡量。除了技术之外，隐私计算最大的不确定性来自于运营模式和合规标准。**运营模式尚未形成完整的体系**，让数据提供方有足够的诱因共享数据，同时保障数据质量让数据使用方有意愿付出费用。就合规标准而言，**隐私计算的合规红线并不明确，让技术发展存在较大的不确定性**，技术与标准需要在发展过程中不断地相互促进。我们预测在三年内，全域隐私计算将在性能和可解释性上有新的突破，并开始出现数据信托机构提供基于隐私计算的数据共享服务。在未来的五到十年，全域隐私计算将改变现有的数据流通方式，新型业务也将在全域数据的基础上诞生，提升全社会以数据为核心的生产效率。



### 产学研项目：大规模稀疏模型训练加速研究和应用

大规模稀疏特征的深度学习模型因其优越的特征表示和特征交叉能力，被广泛应用到商 业公司的广告、推荐及搜索等场景当中。由于一般大规模稀疏特征模型参数规模达百亿规模， 部分场景可达千亿规模，并且在商业系统中需要反复重新训练，迭代模型效果，因此大模型 的训练效率和成本的优化是模型平台的核心挑战。与 NLP/CV 等场景计算密集型大模型不一 样，稀疏特征模型参数量主要集中在**特征 Embedding** 上，这也使得基于大规模稀疏参数的大 模型优化方向和传统大模型不一样。本课题旨在对大规模稀疏特征的深度学习模型进行训练 参数通信优化、软硬件一体化优化、训练加速等方面的研究。课题组将根据研究需要提供 GPU 训练集群资源。 

建议研究方向： 1) 模型训练参数通信优化：利用**参数压缩、参数量化、参数稀疏化以及延迟更新**等手段， 降低大模型训练过程中由于参数传输带来的带宽压力。同时研究 NVSwitch+PCIe+RDMA **异构硬件通信方式优化**； 2) 软硬件一体化优化：通过定制硬件以及相应软件设计的一体化优化，打造适合大规模稀 疏特征的大模型训练框架； 3) 模型训练加速：通过大 Batch 训练方法、适合大规模稀疏特征模型的 Loss 函数，研究 提升模型训练和收敛速度。

广告系统和个性化推荐是近些年互联网上商业价值非常大的应用，底层都依赖大规模稀 疏特征的训练和推理平台。随着互联网用户的快速增长，用户行为数据也在急剧增加，各类 细粒度特征的广泛使用，使得模型从百亿到千亿甚至更高规模演变，大模型已经是广告系统 和推荐系统的标准配置；由于特征的稀疏性，使得广告和推荐类大模型跟其他 nlp/cv 大模 型，在训练和推理上有非常大的区别。虽然大模型带来了不小的业务收益，但同样带来较高 的成本，实验效率也下降了，所以如何在更少的资源上，以更快的训练速度和推理性能满足 业务频繁迭代，是大规模训练平台的挑战。我们自研的大规模分布式训练平台利用：大规模 参数服务器，**混合精度训练**，**多维度并行 pipeline**，**异构计算**等一系列方案来提升训练速 度和降低成本。近年来，软硬件一体化设计的高性能分布式训练系统也逐渐崭露头角，我们希望借助不断升级的硬件，在大规模并发、GPU-CPU 异构计算、参数通信、云原生等方面进 行探索，降低训练和推理成本，提升易用性，加快模型迭代效率。 科研目标 1) 对**大规模分布式训练平台**优化，训练成本下降 5%； 2) 通过 **GPU-CPU 异构推理加速**，提升在线服务 QPS 5%； 3) 学术研究：在大规模深度学习系统方面发布学术论文 1～2 篇，联合申请专利 1～2 篇。

近年来随着机器学习领域模型愈发庞大，分布式训练系统的性能和拓展性也面临着更大的挑战。当前为了应对这种挑战，本课题将从系统设计、通信模式、训练算法优化等方向触发，突破当前系统的性能瓶颈，在落地真实业务场景的同时，在技术上达到业内领先水平。



### 产学研项目：基于预训练模型的事实编辑与多任务精调

精细化用户行为表征是实现内容个性化分发、广告精准营销的关键技术。在真实业务场 景中，海量用户行为属于无标签样本。在传统的有监督-端到端框架内，难以发挥数据优势， 导致学习到的用户表征存在信息瓶颈。因此，构建**无监督**全生命周期的用户表征模型，是提 高业务效率的有效途径。然而，用户的预训练表征在下游任务适配时，常面临事实关联模糊、 知识更新成本高、特征鲁棒性差等问题。此外，由于模型预训练的样本来源于多个领域，如 用户活跃、广告转化、内容浏览等。这导致不同领域样本量/数据分布差异较大，预训练模 型普遍存在**领域偏差**（Domain Bias）。本课题基于预训练模型，重点研究模型知识编辑方法 和多任务精调适配策略，旨在提升预训练模型在内容推荐和广告投放中的准确性。

建议研究方向： 1) 优化大型语言模型结构(如 Bert/GPT)并进行**无监督**用户行为表征； 2) 无需重训练和微调，探索预训练模型中**事实知识**编辑方法； 3) 研究基于**少量标签样本/无标签**样本的多任务通用精调适配范式。



### 产学研项目：下一代纵向联邦深度学习架构和训练方法研究

纵向联邦深度学习（VFDL）已在广告和推荐业务场景落地应用，为数据协同业务提供了新的解决方案。当前 VFDL 主要是基于 Split Learning（SL）架构来实现纵向联邦神经网络模型的训练和推理。SL 架构易于工程实现和与现有业务系统集成。然而在广告和推荐业务实践中，SL 架构也有一些不足之处，例如，联邦训练的效率问题、联邦模型的质量问题、交集 ID 暴露等。鉴于此，本课题旨在探索除 SL 架构以外的下一代 VFDL 架构，以及新的模型训练方法，例如，不暴露交集 ID 的联邦模型训练方法。希望从架构和训练方法层面彻底解决现有 VFDL 方案在实际应用中遇到的问题。 



### 产学研项目：面向AI推理的高性能自动调优算法设计

#### 研究主题

数据中心与服务器前沿技术研究

#### 背景

人工智能（AI）在视觉、语音和自然语言处理等各个领域越来越受欢迎。一方面，数据量的爆发式增长和人工智能模型规模的指数级增长导致人工智能算法训练和推理的算力需求几乎呈指数级增长。另一方面，摩尔定律的计算能力增长趋势正在逐渐放缓，导致计算瓶颈更加严重，例如计算成本、能源消耗和生态影响不断增加。除了通过不懈的架构创新来提高效率，从源头（如FLOPS）降低AI模型的计算负担，开发更高效的方式来挖掘底层计算架构的潜力也至关重要。

深度神经网络在计算机视觉、语音识别和自然语言处理等人工智能应用中取得了举世瞩目的成功，其驱动力来自算法、数据和计算能力三个方面。计算能力不仅支持大型模型的设计，也有助于处理海量数据。随着硬件技术的不断突破，计算能力呈指数级增长，使得许多以前具有挑战性的问题可以通过更复杂的模型和更大的数据集来解决。尽管深度学习模型的计算需求不断增加，但昂贵的计算资源在现实场景中存在成本限制。例如，一些典型的神经网络很难部署在计算能力低或延迟要求严格的物联网设备中。因此，

目前，模型推理的加速主要依赖于CPU、GPU、NPU等各种加速器设备。这些设备通常有相应的加速框架，每个框架都有自己的算子实现。例如，x86 架构中有多种推理引擎（ONNX、TFLite、OpenVino 等）。这些引擎的算子实现存在差异，兼容性不够，导致模型转换时出现问题，导致开发耗时耗力。此外，推理引擎也有相应的加速算子库，如cutlass、cublas、cudnn、acl（Arm Compute Library）等。一个模型的性能不仅在不同的平台（例如，x86、cuda、arm）上存在显着差异，而且部署在不同的推理框架中也存在显着差异。选择合适的框架或算子目前需要人工调优，费时费力。此外，人类经验不能直接重复使用或转化。

为了解决选择最高效的计算框架和算子实现的挑战，我们需要开发一种工具，可以自动搜索和确定当前平台上AI模型的最佳计算实现。这不仅需要**强化学习和自动调参技术**，还需要开发者在**推理框架、算子设计、硬件特性**等方面的丰富专业知识，即成熟的**软硬件协同开发**经验。通过开发这样的工具，AI模型可以在推理过程中自动搜索并获得最佳的计算实现，确保AI模型在当前平台（云端/边缘端）拥有最高效的计算能力，并允许AI模型的快速复用。当前对其他模型或平台的经验，从而节省宝贵的人力、物力和时间。目标是通过**自动优化推理框架**，为阿里巴巴集团各项AI模型推理业务创造可观的商业价值和经济效益。

#### 目标

1. 一个框架，可以在给定的推理环境中自动搜索某些运算符的最有效实现。
2. 一个可以自动搜索某些给定限制的最佳模型架构的框架。
3. 一种可以在不牺牲性能的情况下自动分析并找到压缩给定 AI 模型以加速的最佳策略的方法。

#### 相关研究课题

1. 硬件感知协同模型压缩强化学习
2. 基于机器学习的高效计算算子设计
3. 基于强化学习的神经网络架构搜索
4. 基于机器学习的自适应模型压缩
5. 自动化轻量级AI模型设计



### 产学研项目：TinyML：电池供电设备上的深度学习系统

#### 研究主题

其他

#### 背景

现代智能城市人工智能系统由各种智能传感器提供支持。例如，交通监控传感器、雪/雨探测器、自动驾驶汽车上的激光雷达等。在这些场景中，我们经常会遇到以下挑战：

1）实时数据采集。数据量太大，无法实时上传到中心超级计算机。

2）对鲁棒性和功耗要求高。该系统通常由电池驱动。它必须能够在没有外部电源的情况下工作几个小时。

3）内存非常有限。

在这些场景中，传统的深度学习模型由于其巨大的功耗和内存占用而不能很好地工作。因此，为这些场景定制新颖的深度学习模型非常重要。

#### 目标

- 创新为电池供电设备优化的新深度架构。
- 开发用于搜索和训练深度网络的深度学习管道。
- 在公共基准数据集上验证系统性能，包括但不限于 FLOP、功率瓦数、推理速度。
- 使用 MobileNet 作为基线，在 ImageNet-1k 上实现相同的 top-1 准确度，同时减少 30% 的 FLOP。
- 在 MCU 芯片上，推理速度比 MobileNet 提高 20%。
- 在 MCU 芯片上，与 MobileNet 相比，电池电量消耗减少 20%。
- 发表CCF-A论文1~2篇。

#### 相关研究课题

- 深度网络修剪和量化
- 轻量级视觉转换器
- TinyML 的神经架构搜索
- 边缘设备的延迟和功耗预测



### 阿里巴巴项目制实习生

- **研究型实习生 - 联邦学习中的模型融合与适配**：联邦学习作为一种机器学习范式，能够联合不同参与方、进行联合训练。譬如联合多个端设备的数据和算力、在云端服务器的协调下完成数据不出域的训练。但如何更好的将不同参与方的模型进行融合、以及如何将融合后的模型更好地适配给不同的使用方，这些挑战仍需更好的去解决，以便在广阔的应用场景中发挥作用。
- **研究型实习生-基于在线深度学习的探索与利用（Exploration）**：在线深度学习正逐渐在工业界及学术界获得越来越高的关注与研究，对于新品的冷启动依然是一个非常有挑战的问题，比如新的广告或者商品创建以后，如何能够在获得少量展示pv的情况下更准确地预估其点击率，从而缩短新品的冷启动时间。为了达到冷启动加速的目的，简单而言可以从以下几个方面开展：1）寻找一种更合适的探索和利用的机制，通过利用少量的流量来进行新品的探索，通过合理的机制优化和理论研究，从而让新品获得更快的速度完成冷启动，同时促进长期广告/推荐生态更健康的发展；另一个方面是，2）通常新品细粒度的特征比较缺乏，比如统计特征少或者不置信，然而新品的粗粒度的特征可以有更好的泛化性，或者利用相似商品的信息比如图网络来进行特征增强等方面在模型预估和特征方面针对新品做加强。这里会重点针对第一个方面展开研究，有针对性的解决实际系统中的冷启动问题，探索出一套新的在线深度学习解决方案。
- **研究型实习生- 【2021级华南理工联合培养ART专项】+ 基于预训练语言模型的小样本学习算法研究**：预训练语言模型的提出深刻地推动了自然语言处理（Natural Language Processing，NLP）的发展。当这些预训练语言模型在无标注的语料库上进行自监督训练后，只需要在特定任务、特定领域的训练集上进行微调，即可获得所需的用于下游NLP任务的模型。预训练语言模型在很多应用场景都有很大的需求。然而，在互联网环境下，由于大量新的领域不断涌现，新的任务需求不断被提出；在每个领域、每个NLP任务上都积累足够多的训练数据，将耗费大量的时间、人力与物力，无法满足模型快速生成与迭代的要求，从而对各种业务的算法支持带来困难。迁移学习技术可以利用其他领域的知识或数据来帮助新的领域下模型的学习。其主要缺点在于，模型无法在训练数据极少的情况下，快速适应新的任务需求。因此，研究基于预训练语言模型的小样本学习（Few-Shot Learning）算法，对于构建任务自适应、领域自适应的NLP模型，支持多样化的实际业务，有重要的现实意义。
- **研究型实习生-【2021级华南理工联合培养ART专项】+ 基于预训练语言模型的多任务知识蒸馏算法**：预训练语言模型的提出深刻地推动了自然语言处理（Natural Language Processing，NLP）的发展。当这些预训练语言模型在无标注的语料库上进行自监督训练后，只需要在特定任务、特定领域的训练集上进行微调，即可获得所需的用于下游NLP任务的模型。预训练语言模型在很多应用场景都有很大的需求。然而，预训练语言模型并不是完美无瑕的，这类模型仍然存在以下两个问题：（1）模型参数量太大：举例而言，BERT-base 模型能够包含一亿个参数，较大的 BERT-large 甚至包含 3.4 亿个参数。显然，很难将这种规模的模型部署到资源有限的环境（例如移动设备或嵌入式系统）当中。（2）训练/推理速度慢：在基于 Pod 配置的 4 个 Cloud TPUs（总共 16 个 TPU 芯片）上对 BERT-base 进行训练，或者在 16 个 Cloud TPU（总共 64 个 TPU 芯片）上对 BERT-large 进行训练，每次预训练都需要至少 4 天的时间才能完成。而BERT的推理速度更是严重影响到了需要较高QPS的线上场景，部署成本非常高。提出基于预训练语言模型的知识蒸馏算法，以及从蒸馏算法训练小模型，沉淀到阿里巴巴的算法库 发表CCF-A/B类会议/期刊论文一篇以上
- **研究型实习生 - 深度学习模型快速压缩**：随着深度学习模型参数规模日益增长、以及大模型在各类任务效果的显著提升，模型压缩也成为解决推理延迟和部署成本等挑战的关键技术。模型压缩需要在模型精度和参数或计算压缩率之间取得最佳平衡，主流模型压缩技术为了取得更好的效果，通常会涉及微调训练（例如量化感知训练、稀疏微调训练等），这使得模型压缩本身会引入额外的时间和算力开销，并且开销会随着模型规模增长而变得更加显著。如何既“好”又“快”地实现模型压缩，能够从（包括但不局限于）以下方面进行探索研究：(1)训练后模型压缩(Post-Training Compression)，训练后量化(PTQ)是被验证有效且相对易用的模型压缩技术，可以探索将类似方法拓展应用至稀疏、更低比特量化、混合精度等更多情况；(2)参数高效(Parameter-Efficient)压缩训练，在大模型上进行全量参数的微调训练对算力资源有很高要求，叠加模型压缩引入的额外参数，会进一步增大算力资源开销，降低可训练参数或者进行高效的参数初始化，是提升压缩训练效率的重要因素。希望研究型实习生候选人具备以下能力背景： (1) 熟悉深度学习基本原理、主流深度学习算法及应用、以及至少一种主流框架(PyTorch、TensorFlow等)； (2) 熟悉至少一类主流模型压缩算法(例如量化、剪枝、网络结构搜索等)，并在研究或实习项目中有相应的实践经历； (3) 具备扎实的计算机基础知识、C++/Python编程能力，熟悉常见数据结构和设计模式； (4) 具有很强的学习能力、复杂问题归纳梳理能力、沟通和团队协作能力，具备能够深度钻研技术的耐心； 此外，具备以下方面的背景是很强的加分项： (1) 有很强的学术研究能力和优秀的学术成果(AI领域顶会/顶刊论文) (2) 熟悉体系结构，并有扎实的高性能计算/AI编译器/推理框架/模型优化经验 (3) 作为核心贡献者参与开发或者负责维护AI领域的流行开源项目 预期的研究创新成果：完成高效模型压缩算法设计，取得优于业界主流方法的效果，发表人工智能领域高质量论文/高质量专利创新提案，集成至阿里云PAI平台模型优化工具体系或者相关开源项目中。



### OPPO2024届暑期实习

方向一：负责推荐、搜索、广告算法研发，覆盖广告、信息流、视频、应用商店、全局搜索等多个场景；

方向二：AI技术业务赋能，利用机器学习，深度学习等前沿技术，深度结合信息流、应用商店、搜索、语音助手等核心业务场景解决自然语言处理、语音和图像识别等多领域问题，整体提升用户体验和业务价值；

方向三：ColorOS上的AI产品开发和研究，学习用户行为习惯，优化手机系统性能，探索计算机视觉和自然语言处理在ColorOS业务上的应用，整体提升ColorOS的用户体验；

**方向四：机器学习框架方向，深入优化现有机器学习框架，提升推理框架性能等；**

**方向五：深度学习模型优化、压缩，设备端算法落地等；**

方向六：负责图像检测，识别，语义分割等相关核心算法研究及落地；

方向七：负责语音识别，语音唤醒，回声消除等相关核心算法研究及落地；

方向八：图计算技术（图分析、图神经网络等）的核心算法研究及落地，参与图计算平台的研发；

方向九：行为时序预测技术的先进算法研究及落地；

**方向十：隐私安全计算（联邦学习，差分隐私，同态加密，可信执行环境等）算法研究及落地；**

方向十一：研究探索多领域运动健康方向，对健康数据研究和应用。



### 岗位要求：AutoML

- 1、参与AutoML定制化的实际业务场景中，积累实践经验，赋能字节系众多产品； 2、负责Diffusion Model相关的模型研发、**小样本**训练、**Prompt**工程、**训练优化**、**推理优化**的研究和实现； 3、负责自监督，弱监督，半监督，大规模预训练等**高效数据标签算法**的研究与实现； 4、负责AutoML的前沿算法研究，包括模型**蒸馏、神经网络架构搜索与设计、轻量化、超参调优**等； 5、负责**模型结构压缩和量化**方面的研究及实现； 6、负责模型转换工具开发和**模型部署**。



### 岗位要求：边缘AI工程化

- -负责边缘场景下计算机视觉算法的研发工作，包括但不限于目标检测、分割、人脸识别、人体关键点、目标跟踪等 -负责相关算法在边缘设备上的部署和高效运行 -负责相关算法的持续迭代优化 -负责对用户场景业务场景进行分析，将业务需求转化为技术需求，并提出合适的算法解决方案。



### 岗位要求：机器学习平台/机器学习系统/算法框架/深度学习框架

- 1、利用字节的平台和系统做机器学习的研究（可以和学校研究项目相关），和AML (Applied Machine Learning)团队的工程师和研究员一起探讨问题； 2、对字节的机器学习平台以及系统提出建设性的改进建议，对系统和平台进行优化。

- 1、参与字节跳动AML的机器学习训练框架的研究与开发，服务于全公司各个产品； 2、参与机器学习训练框架底层组件的抽象，设计，优化与落地； 3、与全公司算法部门合作，为重点项目进行算法与系统的联合优化。

- 1、参与设计与研发业界领先的智能语音-算法框架，打造覆盖语音识别、语音合成、语音交互以及自然语音处理等算法方案的一体化研发框架； 2、解决针对Tensorflow以及Pytorch使用上的各种疑难杂症（**分布式**、**优化器**、**ONNX导出**等），对主流模型训练场景端到端效率进行框架优化，追求极致效率； 3、联合算法、引擎支持模型导出以及离在线对齐，参与制定标准化模型交付（**导出**、**对齐**）流程；与算法工程师密切配合共建开发者生态、提升框架易用性等。

- 我们关注于现代机器学习系统当中的各个组件，从**软硬件协同设计**、核心深度学习与机器学习**引擎的优化**、**大规模分布式训练系统**的建设，并建设如**特征存储**、**推理引擎**、**算法编排和服务**等一系列标准组建。我们支持各种深度学习网络的灵活拆分，支持超TB的模型，驱动**异构计算和实时计算**的发展，不断追求效率的极致。我们通过推动多项领先的算法技术，是阿里巴巴技术驱动商业变革的核心引擎，快来加入我们吧！ 职位职责包括但不限于： 1. 设计和实现机器学习系统所需要的**大规模分布式计算系统**； 2. 机器学习全生命周期（**训练、推理、MLOps、CI/CD、AB testing**）组件的开发与优化； 3. 收集理解云上AI客户的需求并转化为技术系统设计； 4. 在机器学习系统的前沿领域（如**分布式训练**、**软硬协同设计**等）参与应用驱动的研究； 5. 与产品、解决方案、销售等兄弟团队实现云上技术到业务的转化。


- 我们的技术团队致力于深度学习从**特征计算**到**训练及预估引擎端到端的平台建设(XDL)**，包括当前重点发展的**图表征学习**，正在定义大数据AI技术的未来；我们支持各种深度学习网络的灵活拆分，支持超TB的模型，驱动异构计算和实时计算的发展，不断追求效率的极致；更重要的是，我们还拥有多项领先的算法技术，是阿里巴巴技术驱动商业变革的核心引擎。  与该领域密切相关的技术方向：统一的索引/图/模型存储 召回/排序/预测的图化执行/在线图计算/路径索引与检索/在线引擎Serveless/图表征学习与向量匹配/全链路算法迭代平台/DL特征计算与建模框架/DL训练与预测加速/端上智能/结构化搜索/图像搜索/搜索与推荐云服务。


- 参与公司深度学习框架的研发和优化，增强框架的计算能力，并在多种不同的国产训练芯片上进行适配。 2. 在国产芯片上对业务模型的性能进行优化，包括计算优化、通信优化及调用流程优化等，提高模型训练的速度。 3. 参与公司内的业务线的前沿算法的训练支持，包括大模型训练、国产芯片训练支持等，结合业务研究员的痛点迭代和优化训练框架。 4. 参与公司内的商业化交付和政府项目交付，主要内容是国产训练软件与国产芯片通用训练支持。


- 你是整个集团广告/推荐/搜索引擎平台和业务背后的支持者； 是驱动商业变革的多项领先算法技术的核心引擎。 在这里你可以： 面向蚂蚁的科技服务平台，基于**大数据、大规模分布式集群计算**环境，进行机器学习建模； 算法产品化建设，参与模型**在线实时计算框架**开发落地； 算法研究和创新，结合实际业务需求，抽象算法问题，给出创新方案； 算法效果迭代改进，进行**特征工程**开发及应用； 支持各种深度学习网络的灵活拆分； 支持超TB的模型，驱动**异构计算和实时计算**的发展； 深度学习当前重点发展的**图表征学习、网络的灵活拆分、超TB的模型、驱动异构计算和实时计算**，不断追求效率的极致； 借助已有的该领域密切相关的技术方向丰满自己！ 我们正在定义大数据AI技术的未来,欢迎加入算法人的理想国~




### 岗位要求：推荐系统架构

- 字节跳动推荐架构团队，负责字节跳动旗下相关产品的推荐系统架构的设计和开发，保障系统稳定和高可用；负责在线服务、离线数据流性能优化，解决系统瓶颈，降低成本开销；抽象系统通用组件和服务，建设推荐中台、数据中台，支撑新产品快速孵化以及为ToB赋能。 1、负责字节跳动旗下相关产品推荐系统开发工作，解决推荐核心系统的架构优化问题； 2、针对推荐场景的架构抽象和流程优化，支持大规模的机器学习的优化，支持推荐平台的研发； 3、针对高并发高吞吐的大规模系统，提升系统的稳定性、性能、可扩展性； 4、应对多数据中心的技术挑战，研发全球化一体的推荐系统； 5、相关组件的研发和优化、新技术的应用和落地。


- 字节跳动推荐架构团队，负责字节跳动旗下相关产品的推荐系统架构的设计和开发，保障系统稳定和高可用；负责在线服务、离线数据流性能优化，解决系统瓶颈，降低成本开销；抽象系统通用组件和服务，建设推荐中台、数据中台，支撑新产品快速孵化以及为ToB赋能。 1、参与字节跳动旗下相关产品的架构抽象和流程优化，支持大规模的机器学习的优化，支持推荐平台的研发； 2、负责高质量Python系统的开发，包含亿级DAU产品的离线数据流，运营系统、审核系统等平台开发； 3、优化在线和离线服务性能，支持高并发、多机房、容灾方案的实施； 4、各业务场景下离线数处理和分析、AB策略的支持； 5、为全公司众多业务提供中台化方案支持和实施。




### 岗位要求：隐私保护/数据安全/隐私计算/密码学

- 1、参与隐私保护技术在产品与服务中的设计与实现。2、分析全球隐私保护法律要求、行业标准及业界实践，制定隐私保护策略与流程。3、制定新技术、新业务发展的隐私保护合规架构，参与业界相同行业标准和规范的制定。4、组织隐私保护行业展会，策划隐私保护发言，与客户交流隐私保护合规实践与技术方案。

- 我们以最大化数据商业价值为目标，正致力于构建数据中台的原生数据安全解决方案，主要工作领域有：**数据安全风控体系**建设、**多方安全计算**和**密码学**前沿安全技术、数据合规及个人隐私保护系统化建设、数据生态的黑灰产对抗等；我们要解决的是数据安全的问题，彻底释放数据价值，努力打造业界领先的原生、动态、智能的数据安全解决方案。


- 参与设计和建设美团金服隐私计算平台和工具；支撑业务方和客户的项目落地，包括方案和文档的撰写、系统的安装和部署、培训、解决业务方和客户在使用隐私计算平台过程中的问题。阅读优秀的开源代码库，在导师的指导下，进行二次开发，开展性能测试和功能测试；


- 研发隐私计算技术及算法，桥接数据孤岛并为敏感数据业务提供安全理论及技术支撑；参与或主导前沿隐私计算理论与技术的分析与预研。对**同态加密**，**安全多方计算**，**差分隐私**，**联邦学习**，**可信执行环境**（如SGX, Trustzone）等至少一种技术有所研究。有相关领域顶会、顶刊论文发表为加分项


- 在这里，你有可能从事以下几种具体工作之一： 1. 负责密码学前沿技术跟踪和创新预研； 2. 负责研发面向场景优化的密码学算法和系统； 3. 负责软硬协同的密码加速技术研究。如**同态加密、安全多方计算、联邦学习、后量子密码、差分隐私、或密码加速芯片设计**等


- 你是隐私计算领域的集大成者， 掌握着解决数据孤岛、促进数据生产要素流通的关键技术； 熟悉该技术在金融、广告、科研、政务等行业如何广泛应用； 熟悉**安全多方计算、同态加密、差分隐私、可信执行环境、联邦学习**等隐私保护技术； 熟悉隐私计算涉及的**密码学、分布式计算、安全硬件、AI、编译器、硬件加速**等多个领域。 你致力于将技术运用到更广大的平台、更多的业务场景， 对隐私计算相关技术有深入研究。 在这里你可以： 隐私计算、隐私保护机器学习算法设计和实现不是梦（包括但不限于**密码、机器学习、实现加速、编译等算法**）； 隐私计算平台研发的深入研究（包括但不限于**隐私保护AI、隐私保护数据库、隐私计算虚拟机、隐私保护营销**等平台）； 在你的推动下相关前沿技术调研和分析更加标准清晰。




### 岗位要求：密码学/蚂蚁集团计算系统

- 隐私保护日益受到重视，数据的密态使用将成为新的数据处理范式。顺应全新的趋势，蚂蚁技术研究院聚焦隐私计算性能优化和加速技术的基础研究。 在这里，你有可能从事以下具体工作之一： 1.负责研发面向场景优化的密码算法系统； 2.负责软硬协同的密码加速技术研究。




### 岗位要求：运筹优化

- 具体职责包括但不限于：1、负责**整数优化和组合优化**的算法研究和工具开发； 2、负责**非线性优化、随机优化、最优控制方法**的研发； 3、负责优化模型以及建模工具的建设； 4、负责针对**电力调度**等行业的求解加速算法； 5、负责利用机器学习对传统优化方法进行加速。 


- 1、负责运筹优化／机制设计方向的算法研究和开发，包括但不限于**组合优化，博弈论，在线优化，随机优化，混合整数优化，线性规划，控制论，高性能求解软件研发，高性能数值代数方法和实现**等； 2、负责将优化技术和机器学习等技术有效结合并应用于菜鸟的**物流和计算资源调度**等领域，实现成本降低，效率提高，协助提高核心竞争力； 3、负责大数据的分析和建模，沉淀行业解决方案，协助拓展业务边界。


