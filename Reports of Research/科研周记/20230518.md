每周和师兄讨论

每天看一篇论文

人无我有

人有我优

人有我廉

### 组会攻略

- 每周看两篇论文
- 看论文的原因是什么
- 解决了什么问题



### [机器学习系统](https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)

![image-20230425193939571](https://raw.githubusercontent.com/ailianligit/ailianligit.github.io/main/images/202304/20230425_1682422781.png)



### 阿里巴巴项目制实习生

- **研究型实习生 - 联邦学习中的模型融合与适配**：联邦学习作为一种机器学习范式，能够联合不同参与方、进行联合训练。譬如联合多个端设备的数据和算力、在云端服务器的协调下完成数据不出域的训练。但如何更好的将不同参与方的模型进行融合、以及如何将融合后的模型更好地适配给不同的使用方，这些挑战仍需更好的去解决，以便在广阔的应用场景中发挥作用。
- **研究型实习生-基于在线深度学习的探索与利用（Exploration）**：在线深度学习正逐渐在工业界及学术界获得越来越高的关注与研究，对于新品的冷启动依然是一个非常有挑战的问题，比如新的广告或者商品创建以后，如何能够在获得少量展示pv的情况下更准确地预估其点击率，从而缩短新品的冷启动时间。为了达到冷启动加速的目的，简单而言可以从以下几个方面开展：1）寻找一种更合适的探索和利用的机制，通过利用少量的流量来进行新品的探索，通过合理的机制优化和理论研究，从而让新品获得更快的速度完成冷启动，同时促进长期广告/推荐生态更健康的发展；另一个方面是，2）通常新品细粒度的特征比较缺乏，比如统计特征少或者不置信，然而新品的粗粒度的特征可以有更好的泛化性，或者利用相似商品的信息比如图网络来进行特征增强等方面在模型预估和特征方面针对新品做加强。这里会重点针对第一个方面展开研究，有针对性的解决实际系统中的冷启动问题，探索出一套新的在线深度学习解决方案。
- **研究型实习生- 【2021级华南理工联合培养ART专项】+ 基于预训练语言模型的小样本学习算法研究**：预训练语言模型的提出深刻地推动了自然语言处理（Natural Language Processing，NLP）的发展。当这些预训练语言模型在无标注的语料库上进行自监督训练后，只需要在特定任务、特定领域的训练集上进行微调，即可获得所需的用于下游NLP任务的模型。预训练语言模型在很多应用场景都有很大的需求。然而，在互联网环境下，由于大量新的领域不断涌现，新的任务需求不断被提出；在每个领域、每个NLP任务上都积累足够多的训练数据，将耗费大量的时间、人力与物力，无法满足模型快速生成与迭代的要求，从而对各种业务的算法支持带来困难。迁移学习技术可以利用其他领域的知识或数据来帮助新的领域下模型的学习。其主要缺点在于，模型无法在训练数据极少的情况下，快速适应新的任务需求。因此，研究基于预训练语言模型的小样本学习（Few-Shot Learning）算法，对于构建任务自适应、领域自适应的NLP模型，支持多样化的实际业务，有重要的现实意义。
- **研究型实习生-【2021级华南理工联合培养ART专项】+ 基于预训练语言模型的多任务知识蒸馏算法**：预训练语言模型的提出深刻地推动了自然语言处理（Natural Language Processing，NLP）的发展。当这些预训练语言模型在无标注的语料库上进行自监督训练后，只需要在特定任务、特定领域的训练集上进行微调，即可获得所需的用于下游NLP任务的模型。预训练语言模型在很多应用场景都有很大的需求。然而，预训练语言模型并不是完美无瑕的，这类模型仍然存在以下两个问题：（1）模型参数量太大：举例而言，BERT-base 模型能够包含一亿个参数，较大的 BERT-large 甚至包含 3.4 亿个参数。显然，很难将这种规模的模型部署到资源有限的环境（例如移动设备或嵌入式系统）当中。（2）训练/推理速度慢：在基于 Pod 配置的 4 个 Cloud TPUs（总共 16 个 TPU 芯片）上对 BERT-base 进行训练，或者在 16 个 Cloud TPU（总共 64 个 TPU 芯片）上对 BERT-large 进行训练，每次预训练都需要至少 4 天的时间才能完成。而BERT的推理速度更是严重影响到了需要较高QPS的线上场景，部署成本非常高。提出基于预训练语言模型的知识蒸馏算法，以及从蒸馏算法训练小模型，沉淀到阿里巴巴的算法库 发表CCF-A/B类会议/期刊论文一篇以上
- **研究型实习生-多任务统一提示学习技术**：大型预训练语言模型已经广泛应用于自然语言处理的各个任务里，通过在下游任务一定数量的标注数据上微调，可以获得SOTA性能。近些年，学者发现大型预训练语言模型不仅在资源充足的全监督任务上有效，得益于预训练模型强大的泛化能力，通过有效的模型设计，特别是采用提示学习，模型在各种零/少样本任务（文本分类、词性标注、实体识别等）也发挥出色，使得基于零少样本任务学习成为可能。但是目前的提示学习大多数是根据单个任务进行设计和优化，在实际场景下，任务与任务直接有一定的关联，我们希望设计多任务统一提示学习技术，通过数据共享、模型参数共享, 充分利用不同任务的数据，共享任务参数，可以进一步提升模型的泛化能力，以此有更强大的零样本和少样本学习能力。 通过统一多任务学习框架获得的预训练模型，既可以直接作为应用模型，一次获得多个任务的结果。也可以作为底座用于跨任务、跨领域训练。
- **垂直领域数据在大规模预训练模型上的非参化应用**：在预训练模型被广泛应用的大背景下，如何进一步利用下游任务领域数据，进行有效、灵活的领域迁移，成为预训练模型落地需要解决的重点任务之一。 经典的领域数据应用方法（领域适应Fine-tuning），容易因数据偏置造成实际应用效果不稳定。尤其是当领域数据数量少，会有over-fitting问题，导致模型泛化能力差。 基于few-shot prompting 的方法从预训练模型中提取知识，有很大的随机性，需要反复试验才能在领域上看到效果。 本项目目标为探索领域数据和知识在预训练模型的下游应用。通过数据使用策略和非参化算法，提高下游数据使用效果。 1. 量化数据质量和数据分布，校验数据覆盖和质量。利用active- learning技术增加领域覆盖度。 通过领域数据特征的提取，白盒化领域数据和预训练模型分布的关系，增强下游领域数据使用效率。 2. 更近一步，在预训练模型参数固定的前提下，如何用非参数的算法，有效灵活的融合领域数据到大模型中。I. 有效挖掘领域数据，研发基于非参数化的K-nearest-neighbor Retrieval算法，优化领域数据应用方法，强化基于NLP预训练模型的领域适应能力，在NLP1-2个重点领域或行业上，超越SOTA方法，平均提升2个点。
- **研究型实习生 - 深度学习模型快速压缩**：随着深度学习模型参数规模日益增长、以及大模型在各类任务效果的显著提升，模型压缩也成为解决推理延迟和部署成本等挑战的关键技术。模型压缩需要在模型精度和参数或计算压缩率之间取得最佳平衡，主流模型压缩技术为了取得更好的效果，通常会涉及微调训练（例如量化感知训练、稀疏微调训练等），这使得模型压缩本身会引入额外的时间和算力开销，并且开销会随着模型规模增长而变得更加显著。如何既“好”又“快”地实现模型压缩，能够从（包括但不局限于）以下方面进行探索研究：(1)训练后模型压缩(Post-Training Compression)，训练后量化(PTQ)是被验证有效且相对易用的模型压缩技术，可以探索将类似方法拓展应用至稀疏、更低比特量化、混合精度等更多情况；(2)参数高效(Parameter-Efficient)压缩训练，在大模型上进行全量参数的微调训练对算力资源有很高要求，叠加模型压缩引入的额外参数，会进一步增大算力资源开销，降低可训练参数或者进行高效的参数初始化，是提升压缩训练效率的重要因素。希望研究型实习生候选人具备以下能力背景： (1) 熟悉深度学习基本原理、主流深度学习算法及应用、以及至少一种主流框架(PyTorch、TensorFlow等)； (2) 熟悉至少一类主流模型压缩算法(例如量化、剪枝、网络结构搜索等)，并在研究或实习项目中有相应的实践经历； (3) 具备扎实的计算机基础知识、C++/Python编程能力，熟悉常见数据结构和设计模式； (4) 具有很强的学习能力、复杂问题归纳梳理能力、沟通和团队协作能力，具备能够深度钻研技术的耐心； 此外，具备以下方面的背景是很强的加分项： (1) 有很强的学术研究能力和优秀的学术成果(AI领域顶会/顶刊论文) (2) 熟悉体系结构，并有扎实的高性能计算/AI编译器/推理框架/模型优化经验 (3) 作为核心贡献者参与开发或者负责维护AI领域的流行开源项目 预期的研究创新成果：完成高效模型压缩算法设计，取得优于业界主流方法的效果，发表人工智能领域高质量论文/高质量专利创新提案，集成至阿里云PAI平台模型优化工具体系或者相关开源项目中。



### OPPO

2024届暑期实习

方向一：负责推荐、搜索、广告算法研发，覆盖广告、信息流、视频、应用商店、全局搜索等多个场景；

方向二：AI技术业务赋能，利用机器学习，深度学习等前沿技术，深度结合信息流、应用商店、搜索、语音助手等核心业务场景解决自然语言处理、语音和图像识别等多领域问题，整体提升用户体验和业务价值；

方向三：ColorOS上的AI产品开发和研究，学习用户行为习惯，优化手机系统性能，探索计算机视觉和自然语言处理在ColorOS业务上的应用，整体提升ColorOS的用户体验；

**方向四：机器学习框架方向，深入优化现有机器学习框架，提升推理框架性能等；**

**方向五：深度学习模型优化、压缩，设备端算法落地等；**

方向六：负责图像检测，识别，语义分割等相关核心算法研究及落地；

方向七：负责语音识别，语音唤醒，回声消除等相关核心算法研究及落地；

方向八：图计算技术（图分析、图神经网络等）的核心算法研究及落地，参与图计算平台的研发；

方向九：行为时序预测技术的先进算法研究及落地；

**方向十：隐私安全计算（联邦学习，差分隐私，同态加密，可信执行环境等）算法研究及落地；**

方向十一：研究探索多领域运动健康方向，对健康数据研究和应用。



### 研究方向：

图联邦学习

**模型压缩：剪枝 蒸馏**

NAS

超参数优化

公平联邦学习

**边缘计算任务调度**

联邦学习模型异构 

个性化联邦学习：联邦学习数据异构

**大模型训练**

时间序列

few-shot

预训练大模型

TinyML

持续学习

鲁棒机器学习（可信机器学习）

轻量化AI



### 参考资料：

犀牛鸟计划

知乎问题

B站视频

B站专栏

prompt tuning lora PEFT

网络经济实验室、陈旭、陈亮、张晓溪

脉脉、求职软件、华为云、阿里云、运筹优化offer

边缘计算论文

NeurIPS/ICML/ICLR2023

剪枝与噪声的关系

联邦学习noniid理论

大模型训练+预训练对比学习论文

联邦NAS

联邦参数优化

参考文献文件夹

MoE、fedml

A Coalition Formation Game Approach for Personalized Federated Learning

Resource-Adaptive Federated Learning with All-In-One Neural Composition

Federated Continual Learning with Weighted Inter-client Transfer

Federated Online Learning Based Recommendation Systems for Mobile Social Applications

Federated Learning on Non-IID Data Silos: An Experimental Study



选择方向的维度兴趣：机器学习 数据驱动决策 博弈 运筹 序列决策 分布式优化 分布式计算 在线优化 随机优化 时间序列 隐私计算 强化学习 联邦学习数学（重要性）：优化 统计 运筹 博弈计算机（无先后顺序）：体系结构 数据库 分布式系统 并行计算产学研热点（无先后顺序）：通用AI 个性化AI 大模型 基础模型 模型压缩 隐私计算就业（数理基础）：证券 互联网大厂 银行 事业单位 算法工程师 量化研究员 系统工程师深造（热点问题 好发论文）：机器学习 强化学习 序列决策 随机优化 博弈论与机制设计 在线优化 计算/网络经济学 联邦学习



产学研热点方向：通用AI 个性化AI 大模型 基础模型 模型压缩 隐私计算 可解释性AI阿里：大小模型协同进化、高精度医疗导航、全域隐私计算、云网端融合百度：超大规模预训练模型呈现知识增强、跨模态统一建模、多学习方式共同演进的趋势，并逐渐实用化；隐私计算技术备受关注，将成为数据价值释放的突破口和构建信任的基础设施；绿色低碳更多纳入AI蓝图，助力实现碳达峰碳中和目标华为：通用AI、可信AI谷歌：More Capable, General-Purpose ML Models；Continued Efficiency Improvements for ML；ML Is Becoming More Personally and Communally Beneficial；Growing Benefits of ML in Science, Health and Sustainability；Deeper and Broader Understanding of ML



机器学习 (ML) 任务是当今边缘计算网络中的主要工作负载之一。 现有的边缘-云调度程序将请求的资源量分配给每个任务，无法最好地利用有限的边缘资源来执行 ML 任务。 本文提出了 TapFinger，一种用于边缘集群的分布式调度程序，它通过共同优化任务放置和细粒度多资源分配来最小化 ML 任务的总完成时间。 为了了解任务的不确定资源敏感性并启用分布式调度，我们采用多代理强化学习（MARL）并提出了几种使其高效的技术，包括作为 MARL 主干的异构图注意网络，定制任务选择阶段 演员网络，以及贝叶斯定理和掩蔽方案的整合。 我们先实现一个单任务调度版本，每次最多调度一个任务。 然后我们推广到多任务调度情况，其中同时调度一系列任务。 我们的设计可以减轻扩展的决策空间并产生快速收敛到最佳调度解决方案。 使用合成和测试床 ML 任务跟踪的大量实验表明，与最先进的调度程序相比，TapFinger 可以将平均任务完成时间减少多达 54.9%，并提高资源效率。

边缘计算是一种分布式计算范式，可将云功能扩展到边缘，以获得更好的服务质量 (QoS) 和数据隐私保护。 通过将服务和资源移动到更靠近最终用户的位置，它可以提供高可用性服务并显着降低服务延迟。 作为当今主要的应用程序工作负载之一，基于边缘的机器学习 (ML) 应用程序，从流量预测到生产工作流监控，通常处理在边缘生成的在线数据流 [2]。 由于边缘设备的资源限制，这些 ML 任务已部署在边缘集群 [2]、[3] 中，例如 NVIDIA EGX [4]、Microsoft Azure Edge [5] 和 AWS Outposts [6]。 它们由编排工具管理，可以在足够的 CPU 和 GPU 以及定制的软件工具包和网络接口卡上运行，例如，用于加密的物联网传感器数据 [4]。 然而，边缘集群的资源仍然有限。 优化基于边缘的 ML 应用程序 QoS 的核心是有效利用资源，同时及时学习所需的 ML 模型

对于各种模型和数据集，ML 训练和推理任务具有不确定性和多样性的性能 [7]、[8]、[9]，难以实现优化的资源效率。 YARN [10]、Kubernetes [11]、KubeEdge [12] 和 OpenYurt [13] 等实用调度程序通常采用预设规则进行资源分配。 这些策略依赖于任务的准确资源估计，而 ML 任务的资源需求通常具有弹性和不确定性（例如，模型收敛所需的时间量），适应各种性能资源权衡 [14]。 已经提出基于学习的云边缘调度程序来解决这种不确定性 [14]、[15]、[16]、[17]、[18]。 然而，它们不能推广到我们的场景，在我们的场景中，需要更好地编码更复杂的决策依赖关系，例如，为了进一步解决问题输入和决策变量的高维性。 事实上，细粒度的资源分配和战略性任务布置对于最大化边缘 ML 任务的总体性能非常重要。 如图 1 所示，由于数据强度和低延迟要求，移动边缘设备需要将其 ML 任务卸载到“正确”的边缘集群，以在最短时间内实现模型收敛。 此外，不同的资源，例如 CPU 和 GPU，会影响任务性能，需要多资源分配方案。 考虑到动态网络连接、作业干扰 [19] 和多资源竞争的复杂性，我们问：如何设计一个可扩展的、细粒度的、有远见的、为边缘 ML 定制的资源调度器？

为实现这一点，我们提出了 TapFinger，这是一种分布式调度程序，用于联合优化任务放置和跨边缘集群的细粒度多资源分配，目标是最大限度地减少 ML 任务的总完成时间。 为实现这一目标，我们面临以下基本挑战。

细粒度的资源分配。 与现有的将请求的资源量分配给每个任务 [8]、[15] 的边缘云调度程序不同，根据需求和供应进行细粒度资源供应 [9] 可以实现更好的资源效率和应用程序 QoS。 边缘调度器面临的挑战是，预测每个并发运行的任务的性能，并从巨大的解决方案空间中战略性地选择最佳资源量，以最大化总体任务性能，例如，实现特定目标所需的训练时间 准确性

不同资源的不确定影响。 大多数 ML 任务调度程序在云 [9]、[17]、[20] 或边缘设置 [3] 中分配单一类型的资源，例如 GPU，未能捕获多种类型的资源对任务性能的影响 . 多资源分配问题通常是 NPard 问题，即使对问题输入 [21] 有完美的了解，当任务性能在给定资源的情况下未知时，这更具挑战性。 强化学习 (RL) 可以有效地处理不确定性 [22]、[23]、[24]，但对于具有复杂约束的在线组合问题仍未得到充分探索。

跨边缘集群的分布式调度。 实际的边云集群通常由中央协调器 [10]、[11] 管理。 然而，集中调度处理多个地理区域的全局信息并且通常具有较差的可扩展性。 因此，分散式方法更适合减少决策空间，并可实现更好的系统可靠性。 尽管如此，如果在每个边缘集群中独立进行自我优化决策，分散调度可能会导致次优任务性能。 分布式调度器之间的有效交互对于优化整个边缘网络中的全局资源效率至关重要。

DL 工作负载的资源调度。 尽管通用任务调度算法，如 Dominant Resource Fairness [28]、Shortest Remaining Job First (SRJF) [29]、Tetris [30] 及其改进的变体 [31]、[32]、[33] 已得到广泛研究， 针对机器学习 (ML) 工作负载量身定制的策略还为时过早 [34]。 几个开创性的调度框架捕获了 DL 任务的不确定执行时间，旨在最小化完成时间。 彭等。 [7] 将 Optimus 设计为通过预测训练速度作为每个任务分配资源的函数来安排分布式 DL 训练任务。 然后，调度程序可以增量调整所有任务的参数服务器和工作人员的数量。 为了利用 DL 任务中的循环模式，Xiao 等人。 [20] 基于现有的 DL 框架开发原语，以实现高效的时间切片和分析驱动的内省。 Tiresias [8] 结合最少获得的服务调度和多级反馈队列来设计抢占式调度算法。 在 [9] 中，提出了 Apathetic Future Share 算法以实现高效的资源共享，并实现了专用的弹性共享 DL 训练系统来验证所提出算法的优势。 最近的一些工作使用多资源流水线和作业间交错 [35]、[36]、新颖的分析方法 [3]、[37] 或新的性能指标 [9]、[38] 实现了 DL 任务的弹性 GPU 集群调度 ]. 王等。 [39] 提出了一种用于边缘云网络的在线抢占式 ML 任务调度算法。 通常，这些是基于规则的算法，依赖于对任务特征的准确估计。 相反，本文旨在实现一个自我优化框架，无需手工制作预测模型来捕获不确定的任务性能。



### 重点考虑的研究方向（优化+机器学习）：

- 分布式机器学习、机器学习系统与运筹优化


- 模型压缩、大模型训练与运筹优化：NAS、超参数优化


- 面向边云协同网络中分布式机器学习的动态优化与资源调度




### 研究方向应考虑的角度：

- 热门中的冷门


- 好不好发论文
- 工业界需求


- 兴趣：运筹优化


- 找工作：机器学习


- 脚踏实地且有意义的方向


- 国家重视实体经济


- 掌握一门手艺
- 高质量数据、高算力、大模型
- NeurIPS/ICLR/ICML顶会方向
- 犀牛鸟计划等产学研结合方向
